{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NLTK\n",
    "___\n"
   ],
   "id": "700c6019c3802579"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 📦 One-Time Setup (NLTK Resources)",
   "id": "17b64a5a35525229"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "3957678e92123d8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T11:06:06.793539Z",
     "start_time": "2025-10-06T11:06:06.775562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import html\n",
    "import ipywidgets as widgets, IPython, platform, ipywidgets, jupyterlab\n",
    "from importlib import reload\n",
    "import io\n",
    "from isort.format import remove_whitespace\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sqlite3\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import string\n",
    "import time\n",
    "from textblob import Word, TextBlob\n",
    "import unicodedata"
   ],
   "id": "9a5c2c52dd74d494",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T14:46:18.207194Z",
     "start_time": "2025-10-06T14:46:18.148605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import processing as pc\n",
    "reload( pc )\n",
    "from processing import Text"
   ],
   "id": "cbed53edd93bfdca",
   "outputs": [],
   "execution_count": 232
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e85c9371a51f6c46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T14:46:20.097520Z",
     "start_time": "2025-10-06T14:46:20.091250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src = r'C:/Users/terry/Desktop/Test/Cleaned/'\n",
    "dst = r'C:/Users/terry/Desktop/Test/Datasets/'\n",
    "tx = Text( )"
   ],
   "id": "4c4a3094edae0ea",
   "outputs": [],
   "execution_count": 233
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T14:46:33.599077Z",
     "start_time": "2025-10-06T14:46:21.836940Z"
    }
   },
   "cell_type": "code",
   "source": "tx.chunk_datasets( src, dst )",
   "id": "96a2c3175c549513",
   "outputs": [],
   "execution_count": 234
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text = tx.load_text( src )\n",
    "collapsed = tx.collapse_whitespace( text )\n",
    "compressed = tx.compress_whitespace( collapsed )\n",
    "normal = tx.normalize_text( compressed )\n",
    "special = tx.remove_special( normal )\n",
    "sentences = tx.chunk_sentences( special )\n"
   ],
   "id": "7f527d0a2ff69006",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T18:59:12.146078Z",
     "start_time": "2025-10-05T18:59:07.313520Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "fa512b07266ea77e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b1afba37621d610e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Text Processing Methods\n",
   "id": "ee23478183ced671"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ✅ Checklist\n",
    "\n",
    "| Step | Task                     | Function/Library               |\n",
    "|------|--------------------------|--------------------------------|\n",
    "| 1    | Load Text                | `open()`, `pandas.read_csv()` |\n",
    "| 2    | Convert to Lowercase     | `.lower()`                     |\n",
    "| 3    | Remove Punctuation       | `string.punctuation`           |\n",
    "| 4    | Remove Numbers           | `re.sub()`                     |\n",
    "| 5    | Trim Whitespaces         | `' '.join()`                   |\n",
    "| 6    | Tokenization             | `nltk.word_tokenize()`         |\n",
    "| 7    | Remove Stopwords         | `nltk.corpus.stopwords`        |\n",
    "| 8    | Lemmatization/Stemming   | `WordNetLemmatizer`, `PorterStemmer` |\n",
    "| 9    | Reconstruct Clean Text   | `' '.join()`                   |\n",
    "| 10   | (Optional) Spellcheck    | `TextBlob.correct()`           |\n",
    "| 11   | Vectorization for ML     | `TfidfVectorizer`, `CountVectorizer`, Word Embeddings |\n",
    "\n",
    "\n",
    "___"
   ],
   "id": "947b8eb1368fe0c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.  Load File",
   "id": "36fbc578426999c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Load Raw Text ===\n",
    "file_path = '<url to file>'\n",
    "_rawtext = ''\n",
    "\n",
    "\n",
    "def load_text( file_path ):\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t\t_rawtext = f.read( )\n",
    "\t\treturn _rawtext\n"
   ],
   "id": "e3d5cc42b6dd4e3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.  Clean Space\n",
    "- Consecutive whitespace reduced to a single space\n",
    "- Leading/trailing spaces removed\n",
    "- Blank lines removed"
   ],
   "id": "ea7db4bf069f99e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_space( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "\t\tThis function:\n",
    "\t\t_____________\n",
    "        Removes extra spaces and blank words from the path pages.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The raw path pages path to be cleaned_lines.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned_lines pages path with:\n",
    "                - Consecutive whitespace reduced to a single space\n",
    "                - Leading/trailing spaces removed\n",
    "                - Blank words removed\n",
    "\n",
    "    \"\"\"\n",
    "\t# Replace multiple spaces or tabs with a single space\n",
    "\ttext = re.sub( r'[\\r\\n\\t]+', ' ', text )  # Newlines, tabs\n",
    "\ttext = re.sub( r'\\s+', ' ', text )  # Collapse multiple spaces\n",
    "\treturn text"
   ],
   "id": "14c5765f6bf2af38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Normalize",
   "id": "63bd713fbba95faa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def normalize( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        This function: Normalizes the path pages path.\n",
    "          - Converts pages to lowercase\n",
    "          - Removes accented characters (e.g., é -> e)\n",
    "          - Removes leading/trailing spaces\n",
    "          - Collapses multiple whitespace characters into a single space\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The raw path pages path to be normalized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A normalized, cleaned_lines version of the path path.\n",
    "\n",
    "    \"\"\"\n",
    "\treturn unicodedata.normalize( 'NFKD', text ).encode( 'ascii', 'ignore' ).decode( 'utf-8' )"
   ],
   "id": "82515d328d466a74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Remove Punctuation",
   "id": "5e0ee21b82c3c43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_punctuation( text: str ) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "\t\tRemove non-alphanumeric characters\n",
    "\t\tand extra whitespace from text.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): The text text string.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tstr: Cleaned text string with only alphanumeric characters and spaces.\n",
    "\n",
    "    \"\"\"\n",
    "    text = re.sub( r'[^a-zA-Z0-9\\s]', ' ', text )\n",
    "    text = re.sub( r'\\s+', ' ', text ).strip( )\n",
    "    return text"
   ],
   "id": "3ec4704593ac2991",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Trim Whitespace\n",
    "\n",
    "- Removes leading and trailing whitespace\n",
    "- Replaces multiple internal spaces with a single space"
   ],
   "id": "5234418835bd42fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def trim_whitespace( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "\t\tPurpose:\n",
    "\t\t---------\n",
    "        This function:\n",
    "          - Removes leading and trailing whitespace\n",
    "          - Replaces multiple internal spaces with a single space\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The raw path path with potential extra whitespace.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            The cleaned_lines path with trimmed and normalized whitespace.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Replace multiple whitespace characters (spaces, tabs, etc.) with a single space\n",
    "\tcleaned_text = re.sub( r'\\s+', ' ', text )\n",
    "\treturn cleaned_text"
   ],
   "id": "37ade6f5b1d34922",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Lemmatize\n",
    "- Reduces words to their base or root form.\n",
    "- Converts text to lowercase\n",
    "- Tokenizes the text into words\n",
    "- Lemmatizes each token using WordNetLemmatizer\n",
    "- Reconstructs the lemmatized tokens into a single string"
   ],
   "id": "5f4db8f82f926ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def lemmatize( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Performs lemmatization on the path pages path.\n",
    "\n",
    "        This function:\n",
    "          - Converts pages to lowercase\n",
    "          - Tokenizes the pages into words\n",
    "          - Lemmatizes each token using WordNetLemmatizer\n",
    "          - Reconstructs the lemmatized words into a single path\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The path pages path to be lemmatized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A path with all words lemmatized.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Initialize lemmatizer\n",
    "\tlemmatizer = WordNetLemmatizer( )\n",
    "\n",
    "\tlower_case = text.lower( )\n",
    "\t# Convert to lowercase and tokenize_text\n",
    "\ttokens = word_tokenize( lower_case )\n",
    "\n",
    "\t# Lemmatize each token\n",
    "\tlemmatized_tokens = [ lemmatizer.lemmatize( token ) for token in tokens ]\n",
    "\n",
    "\t# Join words back to a path\n",
    "\tlemmatized_text = ' '.join( lemmatized_tokens )\n",
    "\n",
    "\treturn lemmatized_text"
   ],
   "id": "93dcb509d45e16f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def lemmatize_tokens( tokens: list[ str ] ) -> List[ str ]:\n",
    "    \"\"\"\n",
    "\n",
    "\t\tPurpose:\n",
    "\t\t--------\n",
    "\t\tLemmatize each token using WordNetLemmatizer.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t--------\n",
    "\t\twords (list): List of word words.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t-------\n",
    "\t\tlist: Lemmatized words.\n",
    "\n",
    "    \"\"\"\n",
    "    return [ lemmatizer.lemmatize( t ) for t in tokens ]"
   ],
   "id": "5b27f35ac9bb6730",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Tokenize Words\n",
    "\n",
    "- Converts text to lowercase\n",
    "- Uses NLTK's word_tokenize to split the text into words and punctuation tokens"
   ],
   "id": "76b7a3f02f7d210"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tokenize_words( words: List[ str ] ) -> List[ str ]:\n",
    "\t\"\"\"\n",
    "\n",
    "\t\tPurpose:\n",
    "\t\t-------\n",
    "        This function:\n",
    "          - Tokenizes the path pages path into individual word words.\n",
    "          - Converts pages to lowercase\n",
    "          - Uses NLTK's word_tokenize to split the pages into words and punctuation words\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        words : List[ str ]\n",
    "            A list of strings to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            A list of token strings (words and punctuation) extracted from the pages.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Convert to lowercase\n",
    "\ttokens = [ ]\n",
    "\tfor w in words:\n",
    "\t\t_token = nltk.word_tokenize( w )\n",
    "\t\ttokens.append( _token )\n",
    "\treturn tokens\n"
   ],
   "id": "3e56143f4b97ab1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8. Remove Special\n",
    "- Retains only alphanumeric characters and whitespace\n",
    "- Removes symbols like @, #, $, %, &, etc.\n",
    "- Preserves letters, numbers, and spaces"
   ],
   "id": "1ffdd326d8a48815"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_special( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Remove all special characters\n",
    "        from the text string, keeping only letters, digits, and whitespace.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text string to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The processed string with special characters removed.\n",
    "\n",
    "    \"\"\"\n",
    "\tcleaned = [ ]\n",
    "\tkeepers = [ '$', 'in', '(', ')', '', 'the', '. ', ': ',\n",
    "\t            '; ', 'and', 'but', 'be', 'was', 'what', 'for' ]\n",
    "\tfor char in text:\n",
    "\t\tif char in keepers:\n",
    "\t\t\tcleaned.append( char )\n",
    "\t\telif char.isalnum( ) or char == ' ':\n",
    "\t\t\tcleaned.append( char )\n",
    "\n",
    "\treturn ''.join( cleaned )"
   ],
   "id": "d915bd40a8960596",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 9. Remove Fragments",
   "id": "f10bd76ecce2c24c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_fragments( chunks: List[ str ] ) -> List[ str ]:\n",
    "\t\"\"\"\n",
    "\n",
    "        Remove all special characters\n",
    "        from the text string, keeping only letters, digits, and whitespace.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text string to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The processed string with special characters removed.\n",
    "\n",
    "    \"\"\"\n",
    "\tcleaned = [ ]\n",
    "\tfor text in chunks:\n",
    "\t\tif len( text) > 3:\n",
    "\t\t\tcleaned.append( text )\n",
    "\t\telse:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\treturn cleaned"
   ],
   "id": "1b1f126c114550db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 10. Remove Special Characters",
   "id": "db077fc923ecabcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_special_characters( text: str, keep_spaces: bool=True ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "\t\tPurpose:\n",
    "\t\t_______\n",
    "\t\tRemove special characters from the text.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): Input string to clean.\n",
    "\t\t\tkeep_spaces (bool): If True, preserves spaces; otherwise removes all non-alphanumerics.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tstr: Cleaned text with only alphanumeric characters (and optionally spaces).\n",
    "\t\"\"\"\n",
    "\tif keep_spaces:\n",
    "\t\treturn re.sub( r'[^a-zA-Z0-9\\s]', ' ', text )\n",
    "\telse:\n",
    "\t\treturn re.sub( r'[^a-zA-Z0-9]', ' ', text )"
   ],
   "id": "56565e0606ba12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 11. Remove HTML\n",
    "- Parses the text as HTML\n",
    "- Extracts and returns only the visible content without tags"
   ],
   "id": "3e5583a42894f537"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_html_tags( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "\t\tPurpose:\n",
    "\t\t-------\n",
    "        This function:\n",
    "        Removes HTML tags from the path pages path.\n",
    "          - Parses the pages as HTML\n",
    "          - Extracts and returns only the visible content without tags\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The path pages containing HTML tags.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned_lines path with all HTML tags removed.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Parse HTML and extract pages\n",
    "\tsoup = BeautifulSoup( text, \"raw_html.parser\" )\n",
    "\tcleaned_text = soup.get_text( separator=' ', strip=True )\n",
    "\treturn cleaned_text"
   ],
   "id": "d5e9b16f93ccfed8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 12. Chunk Words\n",
    "- Tokenizes the text into words\n",
    "- Groups them into consecutive word chunks\n",
    "- Returns a list of strings (each chunk_words)"
   ],
   "id": "b9e77cd71f1df62e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def chunk_words( text: List[ str ], chunk_size: int=50 ) -> List[ List[ str ] ]:\n",
    "\t\"\"\"\n",
    "\n",
    "\t\tPurpose:\n",
    "\t\t--------\n",
    "        Breaks a list of cleaned_lines,\n",
    "        tokenized strings into chunks of a specified num of words.\n",
    "\n",
    "        This function:\n",
    "          - Flattens the path get_list of tokenized strings (i.e., get_list of lists)\n",
    "          - Groups words into chunks of min `chunk_size`\n",
    "          - Returns a list of lists of words\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : get_list of tokenizd words\n",
    "            The path get_list where each element is a get_list of words (words).\n",
    "\n",
    "        chunk_size : int, optional (default=250)\n",
    "            Number of words per chunk_words.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        List[ List[ str ] ]\n",
    "            A list of a list of token chunks. Each chunk is a list of words.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Flatten the get_list of token lists into a single list; create chunks of words\n",
    "\tall_tokens = [ token for sublist in text for token in sublist ]\n",
    "\tchunks = [ all_tokens[ i : i + chunk_size ] for i in range( 0, len( all_tokens ), chunk_size ) ]\n",
    "\treturn chunks"
   ],
   "id": "a6edbb0eb8fa5720",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 13. Chunk Pages",
   "id": "c454ec12bc8bd6e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def chunk_pages( text: List[ str ], chunk_size: int=300 ) -> List[ List[ str ] ]:\n",
    "\t\"\"\"\n",
    "\n",
    "        Breaks a list of cleaned_lines,\n",
    "        tokenized strings into\n",
    "        chunks of a specified num of words.\n",
    "\n",
    "        This function:\n",
    "          - Flattens the path get_list of tokenized strings (i.e., get_list of lists)\n",
    "          - Groups words into chunks of min `chunk_size`\n",
    "          - Returns a list of lists of words\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : get_list of tokenizd words\n",
    "            The path get_list where each element is a get_list of words (words).\n",
    "\n",
    "        chunk_size : int, optional (default=250)\n",
    "            Number of words per chunk_words.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        List[ List[ str ] ]\n",
    "            A list of a list of token chunks. Each chunk is a list of words.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Flatten the get_list of token lists into a single list; Create chunks of words\n",
    "\tall_tokens = [ token for sublist in text for token in sublist ]\n",
    "\ttoken_chunks = [ all_tokens[ i : i + chunk_size ] for i in range( 0, len( all_tokens ), chunk_size ) ]\n",
    "\treturn [ chunk for chunk in token_chunks ]"
   ],
   "id": "449f84f48132bc08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  Remove Encodings",
   "id": "2e1cfa66202711fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def remove_encodings(text: str) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Cleans text of encoding artifacts by resolving HTML entities,\n",
    "        unicode escape sequences, and over-encoded byte strings.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "        Input string potentially containing encoded characters.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "        Cleaned Unicode-normalized text.\n",
    "\n",
    "    \"\"\"\n",
    "    # Decode HTML entities (&amp;, &quot;, &#8217;)\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Decode escaped unicode literals (e.g., \\\\u2019)\n",
    "    try:\n",
    "        text = bytes(text, 'utf-8').decode('unicode_escape')\n",
    "    except UnicodeDecodeError:\n",
    "        pass  # Ignore undecodable sequences\n",
    "\n",
    "    # Normalize accents and symbols (e.g., é → e, ü → u)\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "\n",
    "    # Strip out stray control characters\n",
    "    text = re.sub(r'[\\x00-\\x1F\\x7F]', \"\", text)\n",
    "\n",
    "    return text.strip()"
   ],
   "id": "ddd963e903b2df6c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c2b879393b1660a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 14. Chunk Text\n",
    "\n",
    "- Converts text to lowercase\n",
    "- Tokenizes text using NLTK's word_tokenize\n",
    "- Breaks tokens into chunks of a specified size\n",
    "- Optionally joins tokens into strings (for transformer models)"
   ],
   "id": "e93a4c6067321b6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def chunk_text( text: str, chunk_size: int=50, return_as_string: bool=True ) -> List[ str ]:\n",
    "\t\"\"\"\n",
    "\n",
    "\t\tPurpose:\n",
    "\t\t--------\n",
    "        This function:\n",
    "        Tokenizes cleaned_lines pages and breaks it into chunks for downstream vectors.\n",
    "          - Converts pages to lowercase\n",
    "          - Tokenizes pages using NLTK's word_tokenize\n",
    "          - Breaks words into chunks of a specified size\n",
    "          - Optionally joins words into strings (for transformer models)\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The cleaned_lines path pages to be tokenized and chunked.\n",
    "\n",
    "        chunk_size : int, optional (default=50)\n",
    "            Number of words per chunk_words.\n",
    "\n",
    "        return_string : bool, optional (default=True)\n",
    "            If True, returns each chunk_words as a path; otherwise, returns a get_list of words.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        get_list\n",
    "            A get_list of token chunks. Each chunk_words is either a get_list of words or a path.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Download tokenizer models (only once)\n",
    "\ttokens = nltk.word_tokenize( text )\n",
    "\ttoken_chunks = [ tokens[ i : i + chunk_size ] for i in range( 0, len( tokens ), chunk_size ) ]\n",
    "\treturn [ ' '.join( c ) for c in token_chunks ]"
   ],
   "id": "f6b1ee045b318fd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 15. Remove Errors\n",
    "- Converts text to lowercase\n",
    "- Tokenizes the text into words\n",
    "- Filters out words not recognized as valid English using TextBlob\n",
    "- Returns a string with only correctly spelled words"
   ],
   "id": "b1e7c36fc4666c45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_errors( text: str ) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "\t\tPurpose:\n",
    "\t\t________\n",
    "\t\tRemove all non-English\n",
    "\t\twords but preserve valid numbers\n",
    "\t\tusing the NLTK English vocabulary.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): The text text string.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tstr: The cleaned string with only English words and numbers.\n",
    "\n",
    "    \"\"\"\n",
    "    vocabulary = set( w.lower() for w in words.words() )\n",
    "    keepers = [ '(', ')', '$', '.', ';', ':', ' - '  ]\n",
    "\n",
    "    # Tokenize: includes words and numbers\n",
    "    tokens = re.findall(r'\\b[\\w.]+\\b', text.lower( ))\n",
    "\n",
    "    # Keep words in vocab or numbers\n",
    "    def is_valid_token( tok: str ) -> bool:\n",
    "        return ( tok in vocabulary\n",
    "                or tok.isdigit( )\n",
    "                or tok in keepers )\n",
    "\n",
    "    valid_tokens = [ tok for tok in tokens if is_valid_token( tok ) ]\n",
    "    return ' '.join( valid_tokens )"
   ],
   "id": "818357851dd8bf54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 16. Correct Errors\n",
    "- Converts text to lowercase\n",
    "- Tokenizes the text into words\n",
    "- Applies spelling correction using TextBlob\n",
    "- Reconstructs and returns the corrected text"
   ],
   "id": "5c6cdef587829c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def correct_errors( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Corrects misspelled words in the path pages path.\n",
    "\n",
    "        This function:\n",
    "          - Converts pages to lowercase\n",
    "          - Tokenizes the pages into words\n",
    "          - Applies spelling correction using TextBlob\n",
    "          - Reconstructs and returns the corrected pages\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The path pages path with potential spelling mistakes.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A corrected version of the path path with proper English words.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Convert to lowercase and tokenize_text\n",
    "\ttokens = word_tokenize( text.lower( ) )\n",
    "\n",
    "\t# Apply spelling correction to each token\n",
    "\tcorrected_tokens = [ str( Word( word ).correct( ) ) for word in tokens ]\n",
    "\n",
    "\t# Join the corrected words into a single path\n",
    "\tcorrected_text = ' '.join( corrected_tokens )\n",
    "\n",
    "\treturn corrected_text"
   ],
   "id": "c7d8dfb08dfbf717",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 17.  Remove Headers\n",
    "- Assumes repeated lines at the top or bottom (like titles, page numbers)\n",
    "- Removes lines that are common across multiple pages (heuristic)\n",
    "- Returns cleaned text with main body content only"
   ],
   "id": "d8489c8d8d9d0af4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_headers_footers( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "\t\tRemoves common headers and footers from a pages document.\n",
    "\n",
    "\t\tThis function:\n",
    "\t\t  - Assumes repeated words at the top or bottom (like titles, page numbers)\n",
    "\t\t  - Removes words that are common across multiple pages (heuristic)\n",
    "\t\t  - Returns cleaned_lines pages with main body content only\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t-----------\n",
    "\t\tpages : str\n",
    "\t\t\tThe path pages potentially containing headers/footers.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t--------\n",
    "\t\tstr\n",
    "\t\t\tThe cleaned_lines pages with headers and footers removed.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Split the pages into words\n",
    "\tlines = text.splitlines( )\n",
    "\n",
    "\t# Remove empty words and trim whitespace\n",
    "\tlines = [ line.strip( ) for line in lines if line.strip( ) ]\n",
    "\n",
    "\t# Count line frequencies to identify repeated headers/footers\n",
    "\tline_counts = Counter( lines )\n",
    "\n",
    "\t# Identify frequent words (appear in >1% of total words)\n",
    "\tthreshold = max( 1, int( len( lines ) * 0.01 ) )\n",
    "\trepeated_lines = { line for line, count in line_counts.items( ) if count > threshold }\n",
    "\n",
    "\t# Remove words that are likely headers or footers\n",
    "\tbody_lines = [ line for line in lines if line not in repeated_lines ]\n",
    "\n",
    "\t# Reconstruct the cleaned_lines pages\n",
    "\tcleaned_text = '\\n'.join( body_lines )\n",
    "\n",
    "\treturn cleaned_text\n"
   ],
   "id": "f9c1ba26ea66c445",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 18.  Remove Formatting\n",
    "- Strips HTML tags\n",
    "- Removes Markdown syntax (e.g., *, #, [], etc.)\n",
    "- Collapses whitespace (newlines, tabs)\n",
    "- Optionally removes special characters for clean unformatted text"
   ],
   "id": "76f3936ac0e24e17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_formatting( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Removes formatting artifacts (Markdown, HTML, control characters) from pages.\n",
    "\n",
    "        This function:\n",
    "          - Strips HTML tags\n",
    "          - Removes Markdown syntax (e.g., *, #, [], etc.)\n",
    "          - Collapses whitespace (newlines, tabs)\n",
    "          - Optionally removes special characters for clean unformatted pages\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The formatted path pages.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned_lines version of the pages with formatting removed.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Remove HTML tags\n",
    "\ttext = BeautifulSoup( text, \"raw_html.parser\" ).get_text( separator=' ', strip=True )\n",
    "\n",
    "\t# Remove Markdown syntax\n",
    "\ttext = re.sub( r'\\[.*?\\]\\(.*?\\)', '', text )  # Markdown links\n",
    "\ttext = re.sub( r'[`_*#~>-]', '', text )  # Markdown chars\n",
    "\ttext = re.sub( r'!\\[.*?\\]\\(.*?\\)', '', text )  # Markdown images\n",
    "\n",
    "\t# Remove and normalize whitespace\n",
    "\ttext = re.sub( r'[\\r\\n\\t]+', ' ', text )  # Newlines, tabs\n",
    "\ttext = re.sub( r'\\s+', ' ', text ).strip( )  # Collapse multiple spaces\n",
    "\n",
    "\treturn text"
   ],
   "id": "be4039da12904496",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 19. Remove Stopwords\n",
    "- Tokenizes the input text\n",
    "- Removes common stopwords (e.g., \"the\", \"is\", \"and\", etc.)\n",
    "- Returns the text with only meaningful words"
   ],
   "id": "1c75687f6c08c2c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_stopwords( tokens: List[ str ] ) -> List[ str ]:\n",
    "    \"\"\"\n",
    "\n",
    "\t\tPurpose:\n",
    "\t\t-------\n",
    "\t\tRemove English stopwords from a list of words.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t----------\n",
    "\t\twords (list): Word words.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t--------\n",
    "\t\tlist: Filtered list of words with stopwords removed.\n",
    "\n",
    "    \"\"\"\n",
    "    return [ word for word in tokens if word.lower( ) not in stop_words ]"
   ],
   "id": "342dd22bcdd49aaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 20. Split Sentences",
   "id": "66e071eacb3377be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def split_sentences( text: str ) -> List[ str] :\n",
    "    \"\"\"\n",
    "\n",
    "\t\tPurpose:\n",
    "\t\t________\n",
    "\t\tSplits the text text string into a list of\n",
    "\t\tindividual sentences using NLTK's Punkt sentence tokenizer.\n",
    "\t\tThis function is useful for preparing text for further linguistic processing,\n",
    "\t\tsuch as tokenization, parsing, or named entity recognition.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\ttext : str\n",
    "\t\t\tThe raw text string to be segmented into sentences.\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tList[str]\n",
    "\t\t\tA list of sentence strings, each corresponding to a single sentence detected\n",
    "\t\t\tin the text text.\n",
    "\n",
    "    \"\"\"\n",
    "    return  sent_tokenize( text )"
   ],
   "id": "9667d83b7478ae0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🔍 Clean Text",
   "id": "f4c860851a941aeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_text( text: str ) -> str:\n",
    "\t_first = text.replace( '\\r\\n', '\\n' ).replace( '\\r', '\\n' )\n",
    "\t_second = re.sub( r'\\n\\s*\\d+\\s*\\n', '\\n', _first )\n",
    "\t_third = re.sub( r'(\\w+)\\n(\\w+)', r'\\1\\2', _second )\n",
    "\t_fouth = re.sub( r'(?<!\\n)\\n(?![\\n])', ' ', _third )\n",
    "\t_retval = re.sub( r'\\s+', ' ', _fouth )\n",
    "\treturn _retval"
   ],
   "id": "6ffc1b387c68d803",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🔍 Pipeline",
   "id": "f9435a341c128b29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Preprocessing Configuration ===\n",
    "EN_STOPWORDS = set( stopwords.words( 'english' ) )\n",
    "LEMMATIZER = WordNetLemmatizer( )\n",
    "STEMMER = SnowballStemmer( 'english' )\n",
    "\n",
    "\n",
    "def preprocess_line( line, lower=True, punctuation=True,\n",
    "                     stopwords=False, lemmatize=True, stem=False ):\n",
    "\t'''\n",
    "\t\tProcess a single line of documents with optional steps:\n",
    "\t\t- lower\n",
    "\t\t- punctuation removal\n",
    "\t\t- stopword removal\n",
    "\t\t- lemmatization\n",
    "\t\t- stemming (optional)\n",
    "\t\tReturns the cleaned_lines line as a path.\n",
    "    '''\n",
    "\ttokens = word_tokenize( line )\n",
    "\tprocessed = [ ]\n",
    "\tfor token in tokens:\n",
    "\t\tif lower:\n",
    "\t\t\ttoken = token.lower( )\n",
    "\n",
    "\t\tif punctuation and token in string.punctuation:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif stopwords and token in EN_STOPWORDS:\n",
    "\t\t\tcontinue\n",
    "\t\tif lemmatize:\n",
    "\t\t\ttoken = LEMMATIZER.lemmatize( token )\n",
    "\t\tif stem:\n",
    "\t\t\ttoken = STEMMER.stem( token )\n",
    "\n",
    "\t\tprocessed.append( token )\n",
    "\treturn ' '.join( processed )\n",
    "\n",
    "\n",
    "def process_file( file_path, **preprocess_kwargs ):\n",
    "\t'''\n",
    "        Read a documents file line-by-line, apply preprocessing pipeline to each line,\n",
    "        and return a get_list of cleaned_lines words (original order preserved).\n",
    "        Pass any keyword args supported by `preprocess_line`.\n",
    "    '''\n",
    "\tcleaned_lines = [ ]\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as file:\n",
    "\t\tfor line in file:\n",
    "\t\t\tcleaned = preprocess_line( line, **preprocess_kwargs )\n",
    "\t\t\tcleaned_lines.append( cleaned )\n",
    "\treturn cleaned_lines\n"
   ],
   "id": "af7f3fd7b2990387",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#  Vectorization (Preparation for ML Models)\n",
    "___\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "ba0f01ba423430f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "d3e02f4dbedcc22e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6ea810a20c6020f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Load Raw Text ===\n",
    "file_path = '<url to file>'\n",
    "_rawtext = ''\n",
    "\n",
    "\n",
    "def load_text( file_path ):\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t\t_rawtext = f.read( )\n",
    "\t\treturn _rawtext"
   ],
   "id": "f930499beb91e45f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4cd82d1886e5243",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🧮 1. Bag of Words (BoW) using CountVectorizer",
   "id": "93554b0db878266a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\tcorpus = [ 'Bro loves clean code.', 'Code is life.' ]\n",
    "\tvectorizer = CountVectorizer( )\n",
    "\tX = vectorizer.fit_transform( corpus )\n",
    "\n",
    "\tprint( vectorizer.get_feature_names_out( ) )\n",
    "\tprint( X.toarray( ) )\n"
   ],
   "id": "df2a01ee327d57d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 📊 2. TF-IDF using TfidfVectorizer",
   "id": "606ca62c6f083d04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\tcorpus = [ 'Bro writes awesome code.', 'Code must be clean and clear.' ]\n",
    "\tvectorizer = TfidfVectorizer( )\n",
    "\tX = vectorizer.fit_transform( corpus )\n",
    "\n",
    "\tprint( vectorizer.get_feature_names_out( ) )\n",
    "\tprint( X.toarray( ) )\n"
   ],
   "id": "58a4cf4ec025e261",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🧠 3. Word2Vec using gensim",
   "id": "cd392a973450c93a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\tsentences = [ [ 'bro', 'loves', 'python' ], [ 'clean', 'code', 'rocks' ] ]\n",
    "\tmodel = Word2Vec( sentences, vector_size=100, window=5, min_count=1, workers=4 )\n",
    "\n",
    "\t# VectorStore for the word 'bro'\n",
    "\tvector = model.wv[ 'bro' ]\n",
    "\tprint( vector )\n"
   ],
   "id": "a20f65018fd9c119",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🌍 4. GloVe using gensim (with pre-trained vectors)\n",
   "id": "a920cc11594a37ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\t# Load GloVe vec (convert .txt to .word2vec format beforehand if needed)\n",
    "\tglove_file = r'C:\\Users\\terry\\source\\llm\\glove\\glove.6B.100d.txt'\n",
    "\tmodel = KeyedVectors.load_word2vec_format( glove_file, unicode_errors='ignore' )\n",
    "\n",
    "\t# VectorStore for the word 'code'\n",
    "\tvector = model[ 'code' ]\n",
    "\tprint( vector )\n"
   ],
   "id": "95c58a17aeefed99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🤖 5. BERT / Transformer-based Embeddings using transformers + torch\n",
   "id": "8117a17e891dc4c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\ttokenizer = BertTokenizer.from_pretrained( 'bert-base-uncased' )\n",
    "\tmodel = BertModel.from_pretrained( 'bert-base-uncased' )\n",
    "\n",
    "\tsentence = \"Bro's code always works.\"\n",
    "\tinputs = tokenizer( sentence, return_tensors='pt' )\n",
    "\toutputs = model( **inputs )\n",
    "\n",
    "\t# Get the vector for [CLS] token (sentence embedding)\n",
    "\tsentence_embedding = outputs.last_hidden_state[ :, 0, : ]\n",
    "\tprint( sentence_embedding.shape )\n"
   ],
   "id": "8a9d362295bf0536",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean Document",
   "id": "5e61d59191d25e5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_text( text: str ) -> str:\n",
    "\ttext = text.replace( '\\r\\n', '\\n' ).replace( '\\r', '\\n' )\n",
    "\ttext = re.sub( r'\\n\\s*\\d+\\s*\\n', '\\n', text )\n",
    "\ttext = re.sub( r'(\\w+)-\\n(\\w+)', r'\\1\\2', text )\n",
    "\ttext = re.sub( r'(?<!\\n)\\n(?![\\n])', ' ', text )\n",
    "\ttext = re.sub( r'\\s+', ' ', text )\n",
    "\treturn text.strip( )\n"
   ],
   "id": "42c34418a1218398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fc9d08f36dde47ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "#  OpenAI Embedding\n",
    "___"
   ],
   "id": "9f097164e0f1b335"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### API key",
   "id": "10f8b0039c619d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )"
   ],
   "id": "6f7c70b192899b4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Define embedding function",
   "id": "e1c76c3548b771af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def embed_texts( texts, model='text-embedding-3-small', batch_size=10, sleep=1 ):\n",
    "\tembeddings = [ ]\n",
    "\tfor i in range( 0, len( texts ), batch_size ):\n",
    "\t\tbatch = texts[ i:i + batch_size ]\n",
    "\t\ttry:\n",
    "\t\t\tresponse = openai.embeddings.create( input=batch, model=model )\n",
    "\t\t\tbatch_embeddings = [ e.embedding for e in response.data ]\n",
    "\t\t\tembeddings.extend( batch_embeddings )\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint( f'Error at batch {i}: {e}' )\n",
    "\t\t\t# Retry or sleep to avoid rate limits\n",
    "\t\t\ttime.sleep( sleep )\n",
    "\t\t\tcontinue\n",
    "\n",
    "\treturn embeddings\n"
   ],
   "id": "3d19dce1b72312ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Embed chunks",
   "id": "7a1cbc3787a81ed2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Embed chunks\n",
    "embeddings = embed_texts( chunks )"
   ],
   "id": "2e8dd3b9c7453b0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.  Create DataFrame",
   "id": "7b29c98e70fdc973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Create DataFrame\n",
    "df_embeddings = pd.DataFrame( { chunks, embeddings } )\n"
   ],
   "id": "998cc4a4eb1c122f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Save\n",
   "id": "ca13ccc289f545b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Save\n",
    "df_embeddings.to_parquet( 'public_law_118_32_embeddings.parquet', index=False )\n"
   ],
   "id": "eac78df8b7b91593",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Preview",
   "id": "ae06c79ed3506313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Preview\n",
    "df_embeddings.head( 2 )\n"
   ],
   "id": "e9935e9a89b6aac5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Generate Embeddings\n",
    "- Use a language model (e.g., OpenAI, HuggingFace) to create vector representations of each chunk_words."
   ],
   "id": "f62aab776049a508"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "embeddings = model.encode( chunks, show_progress_bar=True )\n"
   ],
   "id": "a67fd85d25aafd1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Create SQLite Database",
   "id": "d76b8831817cd4d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Design a table that links text chunks to their embeddings.",
   "id": "f06349ccd853f770"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conn = sqlite3.connect( 'vectors.target_values' )\n",
    "cursor = conn.cursor( )\n",
    "sql_create = '''\n",
    "CREATE TABLE IF NOT EXISTS Law_Embeddings\n",
    "(\n",
    "    Id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    Chunk_Tokens TEXT NOT NULL,\n",
    "    Embedding BLOB NOT NULL\n",
    ")\n",
    "'''\n",
    "\n",
    "cursor.execute( sql_create )\n",
    "\n",
    "for chunk, vector in zip( chunks, embeddings ):\n",
    "\tblob = pickle.dumps( vector )\n",
    "\tcursor.execute( 'INSERT INTO Law_Embeddings ( Chunk_Tokens, Embedding ) VALUES (?, ?)',\n",
    "\t\t(chunk, blob) )\n",
    "\n",
    "conn.commit( )\n",
    "conn.close( )\n"
   ],
   "id": "28ca2f108b874b20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  Retrieval (Vector Search in SQLite)",
   "id": "ca5ad81f911f7250"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- You can perform semantic search by encoding a query and comparing via cosine similarity\n",
   "id": "12113e4d3566bd2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def cosine_similarity( a, b ):\n",
    "\treturn np.dot( a, b ) / (np.linalg.norm( a ) * np.linalg.norm( b ))"
   ],
   "id": "70665fe56a643b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query = 'Appropriations for Department of Defense'\n",
    "query_vec = model.encode( [ query ] )[ 0 ]\n",
    "\n",
    "conn = sqlite3.connect( 'vectors.target_values' )\n",
    "cursor = conn.cursor( )\n",
    "cursor.execute( 'SELECT Id, Chunk_Tokens, Embedding FROM Law_Embeddings' )\n",
    "\n",
    "results = [ ]\n",
    "for row in cursor.fetchall( ):\n",
    "\tchunk_id, chunk_text, blob = row\n",
    "\tstored_vec = pickle.loads( blob )\n",
    "\tsim = cosine_similarity( query_vec, stored_vec )\n",
    "\tresults.append( (sim, chunk_text) )\n",
    "\n",
    "# Sort and get top N\n",
    "top_matches = sorted( results, key=lambda x: x[ 0 ], reverse=True )[ :5 ]\n"
   ],
   "id": "675d18dac5bd124d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#   Embedding-Pipeline Script\n",
    "___"
   ],
   "id": "3a7f735556cd0d3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Load Dependencies\n",
    "\n"
   ],
   "id": "e593513dcb937f5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ],
   "id": "cf12b8bc356c9181",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Configuration",
   "id": "99ef339bdfea1fd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a764c56d86acecd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define paths\n",
    "TEXT_FILE = 'PublicLaw_118-42.txt'\n",
    "DB_FILE = 'law_embeddings.target_values'\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n"
   ],
   "id": "e8ee553da46c407c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load and Clean Raw Text",
   "id": "842a900a1b80cde5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_and_clean_text( filepath ):\n",
    "\twith open( filepath, 'r', encoding='utf-8', errors='ignore' ) as file:\n",
    "\t\traw_text = file.read( )\n",
    "\n",
    "\t# Basic normalization\n",
    "\tform_feeds = re.sub( r'\\f+', ' ', raw_text )\n",
    "\tempty_lines = re.sub( r'\\n+', ' ', form_feeds )\n",
    "\textra_spaces = re.sub( r'\\s{2,}', ' ', empty_lines )\n",
    "\treturn extra_spaces\n",
    "\n"
   ],
   "id": "c202cc689865f32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generate Embeddings",
   "id": "ae3ddba5e85404c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_embedding( text, model=OPENAI_MODEL ):\n",
    "\tresponse = openai.Embedding.generate_text( input=text, model=model )\n",
    "\treturn response[ 'target_values'[ 0 ][ 'embedding' ] ]\n",
    "\n",
    "\n",
    "def embed_chunks( chunks ):\n",
    "\tembeddings = [ ]\n",
    "\tfor chunk in tqdm( chunks, desc='EmbeddingRequest chunks via OpenAI' ):\n",
    "\t\ttry:\n",
    "\t\t\tembedding = get_embedding( chunk )\n",
    "\t\t\tembeddings.append( embedding )\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint( f'Error embedding chunk_words: {e}' )\n",
    "\t\t\tembeddings.append( [ 0.0 ] * 1536 )  # Placeholder for failed requests\n",
    "\treturn embeddings\n"
   ],
   "id": "8fa7587e7a3094be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Create SQLite DB\n",
    "\n"
   ],
   "id": "3c985e6d3ecd5e8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_store( chunks, embeddings, db_path ):\n",
    "\tconn = sqlite3.connect( db_path )\n",
    "\tcursor = conn.cursor( )\n",
    "\tsql_create = '''\n",
    "    CREATE TABLE IF NOT EXISTS Law_Embeddings\n",
    "    (\n",
    "        Id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        Chunk_Tokens TEXT NOT NULL,\n",
    "        Embedding BLOB NOT NULL\n",
    "    )\n",
    "    '''\n",
    "\n",
    "\tcursor.execute( sql_create )\n",
    "\tfor chunk, vector in zip( chunks, embeddings ):\n",
    "\t\tblob = pickle.dumps( vector )\n",
    "\t\tsql_insert = 'INSERT INTO Law_Embeddings ( Chunk_Tokens, Embedding ) VALUES ( ?, ? )'\n",
    "\t\tcursor.execute( sql_insert, (chunk, blob) )\n",
    "\n",
    "\tconn.commit( )\n",
    "\tconn.close( )\n"
   ],
   "id": "ea15943e543e0d70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Script",
   "id": "e82751929d4fd563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === MAIN ===\n",
    "def main( ):\n",
    "\tprint( 'Step 1: Load and clean documents' )\n",
    "\tcleaned_text = load_and_clean_text( TEXT_FILE )\n",
    "\n",
    "\tprint( 'Step 2: Chunking documents' )\n",
    "\tchunks = chunk_text( cleaned_text )\n",
    "\tprint( f'Total chunks: {len( chunks )}' )\n",
    "\n",
    "\tprint( 'Step 3: EmbeddingRequest with OpenAI API' )\n",
    "\tembeddings = embed_chunks( chunks )\n",
    "\n",
    "\tprint( 'Step 4: Saving to SQLite' )\n",
    "\tcreate_store( chunks, embeddings, DB_FILE )\n",
    "\n",
    "\tprint( f'Pipeline complete. Embeddings stored in: {DB_FILE}' )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain( )"
   ],
   "id": "5f2d269aaa88212b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Embeddings",
   "id": "de305bd9c7ccbd96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# === 1. Load Model ===\n",
    "# You can try other models like 'all-MiniLM-L6-v2', 'all-mpnet-base-v2', or 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "\n",
    "\n",
    "# === 2. Embed Chunks ===\n",
    "def embed_with_sentence_transformers( texts, model ):\n",
    "\treturn model.encode( texts, show_progress_bar=True, convert_to_numpy=True )\n",
    "\n",
    "\n",
    "local_embeddings = embed_with_sentence_transformers( chunks, model )\n",
    "\n",
    "# === 3. Save in a DataFrame ===\n",
    "df_local = pd.DataFrame(\n",
    "{\n",
    "\t'chunk_words': chunks,\n",
    "\t'embedding': list( local_embeddings )  # numpy arrays to a list for DataFrame compatibility\n",
    "} )\n",
    "\n",
    "# === 4. Save to Disk ===\n",
    "df_local.to_parquet( 'public_law_118_32_local_embeddings.parquet', index=False )\n",
    "\n",
    "# === 5. Preview ===\n",
    "df_local.head( 2 )\n",
    "\n"
   ],
   "id": "47df028a12155604",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine-Tuning",
   "id": "e1efd28f65f33d39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n"
   ],
   "id": "51f033fc98145091",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "client = openai.OpenAI(\n",
    "\tapi_key=os.environ.get( 'OPENAI_API_KEY' ),\n",
    "\torganization='<org id>',\n",
    "\tproject='<project id>',\n",
    ")"
   ],
   "id": "b11d00974dbaa480",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read in the dataset we'll use for this task.\n",
    "# This will be the RecipesNLG dataset, which we've cleaned_lines to only contain documents from www.cookbooks.com\n",
    "recipe_df = pd.read_csv( r'C:\\Users\\terry\\Desktop\\cookbook_recipes_nlg_10k.csv' )\n",
    "recipe_df.head( )"
   ],
   "id": "aa84f0c255158f4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "system_message = 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'\n",
    "\n",
    "def create_user_message( row ):\n",
    "\treturn f'Title: {row[ 'title' ]}\\n\\nIngredients: {row[ 'ingredients' ]}\\n\\nGeneric ingredients: '\n",
    "\n",
    "\n",
    "def prepare_example_conversation( row ):\n",
    "\treturn \\\n",
    "\t{\n",
    "\t\t'messages': [\n",
    "\t\t{\n",
    "\t\t\t'role': 'system',\n",
    "\t\t\t'content': system_message\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t'role': 'user',\n",
    "\t\t\t'content': create_user_message( row )\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t'role': 'assistant',\n",
    "\t\t\t'content': row[ 'NER' ]\n",
    "\t\t}, ]\n",
    "\t}\n",
    "\n"
   ],
   "id": "d29e9579f6f0e9dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use the first 100 rows of the dataset for training\n",
    "training_df = recipe_df.loc[ 0:100 ]\n",
    "\n",
    "# apply the prepare_example_conversation function to each row of the training_df\n",
    "training_data = training_df.apply( prepare_example_conversation, axis=1 ).tolist( )\n",
    "\n",
    "for example in training_data[ :5 ]:\n",
    "\tprint( example )"
   ],
   "id": "23bdc89006166778",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "validation_df = recipe_df.loc[ 101:200 ]\n",
    "validation_data = validation_df.apply(\n",
    "\tprepare_example_conversation, axis=1 ).tolist( )"
   ],
   "id": "4773228f51c07750",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3896c43cec6862e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def write_jsonl( data: List[ Dict ], filename: str ) -> None:\n",
    "\twith open( filename, 'w' ) as out:\n",
    "\t\tfor kvp in data:\n",
    "\t\t\tjout = json.dumps( kvp ) + '\\n'\n",
    "\t\t\tout.write( jout )"
   ],
   "id": "6dcf445d39eee404",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "652e2134d8966009"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "validation_file_name = 'tmp_recipe_finetune_validation.jsonl'\n",
    "write_jsonl( validation_data, validation_file_name )"
   ],
   "id": "a492a14231147287",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "170bad89408062ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def upload_file( file_name: str, purpose: str ) -> str:\n",
    "\twith open( file_name, 'rb' ) as file_fd:\n",
    "\t\tresponse = client.files.create( file=file_fd, purpose=purpose )\n",
    "\treturn response.id"
   ],
   "id": "10f941082775954c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "751aceb620d9c7bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c8dbd2395ef59d03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MODEL = 'openai-4o-mini-2024-07-18'\n",
    "\n",
    "response = client.fine_tuning.jobs.create(\n",
    "\ttraining_file=training_file_id,\n",
    "\tvalidation_file=validation_file_id,\n",
    "\tmodel=MODEL,\n",
    "\tsuffix='recipe-ner',\n",
    ")\n",
    "\n",
    "job_id = response.id"
   ],
   "id": "5ca162663d00cad3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Text Cleaning Pipeline",
   "id": "dcabaa53b3a578c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1dd53ce7908a5042",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "f8d915c3f2d4b28f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "af2bd8b593c87d8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range( 10 ):\n",
    "\tprint( lines[ i ] )"
   ],
   "id": "471b42f7fd2f8492",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "new = r'C:\\Users\\terry\\Desktop\\Text\\Chunked'  + '\\\\' + filename\n",
    "folder = open( new, 'wt+' )\n",
    "processed = [ ]\n",
    "for i, c in enumerate( lines ):\n",
    "\tpart = ' '.join( c )\n",
    "\tline = '{ ' + f'\"{i}\"' + ' : ' + '\"' + part + '\"' + ' },' + '\\r'\n",
    "\tprocessed.append( line )\n",
    "\n",
    "for line in processed:\n",
    "\tfolder.write( line )\n",
    "\n",
    "folder.close( )"
   ],
   "id": "feb3b69467ab7c9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7e63e87a30d13ac0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "42c39b663224ebcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b185730909988566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Clean Files",
   "id": "5d4c2f3eb0bdedf4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_files( src: str, dest: str ) -> None:\n",
    "\ttry:\n",
    "\t\tif src is None:\n",
    "\t\t\traise Exception( 'The argument \"src\" is required.' )\n",
    "\t\telif dest is None:\n",
    "\t\t\traise Exception( 'The argument \"dest\" is required.' )\n",
    "\t\telse:\n",
    "\t\t\tsource = src\n",
    "\t\t\tdestination = dest\n",
    "\t\t\tfiles = os.listdir( source )\n",
    "\t\t\tfor f in files:\n",
    "\t\t\t\tprocessed = [ ]\n",
    "\t\t\t\tfilename = os.path.basename( f )\n",
    "\t\t\t\tsource_path = source + '\\\\' + filename\n",
    "\t\t\t\ttext = open( source_path, 'r', encoding='utf-8', errors='ignore' ).read( )\n",
    "\t\t\t\tsentences = split_sentences( text )\n",
    "\t\t\t\tfor s in sentences:\n",
    "\t\t\t\t\tif s != \" \":\n",
    "\t\t\t\t\t\tlower = s.lower( )\n",
    "\t\t\t\t\t\tspecial = remove_special( lower )\n",
    "\t\t\t\t\t\tspace = clean_space( special )\n",
    "\t\t\t\t\t\tprocessed.append( space )\n",
    "\n",
    "\t\t\t\tdest_path = destination + '\\\\' + filename\n",
    "\t\t\t\tclean = open( dest_path, 'wt', encoding='utf-8', errors='ignore' )\n",
    "\t\t\t\tlines = ' '.join( processed )\n",
    "\t\t\t\tclean.write( lines )\n",
    "\t\t\t\tclean.flush( )\n",
    "\texcept Exception as e:\n",
    "\t\tprint( \"The 'clean_files' function raised an exception:\", e )"
   ],
   "id": "55c395d65e61a4d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Clean Text Files",
   "id": "d63131aafd51ce14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_text_files( src: str, dest: str ) -> None:\n",
    "\ttry:\n",
    "\t\tif src is None:\n",
    "\t\t\traise Exception( 'The argument \"src\" is required.' )\n",
    "\t\telif dest is None:\n",
    "\t\t\traise Exception( 'The argument \"dest\" is required.' )\n",
    "\t\telse:\n",
    "\t\t\tsource = src\n",
    "\t\t\tdestination = dest\n",
    "\t\t\tfiles = os.listdir( source )\n",
    "\t\t\tkeepers = [ '$', 'in', '(', ')', '', 'the', '. ', ': ', '; ', 'and', 'but', 'be', 'was', 'what', 'for' ]\n",
    "\t\t\tfor f in files:\n",
    "\t\t\t\tprocessed = [ ]\n",
    "\t\t\t\tfilename = os.path.basename( f )\n",
    "\t\t\t\tsource_path = source + '\\\\' + filename\n",
    "\t\t\t\ttext = open( source_path, 'r', encoding='utf-8', errors='ignore' ).read( )\n",
    "\t\t\t\tcollapse = text.replace( '\\n', ' ' )\n",
    "\t\t\t\tnormal = normalize( collapse )\n",
    "\t\t\t\tsentences = normal.splitlines( )\n",
    "\t\t\t\tfor s in sentences:\n",
    "\t\t\t\t\tif s != \" \" or s in keepers:\n",
    "\t\t\t\t\t\tlower = s.lower( )\n",
    "\t\t\t\t\t\tspecial = remove_special( lower )\n",
    "\t\t\t\t\t\tprocessed.append( special )\n",
    "\n",
    "\t\t\t\tdest_path = destination + '\\\\' + filename\n",
    "\t\t\t\tclean = open( dest_path, 'wt', encoding='utf-8', errors='ignore' )\n",
    "\t\t\t\tlines = ' '.join( processed )\n",
    "\t\t\t\tfinal = clean_text( lines )\n",
    "\t\t\t\tclean.write( final )\n",
    "\t\t\t\tclean.flush( )\n",
    "\texcept Exception as e:\n",
    "\t\tprint( \"The 'clean_files' function raised an exception:\", e )\n",
    "\n"
   ],
   "id": "bdf217e7d2103cac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Chunk Files",
   "id": "1ce62e73b57e03f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def chunk_files( src: str, dest: str ) -> None:\n",
    "\ttry:\n",
    "\t\tif src is None:\n",
    "\t\t\traise Exception( 'The argument \"src\" is required.' )\n",
    "\t\telif dest is None:\n",
    "\t\t\traise Exception( 'The argument \"dest\" is required.' )\n",
    "\t\telse:\n",
    "\t\t\tsource = src\n",
    "\t\t\tdestination = dest\n",
    "\t\t\tfiles = os.listdir( source )\n",
    "\t\t\tfor f in files:\n",
    "\t\t\t\tprocessed = [ ]\n",
    "\t\t\t\tfilename = os.path.basename( f )\n",
    "\t\t\t\tname = filename.replace( '.txt', '.jsonl' )\n",
    "\t\t\t\tsource_path = source + '\\\\' + filename\n",
    "\t\t\t\ttext = open( source_path, 'r', encoding='utf-8', errors='ignore' ).read( )\n",
    "\t\t\t\tsentences = split_sentences( text )\n",
    "\t\t\t\tfragments = remove_fragments( sentences )\n",
    "\t\t\t\tchunks = chunk_pages( fragments )\n",
    "\t\t\t\tfor i, c in enumerate( chunks ):\n",
    "\t\t\t\t\twords = ''.join( c ).strip( )\n",
    "\t\t\t\t\tline = '{' + f'\"Line-{i}\"' + ' : \"' + words + '\"},\\r'\n",
    "\t\t\t\t\tprocessed.append( line )\n",
    "\n",
    "\t\t\t\tdest_path = destination + '\\\\' + name\n",
    "\t\t\t\tclean = open( dest_path, 'wt', encoding='utf-8', errors='ignore' )\n",
    "\t\t\t\tfor p in processed:\n",
    "\t\t\t\t\tclean.write( p )\n",
    "\t\t\t\tclean.flush( )\n",
    "\texcept Exception as e:\n",
    "\t\tprint( \"The 'chunk_files' function raised an exception:\", e )\n"
   ],
   "id": "6110fa9a029e7d7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f6bf516d1034c3f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "chunk_files( src, dest )",
   "id": "97c806612288ddfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine-Tuning Pipeline",
   "id": "7c5ddae1c859c071"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Datasets",
   "id": "9fa08cfe33d29524"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Datasets\n",
    "omb = r'C:\\Users\\terry\\Desktop\\AI\\fine-tuning\\datasets\\omb.xlsx'\n",
    "cfr31 = r'C:\\Users\\terry\\Desktop\\AI\\fine-tuning\\datasets\\cfr31.xlsx'\n",
    "fastbook = r'C:\\Users\\terry\\Desktop\\AI\\fine-tuning\\datasets\\fastbook.xlsx'\n",
    "redbook = r'C:\\Users\\terry\\Desktop\\AI\\fine-tuning\\datasets\\redbook.xlsx'\n",
    "ledger = r'C:\\Users\\terry\\Desktop\\AI\\fine-tuning\\datasets\\ledger.xlsx'"
   ],
   "id": "e069c9fcfcbbd2bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### System Instructions",
   "id": "abab952ca12e7aae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set System Instructions\n",
    "instructions = '''You are the most knowledgeable Budget Analyst in the federal government who provides detailed responses based on your vast knowledge of budget legislation, and federal appropriations. Your responses to questions about federal finance are complete, transparent, and very detailed using an academic format. Your vast knowledge of and experience in Data Science makes you the best Data Analyst in the world. You are also an expert programmer who is proficient in C#, Python, S L, C++, JavaScript, and VBA. You are famous for the accuracy of your responses so you verify all your answers. This makes the quality of your code very high and it always works. Your responses are always accurate and complete! Your name is Bubba.\n",
    "'''"
   ],
   "id": "47efa7ddc63591f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Messages\n",
   "id": "e95d6e2892e0c0c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set Messages\n",
    "system = '{\"messages\":[{' + f'\"role\":\"system\", \"content\":\"{instructions}\"' + '},{'\n",
    "initial = system + f' \"role\":\"user\", \"content\":\"{Q}\"' + '},{'\n",
    "question = '{\"messages\":[{' + f'\"role\": \"user\",  \"content\":\"{Q}\"' + '},{'\n",
    "answer = f' \"role\":\"assistant\", \"content\":\"{A}\"' + '}]}'\n"
   ],
   "id": "80b9dc49e08e15ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print JSON\n",
    "\n",
    "for r in range( 25 ):\n",
    "\tquestion = '{\"messages\":[{' + f'\"role\":\"user\", \"content\":\"{df_ledger.iloc[ r, 2 ]}\"' + '},{'\n",
    "\tanswer = f'\"role\":\"assistant\", \"content\": \"{df_ledger.iloc[ r, 3 ]}\" ' + '}]}'\n",
    "\trecord = question + answer\n",
    "\tprint( record  )"
   ],
   "id": "314e241553d70eb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### A-11 Data",
   "id": "15adfd8026079931"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define A-11 Data\n",
    "xl_a11 = pd.read_excel( omb, sheet_name='Training' )\n",
    "names = [ 'ID', 'Item', 'Role', 'Content' ]\n",
    "a11_idx = xl_a11.index\n",
    "df_a11 = pd.DataFrame( data=xl_a11, columns=names  )\n",
    "df_a11 = df_a11.reset_index( ).set_index( 'ID' )\n",
    "df_a11 = df_a11.drop( columns=[ 'index' ]  )\n",
    "omb_rows = len( df_a11  )\n",
    "omb_rows"
   ],
   "id": "d4c2b57481a2bd0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# View Dataframe\n",
    "df_a11"
   ],
   "id": "4da849b5db0be898",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print JSON\n",
    "for r in range( len( df_a11 )):\n",
    "\trow = df_a11.iloc[ r, : ]\n",
    "\tif row[ 'Item' ] == 'Q':\n",
    "\t\tquestion = '{\"messages\":[{' + f'\"role\":\"{row[ 'Role' ]}\", \"content\":\"{row[ 'Content' ]}\"' + '},{'\n",
    "\telif  row[ 'Item' ] == 'A':\n",
    "\t\tanswer = f'\"role\":\"{row[ 'Role' ]}\", \"content\": \"{row[ 'Content' ]}\"' + '}]}'\n",
    "\t\trecord = question + answer\n",
    "\tprint( record )"
   ],
   "id": "4c71dc465c8d6cd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "afcdf44e412f48f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### CFR Data",
   "id": "91397e6e2f827a75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define CFR Data\n",
    "xl_cfr31 = pd.read_excel( cfr31, sheet_name='Training' )\n",
    "names = [ 'ID', 'Item', 'Role', 'Content' ]\n",
    "cfr_index = xl_cfr31.index\n",
    "df_cfr31 = pd.DataFrame( data=xl_cfr31, columns=names  )\n",
    "df_cfr31 = df_cfr31.reset_index( ).set_index( 'ID' )\n",
    "df_cfr31 = df_cfr31.drop( columns=[ 'index' ] )\n",
    "cfr_rows = len( df_cfr31 )\n",
    "cfr_rows"
   ],
   "id": "b84b674d7baf6715",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# View Dataframe\n",
    "df_cfr31"
   ],
   "id": "e1945d3078cd47cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print JSON\n",
    "for r in range( len( df_cfr31 ) ):\n",
    "\tif df_cfr31.iloc[ r, 1 ].startswith( 'u' ):\n",
    "\t\tquestion = '{\"messages\":[{' + f'\"role\":\"{df_cfr31.iloc[ r, 1 ]}\", \"content\":\"{df_cfr31.iloc[ r, 2 ]}\"' + '},{'\n",
    "\telif df_cfr31.iloc[ r, 1 ].startswith( 'a' ):\n",
    "\t\tanswer = f'\"role\":\"{df_cfr31.iloc[ r, 1 ]}\", \"content\": \"{df_cfr31.iloc[ r, 2 ]}\"' + '}]}'\n",
    "\trecord = question + answer\n",
    "\tprint( record  )\n",
    "\n"
   ],
   "id": "8c98f202b7b842be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Redbook Data",
   "id": "b3358b800bcaedbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Redbook Data\n",
    "xl_redbook = pd.read_excel( redbook, sheet_name='Training' )\n",
    "names = [ 'ID', 'Item', 'Role', 'Content' ]\n",
    "red_inx = xl_redbook.index\n",
    "df_redbook = pd.DataFrame( data=xl_redbook, columns=names )\n",
    "df_redbook = df_redbook.reset_index( ).set_index( 'ID' )\n",
    "df_redbook = df_redbook.drop( columns=[ 'index' ] )\n",
    "red_rows = len( df_redbook )\n",
    "red_rows"
   ],
   "id": "a9c71f5ebcb9cf7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# View Dataframe\n",
    "df_redbook"
   ],
   "id": "c6fe2814e474853",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print JSON\n",
    "for r in range( len( df_redbook ) ):\n",
    "\tif df_redbook.iloc[ r, 1 ].startswith( 'u' ):\n",
    "\t\tquestion = '{\"messages\":[{' + f'\"role\":\"{df_redbook.iloc[ r, 1 ]}\", \"content\":\"{df_redbook.iloc[ r, 2 ]}\"' + '},{'\n",
    "\telif df_redbook.iloc[ r, 1 ].startswith( 'a' ):\n",
    "\t\tanswer = f'\"role\":\"{df_redbook.iloc[ r, 1 ]}\", \"content\": \"{df_redbook.iloc[ r, 2 ]}\" ' + '}]}'\n",
    "\trecord = question + answer\n",
    "\tprint( record  )"
   ],
   "id": "2c4a931e40a9f48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Ledger Data",
   "id": "ba25e74e31b22923"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Ledger Data\n",
    "xl_ledger = pd.read_excel( ledger, sheet_name='Training' )\n",
    "names = [ 'ID', 'Item', 'Role', 'Content' ]\n",
    "ldgr_inx = xl_ledger.index\n",
    "df_ledger = pd.DataFrame( data=xl_ledger, columns=names, index=ldgr_inx  )\n",
    "df_ledger = df_ledger.reset_index(  ).set_index( 'ID' )\n",
    "df_ledger = df_ledger.drop( columns=[ 'index' ] )\n",
    "ledger_rows = len( df_ledger )\n",
    "ledger_rows"
   ],
   "id": "46e83d4d44129c27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# View Dataframe\n",
    "df_ledger"
   ],
   "id": "249580b9e0ab02c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in df_ledger.columns:\n",
    "\tprint( i )"
   ],
   "id": "9da484b01968d9c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print JSON\n",
    "for r in range( len( df_ledger ) ):\n",
    "\tif df_ledger.iloc[ r, 1 ].startswith( 'u' ):\n",
    "\t\tquestion = '{\"messages\":[{' + f'\"role\":\"{df_ledger.iloc[ r, 1 ]}\", \"content\":\"{df_ledger.iloc[ r, 2 ]}\"' + '},{'\n",
    "\telif df_cfr31.iloc[ r, 1 ].startswith( 'a' ):\n",
    "\t\tanswer = f'\"role\":\"{df_ledger.iloc[ r, 1 ]}\", \"content\": \"{df_ledger.iloc[ r, 2 ]}\" ' + '}]}'\n",
    "\trecord = question + answer\n",
    "\tprint( record  )"
   ],
   "id": "f6483f7f77ed048e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cbcae77914e300a9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
