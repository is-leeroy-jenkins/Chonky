{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NLTK\n",
    "___\n"
   ],
   "id": "700c6019c3802579"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ðŸ“¦ One-Time Setup (NLTK Resources)",
   "id": "17b64a5a35525229"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "3957678e92123d8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T14:22:02.611264Z",
     "start_time": "2025-10-24T14:21:55.343326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import html\n",
    "import ipywidgets as widgets, IPython, platform, ipywidgets, jupyterlab\n",
    "from importlib import reload\n",
    "import io\n",
    "from isort.format import remove_whitespace\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sqlite3\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import string\n",
    "import time\n",
    "from textblob import Word, TextBlob\n",
    "import unicodedata"
   ],
   "id": "9a5c2c52dd74d494",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T14:22:31.957189Z",
     "start_time": "2025-10-24T14:22:31.902470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import processing as pc\n",
    "reload( pc )\n",
    "from processing import Text\n"
   ],
   "id": "cbed53edd93bfdca",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T14:22:33.698835Z",
     "start_time": "2025-10-24T14:22:33.693282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fp = r'C:\\Users\\terry\\Desktop\\Test\\Text\\Balanced Budget and Emergency Deficit Control Act of 1985.txt'\n",
    "src = r'C:/Users/terry/Desktop/Test/Cleaned/'\n",
    "dst = r'C:/Users/terry/Desktop/Test/Chunked/'\n",
    "tx = Text( )"
   ],
   "id": "4c4a3094edae0ea",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:27:38.360563Z",
     "start_time": "2025-10-24T13:27:38.087833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = tx.load_text( fp )\n",
    "collapsed = tx.collapse_whitespace( text )\n",
    "compressed = tx.compress_whitespace( collapsed )\n",
    "normalized = tx.normalize_text( compressed )\n",
    "encoded = tx.remove_encodings( normalized )\n",
    "special = tx.remove_special( encoded )\n",
    "cleaned = tx.remove_fragments( special )\n",
    "recompress = tx.compress_whitespace( cleaned )\n",
    "dataframe = tx.split_sentences( recompress )"
   ],
   "id": "96a2c3175c549513",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T14:22:42.197094Z",
     "start_time": "2025-10-24T14:22:36.962374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cleaned = tx.clean_file( fp )\n",
    "sentences = tx.split_sentences( cleaned )\n",
    "items = ' '.join( sentences )\n",
    "tokens = items.split( None )\n",
    "frequency = tx.calculate_frequency_distribution( tokens )\n",
    "frequency\n"
   ],
   "id": "7f527d0a2ff69006",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             Word  Frequency\n",
       "ID                          \n",
       "0        balanced         14\n",
       "1          budget        369\n",
       "2       emergency         20\n",
       "3         deficit         79\n",
       "4         control         38\n",
       "...           ...        ...\n",
       "1345    concurred          2\n",
       "1346       others          1\n",
       "1347        ments          1\n",
       "1348     insisted          1\n",
       "1349  compilation          1\n",
       "\n",
       "[1350 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>balanced</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>budget</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emergency</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deficit</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>control</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>concurred</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>others</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>ments</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>insisted</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>compilation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1350 rows Ã— 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.  Load File",
   "id": "36fbc578426999c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Load Raw Text ===\n",
    "file_path = '<url to file>'\n",
    "_rawtext = ''\n",
    "\n",
    "\n",
    "def load_text( file_path ):\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t\t_rawtext = f.read( )\n",
    "\t\treturn _rawtext\n"
   ],
   "id": "e3d5cc42b6dd4e3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ðŸ§® 1. Bag of Words (BoW) using CountVectorizer",
   "id": "93554b0db878266a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\tcorpus = [ 'Bro loves clean code.', 'Code is life.' ]\n",
    "\tvectorizer = CountVectorizer( )\n",
    "\tX = vectorizer.fit_transform( corpus )\n",
    "\n",
    "\tprint( vectorizer.get_feature_names_out( ) )\n",
    "\tprint( X.toarray( ) )\n"
   ],
   "id": "df2a01ee327d57d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ðŸ“Š 2. TF-IDF using TfidfVectorizer",
   "id": "606ca62c6f083d04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\tcorpus = [ 'Bro writes awesome code.', 'Code must be clean and clear.' ]\n",
    "\tvectorizer = TfidfVectorizer( )\n",
    "\tX = vectorizer.fit_transform( corpus )\n",
    "\n",
    "\tprint( vectorizer.get_feature_names_out( ) )\n",
    "\tprint( X.toarray( ) )\n"
   ],
   "id": "58a4cf4ec025e261",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ðŸ§  3. Word2Vec using gensim",
   "id": "cd392a973450c93a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\tsentences = [ [ 'bro', 'loves', 'python' ], [ 'clean', 'code', 'rocks' ] ]\n",
    "\tmodel = Word2Vec( sentences, vector_size=100, window=5, min_count=1, workers=4 )\n",
    "\n",
    "\t# VectorStore for the word 'bro'\n",
    "\tvector = model.wv[ 'bro' ]\n",
    "\tprint( vector )\n"
   ],
   "id": "a20f65018fd9c119",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ðŸŒ 4. GloVe using gensim (with pre-trained vectors)\n",
   "id": "a920cc11594a37ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\t# Load GloVe vec (convert .txt to .word2vec format beforehand if needed)\n",
    "\tglove_file = r'C:\\Users\\terry\\source\\llm\\glove\\glove.6B.100d.txt'\n",
    "\tmodel = KeyedVectors.load_word2vec_format( glove_file, unicode_errors='ignore' )\n",
    "\n",
    "\t# VectorStore for the word 'code'\n",
    "\tvector = model[ 'code' ]\n",
    "\tprint( vector )\n"
   ],
   "id": "95c58a17aeefed99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ðŸ¤– 5. BERT / Transformer-based Embeddings using transformers + torch\n",
   "id": "8117a17e891dc4c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\ttokenizer = BertTokenizer.from_pretrained( 'bert-base-uncased' )\n",
    "\tmodel = BertModel.from_pretrained( 'bert-base-uncased' )\n",
    "\n",
    "\tsentence = \"Bro's code always works.\"\n",
    "\tinputs = tokenizer( sentence, return_tensors='pt' )\n",
    "\toutputs = model( **inputs )\n",
    "\n",
    "\t# Get the vector for [CLS] token (sentence embedding)\n",
    "\tsentence_embedding = outputs.last_hidden_state[ :, 0, : ]\n",
    "\tprint( sentence_embedding.shape )\n"
   ],
   "id": "8a9d362295bf0536",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean Document",
   "id": "5e61d59191d25e5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_text( text: str ) -> str:\n",
    "\ttext = text.replace( '\\r\\n', '\\n' ).replace( '\\r', '\\n' )\n",
    "\ttext = re.sub( r'\\n\\s*\\d+\\s*\\n', '\\n', text )\n",
    "\ttext = re.sub( r'(\\w+)-\\n(\\w+)', r'\\1\\2', text )\n",
    "\ttext = re.sub( r'(?<!\\n)\\n(?![\\n])', ' ', text )\n",
    "\ttext = re.sub( r'\\s+', ' ', text )\n",
    "\treturn text.strip( )\n"
   ],
   "id": "42c34418a1218398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fc9d08f36dde47ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "#  OpenAI Embedding\n",
    "___"
   ],
   "id": "9f097164e0f1b335"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### API key",
   "id": "10f8b0039c619d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )"
   ],
   "id": "6f7c70b192899b4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Define embedding function",
   "id": "e1c76c3548b771af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def embed_texts( texts, model='text-embedding-3-small', batch_size=10, sleep=1 ):\n",
    "\tembeddings = [ ]\n",
    "\tfor i in range( 0, len( texts ), batch_size ):\n",
    "\t\tbatch = texts[ i:i + batch_size ]\n",
    "\t\ttry:\n",
    "\t\t\tresponse = openai.embeddings.create( input=batch, model=model )\n",
    "\t\t\tbatch_embeddings = [ e.embedding for e in response.data ]\n",
    "\t\t\tembeddings.extend( batch_embeddings )\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint( f'Error at batch {i}: {e}' )\n",
    "\t\t\t# Retry or sleep to avoid rate limits\n",
    "\t\t\ttime.sleep( sleep )\n",
    "\t\t\tcontinue\n",
    "\n",
    "\treturn embeddings\n"
   ],
   "id": "3d19dce1b72312ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Embed chunks",
   "id": "7a1cbc3787a81ed2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Embed chunks\n",
    "embeddings = embed_texts( chunks )"
   ],
   "id": "2e8dd3b9c7453b0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.  Create DataFrame",
   "id": "7b29c98e70fdc973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Create DataFrame\n",
    "df_embeddings = pd.DataFrame( { chunks, embeddings } )\n"
   ],
   "id": "998cc4a4eb1c122f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Save\n",
   "id": "ca13ccc289f545b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Save\n",
    "df_embeddings.to_parquet( 'public_law_118_32_embeddings.parquet', index=False )\n"
   ],
   "id": "eac78df8b7b91593",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Preview",
   "id": "ae06c79ed3506313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Preview\n",
    "df_embeddings.head( 2 )\n"
   ],
   "id": "e9935e9a89b6aac5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Generate Embeddings\n",
    "- Use a language model (e.g., OpenAI, HuggingFace) to create vector representations of each chunk_words."
   ],
   "id": "f62aab776049a508"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "embeddings = model.encode( chunks, show_progress_bar=True )\n"
   ],
   "id": "a67fd85d25aafd1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Create SQLite Database",
   "id": "d76b8831817cd4d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Design a table that links text chunks to their embeddings.",
   "id": "f06349ccd853f770"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conn = sqlite3.connect( 'vectors.target_values' )\n",
    "cursor = conn.cursor( )\n",
    "sql_create = '''\n",
    "CREATE TABLE IF NOT EXISTS Law_Embeddings\n",
    "(\n",
    "    Id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    Chunk_Tokens TEXT NOT NULL,\n",
    "    Embedding BLOB NOT NULL\n",
    ")\n",
    "'''\n",
    "\n",
    "cursor.execute( sql_create )\n",
    "\n",
    "for chunk, vector in zip( chunks, embeddings ):\n",
    "\tblob = pickle.dumps( vector )\n",
    "\tcursor.execute( 'INSERT INTO Law_Embeddings ( Chunk_Tokens, Embedding ) VALUES (?, ?)',\n",
    "\t\t(chunk, blob) )\n",
    "\n",
    "conn.commit( )\n",
    "conn.close( )\n"
   ],
   "id": "28ca2f108b874b20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  Retrieval (Vector Search in SQLite)",
   "id": "ca5ad81f911f7250"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- You can perform semantic search by encoding a query and comparing via cosine similarity\n",
   "id": "12113e4d3566bd2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def cosine_similarity( a, b ):\n",
    "\treturn np.dot( a, b ) / (np.linalg.norm( a ) * np.linalg.norm( b ))"
   ],
   "id": "70665fe56a643b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query = 'Appropriations for Department of Defense'\n",
    "query_vec = model.encode( [ query ] )[ 0 ]\n",
    "\n",
    "conn = sqlite3.connect( 'vectors.target_values' )\n",
    "cursor = conn.cursor( )\n",
    "cursor.execute( 'SELECT Id, Chunk_Tokens, Embedding FROM Law_Embeddings' )\n",
    "\n",
    "results = [ ]\n",
    "for row in cursor.fetchall( ):\n",
    "\tchunk_id, chunk_text, blob = row\n",
    "\tstored_vec = pickle.loads( blob )\n",
    "\tsim = cosine_similarity( query_vec, stored_vec )\n",
    "\tresults.append( (sim, chunk_text) )\n",
    "\n",
    "# Sort and get top N\n",
    "top_matches = sorted( results, key=lambda x: x[ 0 ], reverse=True )[ :5 ]\n"
   ],
   "id": "675d18dac5bd124d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#   Embedding-Pipeline Script\n",
    "___"
   ],
   "id": "3a7f735556cd0d3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Load Dependencies\n",
    "\n"
   ],
   "id": "e593513dcb937f5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ],
   "id": "cf12b8bc356c9181",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Configuration",
   "id": "99ef339bdfea1fd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a764c56d86acecd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define paths\n",
    "TEXT_FILE = 'PublicLaw_118-42.txt'\n",
    "DB_FILE = 'law_embeddings.target_values'\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n"
   ],
   "id": "e8ee553da46c407c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load and Clean Raw Text",
   "id": "842a900a1b80cde5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_and_clean_text( filepath ):\n",
    "\twith open( filepath, 'r', encoding='utf-8', errors='ignore' ) as file:\n",
    "\t\traw_text = file.read( )\n",
    "\n",
    "\t# Basic normalization\n",
    "\tform_feeds = re.sub( r'\\f+', ' ', raw_text )\n",
    "\tempty_lines = re.sub( r'\\n+', ' ', form_feeds )\n",
    "\textra_spaces = re.sub( r'\\s{2,}', ' ', empty_lines )\n",
    "\treturn extra_spaces\n",
    "\n"
   ],
   "id": "c202cc689865f32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generate Embeddings",
   "id": "ae3ddba5e85404c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_embedding( text, model=OPENAI_MODEL ):\n",
    "\tresponse = openai.Embedding.generate_text( input=text, model=model )\n",
    "\treturn response[ 'target_values'[ 0 ][ 'embedding' ] ]\n",
    "\n",
    "\n",
    "def embed_chunks( chunks ):\n",
    "\tembeddings = [ ]\n",
    "\tfor chunk in tqdm( chunks, desc='EmbeddingRequest chunks via OpenAI' ):\n",
    "\t\ttry:\n",
    "\t\t\tembedding = get_embedding( chunk )\n",
    "\t\t\tembeddings.append( embedding )\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint( f'Error embedding chunk_words: {e}' )\n",
    "\t\t\tembeddings.append( [ 0.0 ] * 1536 )  # Placeholder for failed requests\n",
    "\treturn embeddings\n"
   ],
   "id": "8fa7587e7a3094be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Create SQLite DB\n",
    "\n"
   ],
   "id": "3c985e6d3ecd5e8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_store( chunks, embeddings, db_path ):\n",
    "\tconn = sqlite3.connect( db_path )\n",
    "\tcursor = conn.cursor( )\n",
    "\tsql_create = '''\n",
    "    CREATE TABLE IF NOT EXISTS Law_Embeddings\n",
    "    (\n",
    "        Id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        Chunk_Tokens TEXT NOT NULL,\n",
    "        Embedding BLOB NOT NULL\n",
    "    )\n",
    "    '''\n",
    "\n",
    "\tcursor.execute( sql_create )\n",
    "\tfor chunk, vector in zip( chunks, embeddings ):\n",
    "\t\tblob = pickle.dumps( vector )\n",
    "\t\tsql_insert = 'INSERT INTO Law_Embeddings ( Chunk_Tokens, Embedding ) VALUES ( ?, ? )'\n",
    "\t\tcursor.execute( sql_insert, (chunk, blob) )\n",
    "\n",
    "\tconn.commit( )\n",
    "\tconn.close( )\n"
   ],
   "id": "ea15943e543e0d70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Script",
   "id": "e82751929d4fd563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === MAIN ===\n",
    "def main( ):\n",
    "\tprint( 'Step 1: Load and clean documents' )\n",
    "\tcleaned_text = load_and_clean_text( TEXT_FILE )\n",
    "\n",
    "\tprint( 'Step 2: Chunking documents' )\n",
    "\tchunks = chunk_text( cleaned_text )\n",
    "\tprint( f'Total chunks: {len( chunks )}' )\n",
    "\n",
    "\tprint( 'Step 3: EmbeddingRequest with OpenAI API' )\n",
    "\tembeddings = embed_chunks( chunks )\n",
    "\n",
    "\tprint( 'Step 4: Saving to SQLite' )\n",
    "\tcreate_store( chunks, embeddings, DB_FILE )\n",
    "\n",
    "\tprint( f'Pipeline complete. Embeddings stored in: {DB_FILE}' )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain( )"
   ],
   "id": "5f2d269aaa88212b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Embeddings",
   "id": "de305bd9c7ccbd96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# === 1. Load Model ===\n",
    "# You can try other models like 'all-MiniLM-L6-v2', 'all-mpnet-base-v2', or 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "\n",
    "\n",
    "# === 2. Embed Chunks ===\n",
    "def embed_with_sentence_transformers( texts, model ):\n",
    "\treturn model.encode( texts, show_progress_bar=True, convert_to_numpy=True )\n",
    "\n",
    "\n",
    "local_embeddings = embed_with_sentence_transformers( chunks, model )\n",
    "\n",
    "# === 3. Save in a DataFrame ===\n",
    "df_local = pd.DataFrame(\n",
    "{\n",
    "\t'chunk_words': chunks,\n",
    "\t'embedding': list( local_embeddings )  # numpy arrays to a list for DataFrame compatibility\n",
    "} )\n",
    "\n",
    "# === 4. Save to Disk ===\n",
    "df_local.to_parquet( 'public_law_118_32_local_embeddings.parquet', index=False )\n",
    "\n",
    "# === 5. Preview ===\n",
    "df_local.head( 2 )\n",
    "\n"
   ],
   "id": "47df028a12155604",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine-Tuning",
   "id": "e1efd28f65f33d39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n"
   ],
   "id": "51f033fc98145091",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "client = openai.OpenAI(\n",
    "\tapi_key=os.environ.get( 'OPENAI_API_KEY' ),\n",
    "\torganization='<org id>',\n",
    "\tproject='<project id>',\n",
    ")"
   ],
   "id": "b11d00974dbaa480",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read in the dataset we'll use for this task.\n",
    "# This will be the RecipesNLG dataset, which we've cleaned_lines to only contain documents from www.cookbooks.com\n",
    "recipe_df = pd.read_csv( r'C:\\Users\\terry\\Desktop\\cookbook_recipes_nlg_10k.csv' )\n",
    "recipe_df.head( )"
   ],
   "id": "aa84f0c255158f4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "system_message = 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'\n",
    "\n",
    "def create_user_message( row ):\n",
    "\treturn f'Title: {row[ 'title' ]}\\n\\nIngredients: {row[ 'ingredients' ]}\\n\\nGeneric ingredients: '\n",
    "\n",
    "\n",
    "def prepare_example_conversation( row ):\n",
    "\treturn \\\n",
    "\t{\n",
    "\t\t'messages': [\n",
    "\t\t{\n",
    "\t\t\t'role': 'system',\n",
    "\t\t\t'content': system_message\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t'role': 'user',\n",
    "\t\t\t'content': create_user_message( row )\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t'role': 'assistant',\n",
    "\t\t\t'content': row[ 'NER' ]\n",
    "\t\t}, ]\n",
    "\t}\n",
    "\n"
   ],
   "id": "d29e9579f6f0e9dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use the first 100 rows of the dataset for training\n",
    "training_df = recipe_df.loc[ 0:100 ]\n",
    "\n",
    "# apply the prepare_example_conversation function to each row of the training_df\n",
    "training_data = training_df.apply( prepare_example_conversation, axis=1 ).tolist( )\n",
    "\n",
    "for example in training_data[ :5 ]:\n",
    "\tprint( example )"
   ],
   "id": "23bdc89006166778",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "validation_df = recipe_df.loc[ 101:200 ]\n",
    "validation_data = validation_df.apply(\n",
    "\tprepare_example_conversation, axis=1 ).tolist( )"
   ],
   "id": "4773228f51c07750",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3896c43cec6862e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def write_jsonl( data: List[ Dict ], filename: str ) -> None:\n",
    "\twith open( filename, 'w' ) as out:\n",
    "\t\tfor kvp in data:\n",
    "\t\t\tjout = json.dumps( kvp ) + '\\n'\n",
    "\t\t\tout.write( jout )"
   ],
   "id": "6dcf445d39eee404",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "652e2134d8966009"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "validation_file_name = 'tmp_recipe_finetune_validation.jsonl'\n",
    "write_jsonl( validation_data, validation_file_name )"
   ],
   "id": "a492a14231147287",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "170bad89408062ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def upload_file( file_name: str, purpose: str ) -> str:\n",
    "\twith open( file_name, 'rb' ) as file_fd:\n",
    "\t\tresponse = client.files.create( file=file_fd, purpose=purpose )\n",
    "\treturn response.id"
   ],
   "id": "10f941082775954c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "751aceb620d9c7bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c8dbd2395ef59d03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MODEL = 'openai-4o-mini-2024-07-18'\n",
    "\n",
    "response = client.fine_tuning.jobs.create(\n",
    "\ttraining_file=training_file_id,\n",
    "\tvalidation_file=validation_file_id,\n",
    "\tmodel=MODEL,\n",
    "\tsuffix='recipe-ner',\n",
    ")\n",
    "\n",
    "job_id = response.id"
   ],
   "id": "5ca162663d00cad3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Text Cleaning Pipeline",
   "id": "dcabaa53b3a578c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1dd53ce7908a5042",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "f8d915c3f2d4b28f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "af2bd8b593c87d8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range( 10 ):\n",
    "\tprint( lines[ i ] )"
   ],
   "id": "471b42f7fd2f8492",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "new = r'C:\\Users\\terry\\Desktop\\Text\\Chunked'  + '\\\\' + filename\n",
    "folder = open( new, 'wt+' )\n",
    "processed = [ ]\n",
    "for i, c in enumerate( lines ):\n",
    "\tpart = ' '.join( c )\n",
    "\tline = '{ ' + f'\"{i}\"' + ' : ' + '\"' + part + '\"' + ' },' + '\\r'\n",
    "\tprocessed.append( line )\n",
    "\n",
    "for line in processed:\n",
    "\tfolder.write( line )\n",
    "\n",
    "folder.close( )"
   ],
   "id": "feb3b69467ab7c9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7e63e87a30d13ac0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "42c39b663224ebcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b185730909988566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Clean Files",
   "id": "5d4c2f3eb0bdedf4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_files( src: str, dest: str ) -> None:\n",
    "\ttry:\n",
    "\t\tif src is None:\n",
    "\t\t\traise Exception( 'The argument \"src\" is required.' )\n",
    "\t\telif dest is None:\n",
    "\t\t\traise Exception( 'The argument \"dest\" is required.' )\n",
    "\t\telse:\n",
    "\t\t\tsource = src\n",
    "\t\t\tdestination = dest\n",
    "\t\t\tfiles = os.listdir( source )\n",
    "\t\t\tfor f in files:\n",
    "\t\t\t\tprocessed = [ ]\n",
    "\t\t\t\tfilename = os.path.basename( f )\n",
    "\t\t\t\tsource_path = source + '\\\\' + filename\n",
    "\t\t\t\ttext = open( source_path, 'r', encoding='utf-8', errors='ignore' ).read( )\n",
    "\t\t\t\tsentences = split_sentences( text )\n",
    "\t\t\t\tfor s in sentences:\n",
    "\t\t\t\t\tif s != \" \":\n",
    "\t\t\t\t\t\tlower = s.lower( )\n",
    "\t\t\t\t\t\tspecial = remove_special( lower )\n",
    "\t\t\t\t\t\tspace = clean_space( special )\n",
    "\t\t\t\t\t\tprocessed.append( space )\n",
    "\n",
    "\t\t\t\tdest_path = destination + '\\\\' + filename\n",
    "\t\t\t\tclean = open( dest_path, 'wt', encoding='utf-8', errors='ignore' )\n",
    "\t\t\t\tlines = ' '.join( processed )\n",
    "\t\t\t\tclean.write( lines )\n",
    "\t\t\t\tclean.flush( )\n",
    "\texcept Exception as e:\n",
    "\t\tprint( \"The 'clean_files' function raised an exception:\", e )"
   ],
   "id": "55c395d65e61a4d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Clean Text Files",
   "id": "d63131aafd51ce14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_text_files( src: str, dest: str ) -> None:\n",
    "\ttry:\n",
    "\t\tif src is None:\n",
    "\t\t\traise Exception( 'The argument \"src\" is required.' )\n",
    "\t\telif dest is None:\n",
    "\t\t\traise Exception( 'The argument \"dest\" is required.' )\n",
    "\t\telse:\n",
    "\t\t\tsource = src\n",
    "\t\t\tdestination = dest\n",
    "\t\t\tfiles = os.listdir( source )\n",
    "\t\t\tkeepers = [ '$', 'in', '(', ')', '', 'the', '. ', ': ', '; ', 'and', 'but', 'be', 'was', 'what', 'for' ]\n",
    "\t\t\tfor f in files:\n",
    "\t\t\t\tprocessed = [ ]\n",
    "\t\t\t\tfilename = os.path.basename( f )\n",
    "\t\t\t\tsource_path = source + '\\\\' + filename\n",
    "\t\t\t\ttext = open( source_path, 'r', encoding='utf-8', errors='ignore' ).read( )\n",
    "\t\t\t\tcollapse = text.replace( '\\n', ' ' )\n",
    "\t\t\t\tnormal = normalize( collapse )\n",
    "\t\t\t\tsentences = normal.splitlines( )\n",
    "\t\t\t\tfor s in sentences:\n",
    "\t\t\t\t\tif s != \" \" or s in keepers:\n",
    "\t\t\t\t\t\tlower = s.lower( )\n",
    "\t\t\t\t\t\tspecial = remove_special( lower )\n",
    "\t\t\t\t\t\tprocessed.append( special )\n",
    "\n",
    "\t\t\t\tdest_path = destination + '\\\\' + filename\n",
    "\t\t\t\tclean = open( dest_path, 'wt', encoding='utf-8', errors='ignore' )\n",
    "\t\t\t\tlines = ' '.join( processed )\n",
    "\t\t\t\tfinal = clean_text( lines )\n",
    "\t\t\t\tclean.write( final )\n",
    "\t\t\t\tclean.flush( )\n",
    "\texcept Exception as e:\n",
    "\t\tprint( \"The 'clean_files' function raised an exception:\", e )\n",
    "\n"
   ],
   "id": "bdf217e7d2103cac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Chunk Files",
   "id": "1ce62e73b57e03f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def chunk_files( src: str, dest: str ) -> None:\n",
    "\ttry:\n",
    "\t\tif src is None:\n",
    "\t\t\traise Exception( 'The argument \"src\" is required.' )\n",
    "\t\telif dest is None:\n",
    "\t\t\traise Exception( 'The argument \"dest\" is required.' )\n",
    "\t\telse:\n",
    "\t\t\tsource = src\n",
    "\t\t\tdestination = dest\n",
    "\t\t\tfiles = os.listdir( source )\n",
    "\t\t\tfor f in files:\n",
    "\t\t\t\tprocessed = [ ]\n",
    "\t\t\t\tfilename = os.path.basename( f )\n",
    "\t\t\t\tname = filename.replace( '.txt', '.jsonl' )\n",
    "\t\t\t\tsource_path = source + '\\\\' + filename\n",
    "\t\t\t\ttext = open( source_path, 'r', encoding='utf-8', errors='ignore' ).read( )\n",
    "\t\t\t\tsentences = split_sentences( text )\n",
    "\t\t\t\tfragments = remove_fragments( sentences )\n",
    "\t\t\t\tchunks = chunk_pages( fragments )\n",
    "\t\t\t\tfor i, c in enumerate( chunks ):\n",
    "\t\t\t\t\twords = ''.join( c ).strip( )\n",
    "\t\t\t\t\tline = '{' + f'\"Line-{i}\"' + ' : \"' + words + '\"},\\r'\n",
    "\t\t\t\t\tprocessed.append( line )\n",
    "\n",
    "\t\t\t\tdest_path = destination + '\\\\' + name\n",
    "\t\t\t\tclean = open( dest_path, 'wt', encoding='utf-8', errors='ignore' )\n",
    "\t\t\t\tfor p in processed:\n",
    "\t\t\t\t\tclean.write( p )\n",
    "\t\t\t\tclean.flush( )\n",
    "\texcept Exception as e:\n",
    "\t\tprint( \"The 'chunk_files' function raised an exception:\", e )\n"
   ],
   "id": "6110fa9a029e7d7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f6bf516d1034c3f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "chunk_files( src, dest )",
   "id": "97c806612288ddfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine-Tuning Pipeline",
   "id": "7c5ddae1c859c071"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Datasets",
   "id": "9fa08cfe33d29524"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Datasets\n",
    "omb = r'C:\\Users\\terry\\Desktop\\AI\\fine-tuning\\datasets\\omb.xlsx'\n",
    "cfr31 = r'C:\\Users\\terry\\Desktop\\AI\\fine-tuning\\datasets\\cfr31.xlsx'\n",
    "fastbook = r'C:\\Users\\terry\\Desktop\\AI\\fine-tuning\\datasets\\fastbook.xlsx'\n",
    "redbook = r'C:\\Users\\terry\\Desktop\\AI\\fine-tuning\\datasets\\redbook.xlsx'\n",
    "ledger = r'C:\\Users\\terry\\Desktop\\AI\\fine-tuning\\datasets\\ledger.xlsx'"
   ],
   "id": "e069c9fcfcbbd2bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### System Instructions",
   "id": "abab952ca12e7aae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set System Instructions\n",
    "instructions = '''You are the most knowledgeable Budget Analyst in the federal government who provides detailed responses based on your vast knowledge of budget legislation, and federal appropriations. Your responses to questions about federal finance are complete, transparent, and very detailed using an academic format. Your vast knowledge of and experience in Data Science makes you the best Data Analyst in the world. You are also an expert programmer who is proficient in C#, Python, S L, C++, JavaScript, and VBA. You are famous for the accuracy of your responses so you verify all your answers. This makes the quality of your code very high and it always works. Your responses are always accurate and complete! Your name is Bubba.\n",
    "'''"
   ],
   "id": "47efa7ddc63591f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Messages\n",
   "id": "e95d6e2892e0c0c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set Messages\n",
    "system = '{\"messages\":[{' + f'\"role\":\"system\", \"content\":\"{instructions}\"' + '},{'\n",
    "initial = system + f' \"role\":\"user\", \"content\":\"{Q}\"' + '},{'\n",
    "question = '{\"messages\":[{' + f'\"role\": \"user\",  \"content\":\"{Q}\"' + '},{'\n",
    "answer = f' \"role\":\"assistant\", \"content\":\"{A}\"' + '}]}'\n"
   ],
   "id": "80b9dc49e08e15ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print JSON\n",
    "\n",
    "for r in range( 25 ):\n",
    "\tquestion = '{\"messages\":[{' + f'\"role\":\"user\", \"content\":\"{df_ledger.iloc[ r, 2 ]}\"' + '},{'\n",
    "\tanswer = f'\"role\":\"assistant\", \"content\": \"{df_ledger.iloc[ r, 3 ]}\" ' + '}]}'\n",
    "\trecord = question + answer\n",
    "\tprint( record  )"
   ],
   "id": "314e241553d70eb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### A-11 Data",
   "id": "15adfd8026079931"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define A-11 Data\n",
    "xl_a11 = pd.read_excel( omb, sheet_name='Training' )\n",
    "names = [ 'ID', 'Item', 'Role', 'Content' ]\n",
    "a11_idx = xl_a11.index\n",
    "df_a11 = pd.DataFrame( data=xl_a11, columns=names  )\n",
    "df_a11 = df_a11.reset_index( ).set_index( 'ID' )\n",
    "df_a11 = df_a11.drop( columns=[ 'index' ]  )\n",
    "omb_rows = len( df_a11  )\n",
    "omb_rows"
   ],
   "id": "d4c2b57481a2bd0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# View Dataframe\n",
    "df_a11"
   ],
   "id": "4da849b5db0be898",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print JSON\n",
    "for r in range( len( df_a11 )):\n",
    "\trow = df_a11.iloc[ r, : ]\n",
    "\tif row[ 'Item' ] == 'Q':\n",
    "\t\tquestion = '{\"messages\":[{' + f'\"role\":\"{row[ 'Role' ]}\", \"content\":\"{row[ 'Content' ]}\"' + '},{'\n",
    "\telif  row[ 'Item' ] == 'A':\n",
    "\t\tanswer = f'\"role\":\"{row[ 'Role' ]}\", \"content\": \"{row[ 'Content' ]}\"' + '}]}'\n",
    "\t\trecord = question + answer\n",
    "\tprint( record )"
   ],
   "id": "4c71dc465c8d6cd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "afcdf44e412f48f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### CFR Data",
   "id": "91397e6e2f827a75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define CFR Data\n",
    "xl_cfr31 = pd.read_excel( cfr31, sheet_name='Training' )\n",
    "names = [ 'ID', 'Item', 'Role', 'Content' ]\n",
    "cfr_index = xl_cfr31.index\n",
    "df_cfr31 = pd.DataFrame( data=xl_cfr31, columns=names  )\n",
    "df_cfr31 = df_cfr31.reset_index( ).set_index( 'ID' )\n",
    "df_cfr31 = df_cfr31.drop( columns=[ 'index' ] )\n",
    "cfr_rows = len( df_cfr31 )\n",
    "cfr_rows"
   ],
   "id": "b84b674d7baf6715",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# View Dataframe\n",
    "df_cfr31"
   ],
   "id": "e1945d3078cd47cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print JSON\n",
    "for r in range( len( df_cfr31 ) ):\n",
    "\tif df_cfr31.iloc[ r, 1 ].startswith( 'u' ):\n",
    "\t\tquestion = '{\"messages\":[{' + f'\"role\":\"{df_cfr31.iloc[ r, 1 ]}\", \"content\":\"{df_cfr31.iloc[ r, 2 ]}\"' + '},{'\n",
    "\telif df_cfr31.iloc[ r, 1 ].startswith( 'a' ):\n",
    "\t\tanswer = f'\"role\":\"{df_cfr31.iloc[ r, 1 ]}\", \"content\": \"{df_cfr31.iloc[ r, 2 ]}\"' + '}]}'\n",
    "\trecord = question + answer\n",
    "\tprint( record  )\n",
    "\n"
   ],
   "id": "8c98f202b7b842be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Redbook Data",
   "id": "b3358b800bcaedbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Redbook Data\n",
    "xl_redbook = pd.read_excel( redbook, sheet_name='Training' )\n",
    "names = [ 'ID', 'Item', 'Role', 'Content' ]\n",
    "red_inx = xl_redbook.index\n",
    "df_redbook = pd.DataFrame( data=xl_redbook, columns=names )\n",
    "df_redbook = df_redbook.reset_index( ).set_index( 'ID' )\n",
    "df_redbook = df_redbook.drop( columns=[ 'index' ] )\n",
    "red_rows = len( df_redbook )\n",
    "red_rows"
   ],
   "id": "a9c71f5ebcb9cf7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# View Dataframe\n",
    "df_redbook"
   ],
   "id": "c6fe2814e474853",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print JSON\n",
    "for r in range( len( df_redbook ) ):\n",
    "\tif df_redbook.iloc[ r, 1 ].startswith( 'u' ):\n",
    "\t\tquestion = '{\"messages\":[{' + f'\"role\":\"{df_redbook.iloc[ r, 1 ]}\", \"content\":\"{df_redbook.iloc[ r, 2 ]}\"' + '},{'\n",
    "\telif df_redbook.iloc[ r, 1 ].startswith( 'a' ):\n",
    "\t\tanswer = f'\"role\":\"{df_redbook.iloc[ r, 1 ]}\", \"content\": \"{df_redbook.iloc[ r, 2 ]}\" ' + '}]}'\n",
    "\trecord = question + answer\n",
    "\tprint( record  )"
   ],
   "id": "2c4a931e40a9f48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Ledger Data",
   "id": "ba25e74e31b22923"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Ledger Data\n",
    "xl_ledger = pd.read_excel( ledger, sheet_name='Training' )\n",
    "names = [ 'ID', 'Item', 'Role', 'Content' ]\n",
    "ldgr_inx = xl_ledger.index\n",
    "df_ledger = pd.DataFrame( data=xl_ledger, columns=names, index=ldgr_inx  )\n",
    "df_ledger = df_ledger.reset_index(  ).set_index( 'ID' )\n",
    "df_ledger = df_ledger.drop( columns=[ 'index' ] )\n",
    "ledger_rows = len( df_ledger )\n",
    "ledger_rows"
   ],
   "id": "46e83d4d44129c27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# View Dataframe\n",
    "df_ledger"
   ],
   "id": "249580b9e0ab02c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in df_ledger.columns:\n",
    "\tprint( i )"
   ],
   "id": "9da484b01968d9c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print JSON\n",
    "for r in range( len( df_ledger ) ):\n",
    "\tif df_ledger.iloc[ r, 1 ].startswith( 'u' ):\n",
    "\t\tquestion = '{\"messages\":[{' + f'\"role\":\"{df_ledger.iloc[ r, 1 ]}\", \"content\":\"{df_ledger.iloc[ r, 2 ]}\"' + '},{'\n",
    "\telif df_cfr31.iloc[ r, 1 ].startswith( 'a' ):\n",
    "\t\tanswer = f'\"role\":\"{df_ledger.iloc[ r, 1 ]}\", \"content\": \"{df_ledger.iloc[ r, 2 ]}\" ' + '}]}'\n",
    "\trecord = question + answer\n",
    "\tprint( record  )"
   ],
   "id": "f6483f7f77ed048e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cbcae77914e300a9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
