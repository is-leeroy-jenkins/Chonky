{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Chonky","text":"<p>A modular text-processing framework baseed in python tailored for analysts, data scientists, and machine learning practitioners working with unstructured text. It unifies high-performance NLP utilities and machine learning-ready pipelines to support text ingestion, cleaning, tokenization, feature extraction, and document analysis.</p> <pre><code>python\nfrom chonky import Text\n\nt = Text( )\nprint( t.clean_space( \"A   chonky   example!\\tTabs too.\" ) )\n</code></pre> <pre><code>markdown\n\n# Remove repeating headers/footers\n</code></pre> <pre><code>from chonky.processing import Text\n\npages = [ \"Header\\nPage 1 body\\nFooter\", \"Header\\nPage 2 body\\nFooter\" ]\nt = Text( )\nclean = t.remove_headers( pages, min = 2 )\nprint( clean )\n</code></pre>"},{"location":"#features","title":"Features","text":"<ul> <li>Text Preprocessing: Clean and normalize text by removing HTML, punctuation, special   characters, and stopwords.</li> <li>Tokenization: Sentence and word-level tokenization with support for HuggingFace and OpenAI   tokenizers.</li> <li>Chunking: Token or word chunking for long document management and model input preparation.</li> <li>Text Analysis: Frequency distributions, conditional frequency analysis, TF-IDF, Word2Vec   embeddings.</li> <li>Multi-format Support: Extracts from <code>.txt</code>, <code>.docx</code>, and <code>.pdf</code> files with high fidelity.</li> <li>Custom Pipelines: Utilities for JSONL export, batch cleaning, and document segmentation.</li> <li>LLM-Compatible: Includes OpenAI and HuggingFace tokenizer interfaces for seamless integration.</li> </ul>"},{"location":"#setup-instructions","title":"Setup Instructions","text":"<p>To ensure a clean and isolated environment for running Chonky, follow these steps:</p>"},{"location":"#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone https://github.com/yourusername/Chonky.git\ncd Chonky\ncd venv/Scripts\n./activate.bat\ncd ../../\npip install -r requirements.txt\n</code></pre>"},{"location":"#text-class","title":"Text Class","text":"<ul> <li>General-purpose text processor.</li> <li>Methods: <code>load_text</code>, <code>normalize_text</code>, <code>remove_html</code>, <code>remove_punctuation</code>, <code>lemmatize_tokens</code>,   <code>tokenize_text</code>, <code>chunk_text</code>, <code>create_word2vec</code>, <code>create_tfidf</code>, and more.</li> </ul> Method Description <code>load_text(path)</code> Loads raw text from a file. <code>split_lines(path)</code> Splits text into individual lines. <code>split_pages(path, delimit)</code> Splits text by page delimiters. <code>collapse_whitespace(text)</code> Collapses multiple whitespaces into single spaces. <code>remove_punctuation(text)</code> Removes punctuation from text. <code>remove_special(text)</code> Removes special characters while preserving select symbols. <code>remove_html(text)</code> Strips HTML tags using BeautifulSoup. <code>remove_errors(text)</code> Removes non-English or misspelled words. <code>correct_errors(text)</code> Attempts to autocorrect spelling using TextBlob. <code>remove_markdown(text)</code> Strips Markdown syntax like <code>*</code>, <code>#</code>, etc. <code>remove_stopwords(text)</code> Removes English stopwords. <code>remove_headers(pages)</code> Removes repetitive headers/footers using frequency. <code>normalize_text(text)</code> Normalizes text to lowercase ASCII. <code>lemmatize_tokens(tokens)</code> Lemmatizes tokens using NLTK's WordNet. <code>tokenize_text(text)</code> Cleans and tokenizes raw text. <code>tokenize_words(words)</code> Tokenizes a list of words. <code>tokenize_sentences(text)</code> Sentence tokenization using NLTK. <code>split_paragraphs(path)</code> Splits text file into paragraphs. <code>chunk_text(text, size)</code> Splits text into word chunks. <code>chunk_words(words, size)</code> Chunks a list of tokenized words. <code>split_sentences(text)</code> Returns a list of sentences. <code>compute_frequency_distribution(lines)</code> Computes frequency of tokens. <code>compute_conditional_distribution(lines)</code> Computes conditional frequency grouped by condition. <code>create_vocabulary(freq_dist)</code> Creates vocabulary list from token frequency. <code>create_wordbag(words)</code> Constructs Bag-of-Words from token list. <code>create_word2vec(words)</code> Trains a Word2Vec model from tokenized sentences. <code>create_tfidf(words)</code> Generates TF-IDF matrix. <code>clean_files(src, dest)</code> Batch cleans <code>.txt</code> files from source to destination. <code>convert_jsonl(src, dest)</code> Converts text files into JSONL format."},{"location":"#word-class","title":"Word Class","text":"<ul> <li>Parses <code>.docx</code> files using Python-docx.</li> <li>Sentence segmentation, vocabulary extraction, frequency computation.</li> </ul> Method Description <code>extract_text()</code> Extracts text and paragraphs from a <code>.docx</code> file. <code>split_sentences()</code> Splits extracted text into sentences. <code>clean_sentences()</code> Cleans individual sentences: lowercases and removes punctuation. <code>create_vocabulary()</code> Builds vocabulary list from cleaned sentences. <code>compute_frequency_distribution()</code> Computes token frequency from sentences. <code>summarize()</code> Prints summary stats: paragraphs, sentences, vocab size."},{"location":"#pdf-class","title":"PDF Class","text":"<ul> <li>Reads <code>.pdf</code> files using <code>PyMuPDF</code>.</li> <li>Extracts structured or unstructured text and exports CSV/Excel.</li> </ul> Method Description <code>extract_lines(path, max)</code> Extracts and cleans lines of text from PDF pages. <code>extract_text(path, max)</code> Extracts full concatenated text from PDF. <code>extract_tables(path, max)</code> Extracts tables into pandas DataFrames. <code>export_csv(tables, filename)</code> Exports extracted tables to CSV files. <code>export_text(lines, path)</code> Writes extracted text lines to a <code>.txt</code> file. <code>export_excel(tables, path)</code> Saves tables to an Excel workbook."},{"location":"#example-usage","title":"Example Usage","text":"<pre><code>    python\n    from processing import Text\n    processor = Text()\n    text = processor.load_text(\"example.txt\")\n    clean = processor.remove_stopwords(text)\n    tokens = processor.tokenize_words(clean)\n    chunks = processor.chunk_text(clean, size=100)\n</code></pre>"},{"location":"#initialize-processor","title":"Initialize Processor","text":"<pre><code>  processor = Text()\n</code></pre>"},{"location":"#load-raw-text","title":"Load Raw Text","text":"<pre><code>  raw_text = processor.load_text(\"data/sample.txt\")\n</code></pre>"},{"location":"#clean-normalize","title":"Clean &amp; Normalize","text":"<pre><code>  text = processor.remove_html(raw_text)                     # \ud83e\uddf9 Strip HTML\n  text = processor.normalize_text(text)                      # \ud83d\udd21 Lowercase + ASCII\n  text = processor.remove_markdown(text)                     # \u2728 Remove markdown (#, *, etc.)\n  text = processor.remove_special(text)                      # \u274c Remove special chars\n  text = processor.remove_punctuation(text)                  # \ud83e\ude9b Remove punctuation\n  text = processor.collapse_whitespace(text)                 # \ud83d\udccf Collapse whitespace\n</code></pre>"},{"location":"#spelling-stopwords","title":"Spelling &amp; Stopwords","text":"<pre><code>  cleaned_text = processor.remove_errors(text)               # \ud83e\uddec Remove misspellings\n  corrected_text = processor.correct_errors(cleaned_text)    # \ud83d\udd01 Auto-correct spelling\n  no_stopwords_text = processor.remove_stopwords(corrected_text)  # \ud83d\udeab Remove stopwords\n</code></pre>"},{"location":"#tokenization","title":"Tokenization","text":"<pre><code>  word_tokens = processor.tokenize_words(no_stopwords_text)       # \ud83e\udde9 Word tokens\n  sentence_tokens = processor.tokenize_sentences(no_stopwords_text)  # \ud83e\uddfe Sentence tokens\n</code></pre>"},{"location":"#lemmatization","title":"Lemmatization","text":"<pre><code>  lemmatized_tokens = processor.lemmatize_tokens(word_tokens)\n</code></pre>"},{"location":"#chunking","title":"Chunking","text":"<pre><code>  text_chunks = processor.chunk_text(no_stopwords_text, max=800)   # \ud83e\uddf3 Word chunked text\n  word_chunks = processor.chunk_words(word_tokens, max=100, over=50)  # \ud83c\udf92 Token chunks\n</code></pre>"},{"location":"#structural-splitting","title":"Structural Splitting","text":"<pre><code>  line_groups = processor.split_lines(\"data/sample.txt\")           # \ud83d\udccf Lines\n  paragraphs = processor.split_paragraphs(\"data/sample.txt\")       # \ud83d\udcc4 Paragraphs\n  pages = processor.split_pages(\"data/sample.txt\", delimit=\"\\f\")   # \ud83d\udcc3 Pages (form-feed)\n</code></pre>"},{"location":"#frequency-vocabulary","title":"Frequency &amp; Vocabulary","text":"<pre><code>  freq_dist = processor.compute_frequency_distribution(word_tokens)  # \ud83d\udcc8 Frequency dist\n  cond_freq = processor.compute_conditional_distribution(word_tokens, condition=\"POS\")  # \ud83e\uddee\n  Conditional\n  vocabulary = processor.create_vocabulary(freq_dist, min=2)         # \ud83d\udcd6 Vocabulary\n</code></pre>"},{"location":"#vector-representations","title":"Vector Representations","text":"<pre><code>  bow_vector = processor.create_wordbag(word_tokens)                 # \ud83e\uddf0 Bag-of-Words\n  word2vec_model = processor.create_word2vec([word_tokens], vector_size=100, window=5)  # \ud83e\uddec Word2Vec\n  tfidf_matrix, feature_names = processor.create_tfidf(word_tokens, max_features=500)   # \ud83d\udcd0 TF-IDF\n</code></pre>"},{"location":"#batch-utilities","title":"Batch Utilities","text":"<pre><code>  processor.clean_files(\"data/input_dir\", \"data/cleaned_output_dir\")     # \ud83e\uddfc Clean .txt files in bulk\n  processor.convert_jsonl(\"data/cleaned_output_dir\", \"data/jsonl_output_dir\")  # \ud83d\udd04 .txt \u27a1\ufe0f .jsonl\n</code></pre>"},{"location":"#dependencies","title":"Dependencies","text":"Package Description <code>nltk</code> Natural language toolkit for tokenization, stopwords, tagging, etc. <code>gensim</code> Library for Word2Vec and topic modeling. <code>spacy</code> Industrial-strength NLP for tagging and parsing. <code>scikit-learn</code> Machine learning library used for TF-IDF and dimensionality reduction. <code>pandas</code> Data analysis and manipulation tool. <code>numpy</code> Fundamental package for scientific computing. <code>tiktoken</code> OpenAI\u2019s tokenizer for GPT models. <code>transformers</code> HuggingFace\u2019s model and tokenizer interface. <code>pymupdf</code> PDF extraction with PyMuPDF (a.k.a. <code>fitz</code>). <code>python-docx</code> Extracts text from Microsoft Word <code>.docx</code> documents. <code>beautifulsoup4</code> Parses and cleans HTML/XML content. <code>pydantic</code> Data validation and parsing with Python type hints."},{"location":"#license","title":"License","text":"<p>Chonky is published under the MIT General Public License v3.</p>"},{"location":"reference/api/","title":"API Reference","text":""},{"location":"reference/api/#boogr","title":"boogr","text":"<pre><code>Assembly:                Chonky\nFilename:                boogr.py\nAuthor:                  Terry D. Eppler\nCreated:                 05-31-2022\n\nLast Modified By:        Terry D. Eppler\nLast Modified On:        05-01-2025\n</code></pre> <p> <pre><code>   Chonky is a modular text-processing framework for machine-learning workflows based in python\n</code></pre> <p>Permission is hereby granted, free of charge, to any person obtaining a copy    of this software and associated documentation files (the \u201cSoftware\u201d),    to deal in the Software without restriction,    including without limitation the rights to use,    copy, modify, merge, publish, distribute, sublicense,    and/or sell copies of the Software,    and to permit persons to whom the Software is furnished to do so,    subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all    copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,    INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,    FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.    IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,    DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,    ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER    DEALINGS IN THE SOFTWARE.</p> <p>You can contact me at:  terryeppler@gmail.com or eppler.terry@epa.gov</p> <p></p>    boogr.py  <p>options: show_root_heading: true</p>"},{"location":"reference/api/#boogr.Dark","title":"Dark","text":"<p>               Bases: <code>BaseModel</code></p> Constructor <p>Dark( )</p> Pupose <p>Class representing the theme</p> Source code in <code>boogr.py</code> <pre><code>class Dark( BaseModel ):\n\t'''\n\n        Constructor:\n\t\t-----------\n        Dark( )\n\n        Pupose:\n\t\t-------\n\t\tClass representing the theme\n\n    '''\n\n\tclass Config:\n\t\tarbitrary_types_allowed = True\n\t\textra = 'ignore'\n\t\tallow_mutation = True\n\n\tdef __init__( self ):\n\t\tsuper( ).__init__( )\n\t\tsg.theme( 'DarkGrey15' )\n\t\tsg.theme_input_text_color( '#FFFFFF' )\n\t\tsg.theme_element_text_color( '#69B1EF' )\n\t\tsg.theme_text_color( '#69B1EF' )\n\t\tself.theme_background = sg.theme_background_color( )\n\t\tself.theme_textcolor = sg.theme_text_color( )\n\t\tself.element_forecolor = sg.theme_element_text_color( )\n\t\tself.element_backcolor = sg.theme_background_color( )\n\t\tself.text_backcolor = sg.theme_text_element_background_color( )\n\t\tself.text_forecolor = sg.theme_element_text_color( )\n\t\tself.input_forecolor = sg.theme_input_text_color( )\n\t\tself.input_backcolor = sg.theme_input_background_color( )\n\t\tself.button_backcolor = sg.theme_button_color_background( )\n\t\tself.button_forecolor = sg.theme_button_color_text( )\n\t\tself.button_color = sg.theme_button_color( )\n\t\tself.icon_path = r'/\\resources\\ico\\ninja.ico'\n\t\tself.theme_font = ('Roboto', 11)\n\t\tself.scrollbar_color = '#755600'\n\t\tself.form_size = (400, 200)\n\t\tsg.set_global_icon( icon=self.icon_path )\n\t\tsg.set_options( font=self.theme_font )\n\t\tsg.user_settings_save( 'Boo', r'/\\resources\\theme' )\n\n\n\tdef __dir__( self ) -&gt; List[ str ] | None:\n\t\t'''\n\n\t\t    Purpose:\n\t\t    --------\n\t\t    Creates a List[ str ] of type members\n\n\t\t    Parameters:\n\t\t    ----------\n\t\t\tself\n\n\t\t    Returns:\n\t\t    ---------\n\t\t\tList[ str ] | None\n\n\t\t'''\n\t\treturn [ 'form_size', 'theme_background',\n\t\t         'theme_textcolor', 'element_backcolor', 'element_forecolor',\n\t\t         'text_forecolor', 'text_backcolor', 'input_backcolor',\n\t\t         'input_forecolor', 'button_color', 'button_backcolor',\n\t\t         'button_forecolor', 'icon_path', 'theme_font',\n\t\t         'scrollbar_color' ]\n</code></pre>"},{"location":"reference/api/#boogr.Dark.__dir__","title":"__dir__","text":"<pre><code>__dir__()\n</code></pre>"},{"location":"reference/api/#boogr.Dark.__dir__--purpose","title":"Purpose:","text":"<p>Creates a List[ str ] of type members</p> <pre><code>self\n</code></pre> <pre><code>List[ str ] | None\n</code></pre> Source code in <code>boogr.py</code> <pre><code>def __dir__( self ) -&gt; List[ str ] | None:\n\t'''\n\n\t    Purpose:\n\t    --------\n\t    Creates a List[ str ] of type members\n\n\t    Parameters:\n\t    ----------\n\t\tself\n\n\t    Returns:\n\t    ---------\n\t\tList[ str ] | None\n\n\t'''\n\treturn [ 'form_size', 'theme_background',\n\t         'theme_textcolor', 'element_backcolor', 'element_forecolor',\n\t         'text_forecolor', 'text_backcolor', 'input_backcolor',\n\t         'input_forecolor', 'button_color', 'button_backcolor',\n\t         'button_forecolor', 'icon_path', 'theme_font',\n\t         'scrollbar_color' ]\n</code></pre>"},{"location":"reference/api/#boogr.Error","title":"Error","text":"<p>               Bases: <code>Exception</code></p>"},{"location":"reference/api/#boogr.Error--purpose","title":"Purpose:","text":"<pre><code>    Class wrapping error used as the path argument for ErrorDialog class\n</code></pre> Constructor <p>Error( error: Exception, heading: str=None, cause: str=None,         method: str=None, module: str=None )</p> Source code in <code>boogr.py</code> <pre><code>class Error( Exception ):\n\t'''\n\n        Purpose:\n        ---------\n\t\tClass wrapping error used as the path argument for ErrorDialog class\n\n        Constructor:\n\t\t----------\n        Error( error: Exception, heading: str=None, cause: str=None,\n                method: str=None, module: str=None )\n\n    '''\n\n\tdef __init__( self, error: Exception, heading: str=None, cause: str=None,\n\t              method: str=None, module: str=None ):\n\t\tsuper( ).__init__( )\n\t\tself.exception = error\n\t\tself.heading = heading\n\t\tself.cause = cause\n\t\tself.method = method\n\t\tself.module = module\n\t\tself.type = exc_info( )[ 0 ]\n\t\tself.trace = traceback.format_exc( )\n\t\tself.info = str( exc_info( )[ 0 ] ) + ': \\r\\n \\r\\n' + traceback.format_exc( )\n\n\n\tdef __str__( self ) -&gt; str | None:\n\t\t'''\n\n            Purpose:\n            --------\n\t\t\treturns a string reprentation of the object\n\n            Parameters:\n            ----------\n\t\t\tself\n\n            Returns:\n            ---------\n\t\t\tstr | None\n\n\t\t'''\n\t\tif self.info is not None:\n\t\t\treturn self.info\n\n\n\tdef __dir__( self ) -&gt; List[ str ] | None:\n\t\t'''\n\n\t\t    Purpose:\n\t\t    --------\n\t\t    Creates a List[ str ] of type members\n\n\t\t    Parameters:\n\t\t    ----------\n\t\t\tself\n\n\t\t    Returns:\n\t\t    ---------\n\t\t\tList[ str ] | None\n\n\t\t'''\n\t\treturn [ 'message', 'cause',  'method', 'module', 'scaler', 'stack_trace', 'info' ]\n</code></pre>"},{"location":"reference/api/#boogr.Error.__dir__","title":"__dir__","text":"<pre><code>__dir__()\n</code></pre>"},{"location":"reference/api/#boogr.Error.__dir__--purpose","title":"Purpose:","text":"<p>Creates a List[ str ] of type members</p> <pre><code>self\n</code></pre> <pre><code>List[ str ] | None\n</code></pre> Source code in <code>boogr.py</code> <pre><code>def __dir__( self ) -&gt; List[ str ] | None:\n\t'''\n\n\t    Purpose:\n\t    --------\n\t    Creates a List[ str ] of type members\n\n\t    Parameters:\n\t    ----------\n\t\tself\n\n\t    Returns:\n\t    ---------\n\t\tList[ str ] | None\n\n\t'''\n\treturn [ 'message', 'cause',  'method', 'module', 'scaler', 'stack_trace', 'info' ]\n</code></pre>"},{"location":"reference/api/#boogr.Error.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre>"},{"location":"reference/api/#boogr.Error.__str__--purpose","title":"Purpose:","text":"<pre><code>        returns a string reprentation of the object\n</code></pre> <pre><code>        self\n</code></pre> <pre><code>        str | None\n</code></pre> Source code in <code>boogr.py</code> <pre><code>\tdef __str__( self ) -&gt; str | None:\n\t\t'''\n\n            Purpose:\n            --------\n\t\t\treturns a string reprentation of the object\n\n            Parameters:\n            ----------\n\t\t\tself\n\n            Returns:\n            ---------\n\t\t\tstr | None\n\n\t\t'''\n\t\tif self.info is not None:\n\t\t\treturn self.info\n</code></pre>"},{"location":"reference/api/#boogr.ErrorDialog","title":"ErrorDialog","text":"<p>               Bases: <code>Dark</code></p> <p>Construcotr:  ErrorDialog( error )</p> <p>Purpose:  Class that displays excetption target_values that accepts a single, optional argument 'error' of scaler Error</p> Source code in <code>boogr.py</code> <pre><code>class ErrorDialog( Dark ):\n\t'''\n\n\t    Construcotr:  ErrorDialog( error )\n\n\t    Purpose:  Class that displays excetption target_values that accepts\n            a single, optional argument 'error' of scaler Error\n\n    '''\n\n\t# Fields\n\terror: Exception=None\n\theading: str=None\n\tmodule: str=None\n\tinfo: str=None\n\tcause: str=None\n\tmethod: str=None\n\n\tdef __init__( self, error: Error ):\n\t\tsuper( ).__init__( )\n\t\tsg.theme( 'DarkGrey15' )\n\t\tsg.theme_input_text_color( '#FFFFFF' )\n\t\tsg.theme_element_text_color( '#69B1EF' )\n\t\tsg.theme_text_color( '#69B1EF' )\n\t\tself.theme_background=sg.theme_background_color( )\n\t\tself.theme_textcolor = sg.theme_text_color( )\n\t\tself.element_forecolor = sg.theme_element_text_color( )\n\t\tself.element_backcolor = sg.theme_background_color( )\n\t\tself.text_backcolor = sg.theme_text_element_background_color( )\n\t\tself.text_forecolor = sg.theme_element_text_color( )\n\t\tself.input_forecolor = sg.theme_input_text_color( )\n\t\tself.input_backcolor = sg.theme_input_background_color( )\n\t\tself.button_backcolor = sg.theme_button_color_background( )\n\t\tself.button_forecolor = sg.theme_button_color_text( )\n\t\tself.button_color = sg.theme_button_color( )\n\t\tself.icon_path = r'/\\resources\\ico\\ninja.ico'\n\t\tself.theme_font = ('Roboto', 11)\n\t\tself.scrollbar_color = '#755600'\n\t\tsg.set_global_icon( icon = self.icon_path )\n\t\tsg.set_options( font = self.theme_font )\n\t\tsg.user_settings_save( 'Mathy', r'/\\resources\\theme' )\n\t\tself.form_size = (500, 300)\n\t\tself.error = error\n\t\tself.heading = error.heading\n\t\tself.module = error.module\n\t\tself.info = error.trace\n\t\tself.cause = error.cause\n\t\tself.method = error.method\n\n\n\tdef __str__( self ) -&gt; str | None:\n\t\t'''\n\n            Purpose:\n            --------\n\t\t\treturns a string reprentation of the object\n\n            Parameters:\n            ----------\n\t\t\tself\n\n            Returns:\n            ---------\n\t\t\tstr | None\n\n\t\t'''\n\t\treturn self.info\n\n\n\tdef __dir__( self ) -&gt; List[ str ] | None:\n\t\t'''\n\n\t\t    Purpose:\n\t\t    --------\n\t\t    Creates a List[ str ] of type members\n\n\t\t    Parameters:\n\t\t    ----------\n\t\t\tself\n\n\t\t    Returns:\n\t\t    ---------\n\t\t\tList[ str ] | None\n\n\t\t'''\n\t\treturn [ 'size', 'settings_path', 'theme_background',\n\t\t         'theme_textcolor', 'element_backcolor', 'element_forecolor',\n\t\t         'text_forecolor', 'text_backcolor', 'input_backcolor',\n\t\t         'input_forecolor', 'button_color', 'button_backcolor',\n\t\t         'button_forecolor', 'icon_path', 'theme_font',\n\t\t         'scrollbar_color', 'progressbar_color',\n\t\t         'info', 'cause', 'method', 'error', 'heading',\n\t\t         'module', 'scaler', 'message' 'show' ]\n\n\n\tdef show( self ) -&gt; object:\n\t\t'''\n\n            Purpose:\n            --------\n\n\n            Parameters:\n            ----------\n\n\n            Returns:\n            ---------\n\n\n\t\t'''\n\t\t_msg = self.heading if isinstance( self.heading, str ) else None\n\t\t_info = f'Module:\\t{self.module}\\r\\nClass:\\t{self.cause}\\r\\n' \\\n\t\t        f'Method:\\t{self.method}\\r\\n \\r\\n{self.info}'\n\t\t_red = '#F70202'\n\t\t_font = ('Roboto', 10)\n\t\t_padsz = (3, 3)\n\t\t_layout = [ [ sg.Text( ) ],\n\t\t            [ sg.Text( f'{_msg}', size=(100, 1), key='-MSG-', text_color=_red,\n\t\t\t            font=_font ) ],\n\t\t            [ sg.Text( size=(150, 1) ) ],\n\t\t            [ sg.Multiline( f'{_info}', key='-INFO-', size=(80, 7), pad=_padsz ) ],\n\t\t            [ sg.Text( ) ],\n\t\t            [ sg.Text( size=(20, 1) ), sg.Cancel( size=(15, 1), key='-CANCEL-' ),\n\t\t              sg.Text( size=(10, 1) ), sg.Ok( size=(15, 1), key='-OK-' ) ] ]\n\n\t\t_window = sg.Window( r' Mathy', _layout,\n\t\t\ticon=self.icon_path,\n\t\t\tfont=self.theme_font,\n\t\t\tsize=self.form_size )\n\n\t\twhile True:\n\t\t\t_event, _values = _window.read( )\n\t\t\tif _event in (sg.WIN_CLOSED, sg.WIN_X_EVENT, 'Canel', '-OK-'):\n\t\t\t\tbreak\n\n\t\t_window.close( )\n</code></pre>"},{"location":"reference/api/#boogr.ErrorDialog.__dir__","title":"__dir__","text":"<pre><code>__dir__()\n</code></pre>"},{"location":"reference/api/#boogr.ErrorDialog.__dir__--purpose","title":"Purpose:","text":"<p>Creates a List[ str ] of type members</p> <pre><code>self\n</code></pre> <pre><code>List[ str ] | None\n</code></pre> Source code in <code>boogr.py</code> <pre><code>def __dir__( self ) -&gt; List[ str ] | None:\n\t'''\n\n\t    Purpose:\n\t    --------\n\t    Creates a List[ str ] of type members\n\n\t    Parameters:\n\t    ----------\n\t\tself\n\n\t    Returns:\n\t    ---------\n\t\tList[ str ] | None\n\n\t'''\n\treturn [ 'size', 'settings_path', 'theme_background',\n\t         'theme_textcolor', 'element_backcolor', 'element_forecolor',\n\t         'text_forecolor', 'text_backcolor', 'input_backcolor',\n\t         'input_forecolor', 'button_color', 'button_backcolor',\n\t         'button_forecolor', 'icon_path', 'theme_font',\n\t         'scrollbar_color', 'progressbar_color',\n\t         'info', 'cause', 'method', 'error', 'heading',\n\t         'module', 'scaler', 'message' 'show' ]\n</code></pre>"},{"location":"reference/api/#boogr.ErrorDialog.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre>"},{"location":"reference/api/#boogr.ErrorDialog.__str__--purpose","title":"Purpose:","text":"<pre><code>        returns a string reprentation of the object\n</code></pre> <pre><code>        self\n</code></pre> <pre><code>        str | None\n</code></pre> Source code in <code>boogr.py</code> <pre><code>\tdef __str__( self ) -&gt; str | None:\n\t\t'''\n\n            Purpose:\n            --------\n\t\t\treturns a string reprentation of the object\n\n            Parameters:\n            ----------\n\t\t\tself\n\n            Returns:\n            ---------\n\t\t\tstr | None\n\n\t\t'''\n\t\treturn self.info\n</code></pre>"},{"location":"reference/api/#boogr.ErrorDialog.show","title":"show","text":"<pre><code>show()\n</code></pre>"},{"location":"reference/api/#boogr.ErrorDialog.show--purpose","title":"Purpose:","text":""},{"location":"reference/api/#boogr.ErrorDialog.show--parameters","title":"Parameters:","text":""},{"location":"reference/api/#boogr.ErrorDialog.show--returns","title":"Returns:","text":"Source code in <code>boogr.py</code> <pre><code>\tdef show( self ) -&gt; object:\n\t\t'''\n\n            Purpose:\n            --------\n\n\n            Parameters:\n            ----------\n\n\n            Returns:\n            ---------\n\n\n\t\t'''\n\t\t_msg = self.heading if isinstance( self.heading, str ) else None\n\t\t_info = f'Module:\\t{self.module}\\r\\nClass:\\t{self.cause}\\r\\n' \\\n\t\t        f'Method:\\t{self.method}\\r\\n \\r\\n{self.info}'\n\t\t_red = '#F70202'\n\t\t_font = ('Roboto', 10)\n\t\t_padsz = (3, 3)\n\t\t_layout = [ [ sg.Text( ) ],\n\t\t            [ sg.Text( f'{_msg}', size=(100, 1), key='-MSG-', text_color=_red,\n\t\t\t            font=_font ) ],\n\t\t            [ sg.Text( size=(150, 1) ) ],\n\t\t            [ sg.Multiline( f'{_info}', key='-INFO-', size=(80, 7), pad=_padsz ) ],\n\t\t            [ sg.Text( ) ],\n\t\t            [ sg.Text( size=(20, 1) ), sg.Cancel( size=(15, 1), key='-CANCEL-' ),\n\t\t              sg.Text( size=(10, 1) ), sg.Ok( size=(15, 1), key='-OK-' ) ] ]\n\n\t\t_window = sg.Window( r' Mathy', _layout,\n\t\t\ticon=self.icon_path,\n\t\t\tfont=self.theme_font,\n\t\t\tsize=self.form_size )\n\n\t\twhile True:\n\t\t\t_event, _values = _window.read( )\n\t\t\tif _event in (sg.WIN_CLOSED, sg.WIN_X_EVENT, 'Canel', '-OK-'):\n\t\t\t\tbreak\n\n\t\t_window.close( )\n</code></pre>"},{"location":"reference/api/#enums","title":"enums","text":"<p>Assembly:                Mathy   Filename:                enums.py   Author:                  Terry D. Eppler   Created:                 05-31-2022</p> <p>Last Modified By:        Terry D. Eppler   Last Modified On:        05-01-2025</p> <p> <pre><code> Boo\n</code></pre> <p>Permission is hereby granted, free of charge, to any person obtaining a copy  of this software and associated documentation files (the \u201cSoftware\u201d),  to deal in the Software without restriction,  including without limitation the rights to use,  copy, modify, merge, publish, distribute, sublicense,  and/or sell copies of the Software,  and to permit persons to whom the Software is furnished to do so,  subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all  copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,  DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER  DEALINGS IN THE SOFTWARE.</p> <p>You can contact me at:  terryeppler@gmail.com or eppler.terry@epa.gov</p> <p></p>          enums.py  <p>options: show_root_heading: true</p>"},{"location":"reference/api/#enums.Client","title":"Client","text":"<p>               Bases: <code>Enum</code></p> Purpose <p>Enumeration of auxiliary applications</p> Source code in <code>enums.py</code> <pre><code>class Client( Enum ):\n\t'''\n\n\t\tPurpose:\n\t\t\tEnumeration of auxiliary applications\n\n\t'''\n\tSQLite = auto( )\n\tAccess = auto( )\n\tExcel = auto( )\n\tWord = auto( )\n\tEdge = auto( )\n\tChrome = auto( )\n\tControlPanel = auto( )\n\tCalculator = auto( )\n\tOutlook = auto( )\n\tPyscripter = auto( )\n\tTaskManager = auto( )\n\tStorage = auto( )\n</code></pre>"},{"location":"reference/api/#enums.Scaler","title":"Scaler","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of scaling algorythms</p> Source code in <code>enums.py</code> <pre><code>class Scaler( Enum ):\n\t'''\n\n\t\tEnumeration of scaling algorythms\n\n\t'''\n\tSimple = auto( )\n\tStandard = auto( )\n\tNormal = auto( )\n\tOneHot = auto( )\n\tNeighbor = auto( )\n\tMinMax = auto( )\n\tOrdinal = auto( )\n\tRobust = auto( )\n</code></pre>"},{"location":"reference/api/#minions","title":"minions","text":"<p>Assembly:                Mathy   Filename:                {minions.py   Author:                  Terry D. Eppler   Created:                 05-31-2022</p> <p>Last Modified By:        Terry D. Eppler   Last Modified On:        05-01-2025</p> <p> <pre><code> Mathy Minions\n</code></pre> <p>Permission is hereby granted, free of charge, to any person obtaining a copy  of this software and associated documentation files (the \u201cSoftware\u201d),  to deal in the Software without restriction,  including without limitation the rights to use,  copy, modify, merge, publish, distribute, sublicense,  and/or sell copies of the Software,  and to permit persons to whom the Software is furnished to do so,  subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all  copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,  DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER  DEALINGS IN THE SOFTWARE.</p> <p>You can contact me at:  terryeppler@gmail.com or eppler.terry@epa.gov</p> <p></p>          minions.py  <p>options: show_root_heading: true</p>"},{"location":"reference/api/#minions.App","title":"App","text":""},{"location":"reference/api/#minions.App--constructor","title":"Constructor:","text":"<p>App( client: enum )</p>"},{"location":"reference/api/#minions.App--purpose","title":"Purpose:","text":"<p>Class defines object providing factory methods run( ) and run( args ) that run processes based on 'Client' enumeration path args</p> Source code in <code>minions.py</code> <pre><code>class App( ):\n\t'''\n\n\t\tConstructor:\n\t\t-------------\n\t\tApp( client: enum )\n\n\n\t\tPurpose:\n\t\t---------\n\t\tClass defines object providing factory methods run( ) and run( args ) that run\n\t\tprocesses based on 'Client' enumeration path args\n\n\t'''\n\n\tdef __init__( self, client: Client ):\n\t\tself.app = client\n\t\tself.sqlite = r'target_values\\sqlite\\gui\\SQLiteDatabaseBrowserPortable.exe'\n\t\tself.access = r'C:\\Program Files\\Microsoft Office\\root\\Office16\\MSACCESS.EXE'\n\t\tself.excel = r'C:\\Program Files\\Microsoft Office\\root\\Office16\\EXCEL.EXE'\n\t\tself.edge = r'C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe'\n\t\tself.chrome = r'C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe'\n\t\tself.control_panel = r'C:\\Windows\\System32\\control.exe'\n\t\tself.calculator = r'C:\\Windows\\System32\\calc.exe'\n\t\tself.outlook = r'C:\\Program Files\\Microsoft Office\\root\\Office16\\OUTLOOK.EXE'\n\t\tself.pyscripter = r'target_values\\python\\PyScripter\\PyScripter.exe'\n\t\tself.task_manager = r'C:\\Windows\\System32\\Taskmgr.exe'\n\t\tself.storage = r'C:\\Users\\terry\\AppData\\Local\\Microsoft\\OneDrive\\OneDrive.exe'\n\t\tself.word = r'C:\\Program Files\\Microsoft Office\\root\\Office16\\WINWORD.EXE'\n\n\tdef __dir__( self ) -&gt; list[ str ]:\n\t\treturn [ 'sqlite', 'access', 'excel', 'chrome',\n\t\t         'edge', 'control', 'calculator', 'task_manager',\n\t\t         'outlook', 'pyscripter', 'storage', 'word',\n\t\t         'run', 'run_args' ]\n\n\tdef run( self ):\n\t\t'''\n\n\t\t\tPurpose:\n\t\t\t---------\n\n\t\t\tParameters:\n\t\t\t----------\n\n\t\t\tReturns:\n\t\t\t-------\n\n\t\t'''\n\t\ttry:\n\t\t\tif self.app == Client.SQLite:\n\t\t\t\tsp.Popen( self.sqlite )\n\t\t\telif self.app == Client.Access:\n\t\t\t\tsp.Popen( self.access )\n\t\t\telif self.app == Client.Excel:\n\t\t\t\tsp.Popen( self.excel )\n\t\t\telif self.app == Client.Edge:\n\t\t\t\tsp.Popen( self.edge )\n\t\t\telif self.app == Client.Chrome:\n\t\t\t\tsp.Popen( self.chrome )\n\t\t\telif self.app == Client.ControlPanel:\n\t\t\t\tsp.Popen( self.control_panel )\n\t\t\telif self.app == Client.Calculator:\n\t\t\t\tsp.Popen( self.calculator )\n\t\t\telif self.app == Client.Outlook:\n\t\t\t\tsp.Popen( self.outlook )\n\t\t\telif self.app == Client.Pyscripter:\n\t\t\t\tsp.Popen( self.pyscripter )\n\t\t\telif self.app == Client.TaskManager:\n\t\t\t\tsp.Popen( self.task_manager )\n\t\t\telif self.app == Client.Storage:\n\t\t\t\tsp.Popen( self.storage )\n\t\t\telif self.app == Client.Word:\n\t\t\t\tsp.Popen( self.word )\n\t\texcept Exception as e:\n\t\t\t_exc = Error( e )\n\t\t\t_exc.module = 'Minion'\n\t\t\t_exc.cause = 'App'\n\t\t\t_exc.method = 'run( self )'\n\t\t\t_err = ErrorDialog( _exc )\n\t\t\t_err.show( )\n\n\tdef run_args( self, args: str ):\n\t\t'''\n\n\t\t\tPurpose:\n\t\t\t-------\n\n\t\t\tParameters:\n\t\t\t-----------\n\n\t\t\tReturns:\n\t\t\t---------\n\n\t\t'''\n\t\ttry:\n\t\t\tif args is not None and self.app == Client.SQLite:\n\t\t\t\tif os.path.isfile( args ):\n\t\t\t\t\tsp.Popen( [ self.sqlite, args ] )\n\t\t\telif args is not None and self.app == Client.Access:\n\t\t\t\tif os.path.isfile( args ):\n\t\t\t\t\tsp.Popen( [ self.access, args ] )\n\t\t\telif args is not None and self.app == Client.Excel:\n\t\t\t\tif os.path.isfile( args ):\n\t\t\t\t\tsp.Popen( [ self.excel, args ] )\n\t\t\telif args is not None and self.app == Client.Edge:\n\t\t\t\tsp.Popen( args )\n\t\t\telif args is not None and self.app == Client.Chrome:\n\t\t\t\tsp.Popen( [ self.chrome, args ] )\n\t\t\telif args is not None and self.app == Client.Outlook:\n\t\t\t\tsp.Popen( [ self.outlook, args ] )\n\t\t\telif args is not None and self.app == Client.Pyscripter:\n\t\t\t\tsp.Popen( [ self.pyscripter, args ] )\n\t\t\telif args is not None and self.app == Client.Word:\n\t\t\t\tsp.Popen( [ self.__word, args ] )\n\t\t\telif args is not None and self.app == Client.TaskManager:\n\t\t\t\tsp.Popen( [ self.task_manager, args ] )\n\t\t\telif args is not None and self.app == Client.ControlPanel:\n\t\t\t\tsp.Popen( [ self.control_panel, args ] )\n\t\t\telif args is not None and self.app == Client.Storage:\n\t\t\t\tsp.Popen( [ self.storage, args ] )\n\t\texcept Exception as e:\n\t\t\t_exc = Error( e )\n\t\t\t_exc.module = 'minion'\n\t\t\t_exc.cause = 'App'\n\t\t\t_exc.method = 'run_args( self, args )'\n\t\t\t_err = ErrorDialog( _exc )\n\t\t\t_err.show( )\n</code></pre>"},{"location":"reference/api/#minions.App.run","title":"run","text":"<pre><code>run()\n</code></pre>"},{"location":"reference/api/#minions.App.run--purpose","title":"Purpose:","text":""},{"location":"reference/api/#minions.App.run--parameters","title":"Parameters:","text":""},{"location":"reference/api/#minions.App.run--returns","title":"Returns:","text":"Source code in <code>minions.py</code> <pre><code>def run( self ):\n\t'''\n\n\t\tPurpose:\n\t\t---------\n\n\t\tParameters:\n\t\t----------\n\n\t\tReturns:\n\t\t-------\n\n\t'''\n\ttry:\n\t\tif self.app == Client.SQLite:\n\t\t\tsp.Popen( self.sqlite )\n\t\telif self.app == Client.Access:\n\t\t\tsp.Popen( self.access )\n\t\telif self.app == Client.Excel:\n\t\t\tsp.Popen( self.excel )\n\t\telif self.app == Client.Edge:\n\t\t\tsp.Popen( self.edge )\n\t\telif self.app == Client.Chrome:\n\t\t\tsp.Popen( self.chrome )\n\t\telif self.app == Client.ControlPanel:\n\t\t\tsp.Popen( self.control_panel )\n\t\telif self.app == Client.Calculator:\n\t\t\tsp.Popen( self.calculator )\n\t\telif self.app == Client.Outlook:\n\t\t\tsp.Popen( self.outlook )\n\t\telif self.app == Client.Pyscripter:\n\t\t\tsp.Popen( self.pyscripter )\n\t\telif self.app == Client.TaskManager:\n\t\t\tsp.Popen( self.task_manager )\n\t\telif self.app == Client.Storage:\n\t\t\tsp.Popen( self.storage )\n\t\telif self.app == Client.Word:\n\t\t\tsp.Popen( self.word )\n\texcept Exception as e:\n\t\t_exc = Error( e )\n\t\t_exc.module = 'Minion'\n\t\t_exc.cause = 'App'\n\t\t_exc.method = 'run( self )'\n\t\t_err = ErrorDialog( _exc )\n\t\t_err.show( )\n</code></pre>"},{"location":"reference/api/#minions.App.run_args","title":"run_args","text":"<pre><code>run_args(args)\n</code></pre>"},{"location":"reference/api/#minions.App.run_args--purpose","title":"Purpose:","text":""},{"location":"reference/api/#minions.App.run_args--parameters","title":"Parameters:","text":""},{"location":"reference/api/#minions.App.run_args--returns","title":"Returns:","text":"Source code in <code>minions.py</code> <pre><code>def run_args( self, args: str ):\n\t'''\n\n\t\tPurpose:\n\t\t-------\n\n\t\tParameters:\n\t\t-----------\n\n\t\tReturns:\n\t\t---------\n\n\t'''\n\ttry:\n\t\tif args is not None and self.app == Client.SQLite:\n\t\t\tif os.path.isfile( args ):\n\t\t\t\tsp.Popen( [ self.sqlite, args ] )\n\t\telif args is not None and self.app == Client.Access:\n\t\t\tif os.path.isfile( args ):\n\t\t\t\tsp.Popen( [ self.access, args ] )\n\t\telif args is not None and self.app == Client.Excel:\n\t\t\tif os.path.isfile( args ):\n\t\t\t\tsp.Popen( [ self.excel, args ] )\n\t\telif args is not None and self.app == Client.Edge:\n\t\t\tsp.Popen( args )\n\t\telif args is not None and self.app == Client.Chrome:\n\t\t\tsp.Popen( [ self.chrome, args ] )\n\t\telif args is not None and self.app == Client.Outlook:\n\t\t\tsp.Popen( [ self.outlook, args ] )\n\t\telif args is not None and self.app == Client.Pyscripter:\n\t\t\tsp.Popen( [ self.pyscripter, args ] )\n\t\telif args is not None and self.app == Client.Word:\n\t\t\tsp.Popen( [ self.__word, args ] )\n\t\telif args is not None and self.app == Client.TaskManager:\n\t\t\tsp.Popen( [ self.task_manager, args ] )\n\t\telif args is not None and self.app == Client.ControlPanel:\n\t\t\tsp.Popen( [ self.control_panel, args ] )\n\t\telif args is not None and self.app == Client.Storage:\n\t\t\tsp.Popen( [ self.storage, args ] )\n\texcept Exception as e:\n\t\t_exc = Error( e )\n\t\t_exc.module = 'minion'\n\t\t_exc.cause = 'App'\n\t\t_exc.method = 'run_args( self, args )'\n\t\t_err = ErrorDialog( _exc )\n\t\t_err.show( )\n</code></pre>"},{"location":"reference/api/#minions.Client","title":"Client","text":"<p>               Bases: <code>Enum</code></p> Purpose <p>Enumeration of auxiliary applications</p> Source code in <code>enums.py</code> <pre><code>class Client( Enum ):\n\t'''\n\n\t\tPurpose:\n\t\t\tEnumeration of auxiliary applications\n\n\t'''\n\tSQLite = auto( )\n\tAccess = auto( )\n\tExcel = auto( )\n\tWord = auto( )\n\tEdge = auto( )\n\tChrome = auto( )\n\tControlPanel = auto( )\n\tCalculator = auto( )\n\tOutlook = auto( )\n\tPyscripter = auto( )\n\tTaskManager = auto( )\n\tStorage = auto( )\n</code></pre>"},{"location":"reference/api/#minions.Scaler","title":"Scaler","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of scaling algorythms</p> Source code in <code>enums.py</code> <pre><code>class Scaler( Enum ):\n\t'''\n\n\t\tEnumeration of scaling algorythms\n\n\t'''\n\tSimple = auto( )\n\tStandard = auto( )\n\tNormal = auto( )\n\tOneHot = auto( )\n\tNeighbor = auto( )\n\tMinMax = auto( )\n\tOrdinal = auto( )\n\tRobust = auto( )\n</code></pre>"},{"location":"reference/api/#processing","title":"processing","text":"<pre><code>Assembly:                Boo\nFilename:                processing.py\nAuthor:                  Terry D. Eppler\nCreated:                 05-31-2022\n\nLast Modified By:        Terry D. Eppler\nLast Modified On:        05-01-2025\n</code></pre> <p> <pre><code>       Boo is a df analysis tool that integrates various Generative AI, Text-Processing, and\n       Machine-Learning algorithms for federal analysts.\n       Copyright \u00a9  2022  Terry Eppler\n</code></pre> <p>Permission is hereby granted, free of charge, to any person obtaining a copy    of this software and associated documentation files (the \u201cSoftware\u201d),    to deal in the Software without restriction,    including without limitation the rights to use,    copy, modify, merge, publish, distribute, sublicense,    and/or sell copies of the Software,    and to permit persons to whom the Software is furnished to do so,    subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all    copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,    INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,    FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.    IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,    DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,    ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER    DEALINGS IN THE SOFTWARE.</p> <p>You can contact me at:  terryeppler@gmail.com or eppler.terry@epa.gov</p> <p></p>    processing.py  <p>options: show_root_heading: true</p>"},{"location":"reference/api/#processing.PDF","title":"PDF","text":"<p>               Bases: <code>Processor</code></p> <pre><code>Purpose:\n--------\nA utility class for extracting clean pages from PDF files into a list of strings.\nHandles nuances such as layout artifacts, page separation, optional filtering,\nand includes df detection capabilities.\n</code></pre>"},{"location":"reference/api/#processing.PDF--methods","title":"Methods:","text":"<p>extract_lines( self, path, max: int=None) -&gt; List[ str ] extract_text( self, path, max: int=None) -&gt; str export_csv( self, tables: List[ pd.DataFrame ], filename: str=None ) -&gt; None export_text( self, words: List[ str ], path: str=None ) -&gt; None export_excel( self, tables: List[ pd.DataFrame ], path: str=None ) -&gt; None</p> Source code in <code>processing.py</code> <pre><code>class PDF( Processor ):\n\t\"\"\"\n\n\t\tPurpose:\n\t\t--------\n\t\tA utility class for extracting clean pages from PDF files into a list of strings.\n\t\tHandles nuances such as layout artifacts, page separation, optional filtering,\n\t\tand includes df detection capabilities.\n\n\n\t    Methods:\n\t    --------\n\t    extract_lines( self, path, max: int=None) -&gt; List[ str ]\n\t    extract_text( self, path, max: int=None) -&gt; str\n\t    export_csv( self, tables: List[ pd.DataFrame ], filename: str=None ) -&gt; None\n\t    export_text( self, words: List[ str ], path: str=None ) -&gt; None\n\t    export_excel( self, tables: List[ pd.DataFrame ], path: str=None ) -&gt; None\n\n\t\"\"\"\n\tstrip_headers: Optional[ bool ]\n\tminimum_length: Optional[ int ]\n\textract_tables: Optional[ bool ]\n\textracted_lines: Optional[ List ]\n\textracted_tables: Optional[ List ]\n\textracted_pages: Optional[ List ]\n\n\tdef __init__( self, headers: bool=False, min: int=10, tables: bool=True ) -&gt; None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t-----------\n\t\t\tInitialize the PDF pages extractor with configurable settings.\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- headers (bool): If True, attempts to strip recurring headers/footers.\n\t\t\t- min (int): Minimum num of characters for a line to be included.\n\t\t\t- tables (bool): If True, extract pages from detected tables using block\n\t\t\tgrouping.\n\n\t\t\"\"\"\n\t\tsuper( ).__init__( )\n\t\tself.strip_headers = headers\n\t\tself.minimum_length = min\n\t\tself.extract_tables = tables\n\t\tself.pages = [ ]\n\t\tself.lines = [ ]\n\t\tself.clean_lines = [ ]\n\t\tself.extracted_lines = [ ]\n\t\tself.extracted_tables = [ ]\n\t\tself.extracted_pages = [ ]\n\t\tself.tables = None\n\t\tself.file_path = ''\n\t\tself.page = ''\n\n\tdef __dir__( self ) -&gt; List[ str ] | None:\n\t\t'''\n\n\t\t\tPurpose:\n\t\t\t---------\n\t\t\tProvides a list of strings representing class members.\n\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- self\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- List[ str ] | None\n\n\t\t'''\n\t\treturn [ 'strip_headers', 'minimum_length', 'extract_tables',\n\t\t         'path', 'page', 'pages', 'words', 'clean_lines', 'extracted_lines',\n\t\t         'extracted_tables', 'extracted_pages', 'extract_lines',\n\t\t         'extract_text', 'export_csv', 'export_text', 'export_excel' ]\n\n\tdef extract_lines( self, path: str, max: Optional[ int ]=None ) -&gt; List[ str ] | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t--------\n\t\t\tExtract words of pages from a PDF,\n\t\t\toptionally limiting to the first N pages.\n\n\t\t\tParameters:\n\t\t\t----------\n\t\t\t- path (str): Path to the PDF file\n\t\t\t- max (Optional[int]): Max num of pages to process (None for all pages)\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- List[str]: Cleaned list of non-empty words\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif path is None:\n\t\t\t\traise Exception( 'The argument \"path\" must be specified' )\n\t\t\telse:\n\t\t\t\tself.file_path = path\n\t\t\t\tself.extracted_lines = [ ]\n\t\t\t\twith fitz.open( self.file_path ) as doc:\n\t\t\t\t\tfor i, page in enumerate( doc ):\n\t\t\t\t\t\tif max is not None and i &gt;= max:\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\tif self.extract_tables:\n\t\t\t\t\t\t\tpage_lines = self._extract_tables( page )\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t_text = page.get_text( 'text' )\n\t\t\t\t\t\t\tpage_lines = _text.splitlines( )\n\t\t\t\t\t\tfiltered = self._filter_lines( page_lines )\n\t\t\t\t\t\tself.extracted_lines.extend( filtered )\n\t\t\t\treturn self.extracted_lines\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'PDF'\n\t\t\texception.method = ('extract_lines( self, path: str, max: Optional[ int ]=None ) -&gt; '\n\t\t\t                    'List[ str ]')\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef _extract_tables( self, page: Page ) -&gt; List[ str ] | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t--------\n\t\t\tAttempt to extract structured blocks\n\t\t\tsuch as tables using spatial grouping.\n\n\t\t\tParameters:\n\t\t\t----------\n\t\t\t- page: PyMuPDF page object\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- List[str]: Grouped blocks including potential tables\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif page is None:\n\t\t\t\traise Exception( 'The argument \"page\" cannot be None' )\n\t\t\telse:\n\t\t\t\ttf = page.find_tables( )\n\t\t\t\tlines = [ ]\n\t\t\t\tfor t in getattr( tf, 'tables', [ ] ):\n\t\t\t\t\tdf = pd.DataFrame( t.extract( ) )  # or t.to_pandas()\n\t\t\t\t\tfor row in df.values.tolist( ):\n\t\t\t\t\t\tlines.append( ' '.join( map( str, row ) ) )\n\t\t\t\treturn lines\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'PDF'\n\t\t\texception.method = '_extract_tables( self, page ) -&gt; List[ str ]:'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef _filter_lines( self, lines: List[ str ] ) -&gt; List[ str ] | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t-----------\n\t\t\tFilter and clean words from a page of pages.\n\n\t\t\tParameters:\n\t\t\t- words (List[str]): Raw words of pages\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- List[str]: Filtered, non-trivial words\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif lines is None:\n\t\t\t\traise Exception( 'The argument \"lines\" is None' )\n\t\t\telse:\n\t\t\t\tself.lines = lines\n\t\t\t\tclean = [ ]\n\t\t\t\tfor line in lines:\n\t\t\t\t\tline = line.strip( )\n\t\t\t\t\tif len( line ) &lt; self.minimum_length:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif self.strip_headers and self._has_repeating_header( line ):\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tclean.append( line )\n\t\t\t\treturn clean\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'PDF'\n\t\t\texception.method = '_filter_lines( self, words: List[ str ] ) -&gt; List[ str ]'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef _has_repeating_header( self, line: str ) -&gt; bool | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t--------\n\t\t\tHeuristic to detect common headers/footers (basic implementation).\n\n\t\t\tParameters:\n\t\t\t- line (str): A line of pages\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- bool: True if line is likely a header or footer\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif line is None:\n\t\t\t\traise Exception( 'The argument \"line\" is None' )\n\t\t\telse:\n\t\t\t\t_keywords = [ 'page', 'public law', 'u.s. government', 'united states' ]\n\t\t\t\treturn any( kw in line.lower( ) for kw in _keywords )\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'PDF'\n\t\t\texception.method = '_has_repeating_header( self, line: str ) -&gt; bool'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef extract_text( self, path: str, max: Optional[ int ]=None ) -&gt; str | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t---------\n\t\t\tExtract the entire pages from a PDF into one continuous path.\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- path (str): Path to the PDF file\n\t\t\t- max (Optional[int]): Maximum num of pages to process\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- str: Full concatenated pages\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif path is None:\n\t\t\t\traise Exception( 'The argument \"path\" must be specified' )\n\t\t\telse:\n\t\t\t\tif max is not None and max &gt; 0:\n\t\t\t\t\tself.file_path = path\n\t\t\t\t\tself.lines = self.extract_lines( self.file_path, max=max )\n\t\t\t\t\treturn '\\n'.join( self.lines )\n\t\t\t\telif max is None or max &lt;= 0:\n\t\t\t\t\tself.file_path = path\n\t\t\t\t\tself.lines = self.extract_lines( self.file_path )\n\t\t\t\t\treturn '\\n'.join( self.lines )\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'PDF'\n\t\t\texception.method = 'extract_text( self, path: str, max: Optional[ int ]=None ) -&gt; str:'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef extract_tables( self, path: str, max: Optional[ int ]=None ) -&gt; (\n\t\t\tList[ pd.DataFrame ] | None):\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t-----------\n\t\t\tExtract tables from the PDF and return them as a list of DataFrames.\n\n\t\t\tParameters:\n\t\t\t- path (str): Path to the PDF file\n\t\t\t- max (Optional[int]): Maximum num of pages to process\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- List[pd.DataFrame]: List of DataFrames representing detected tables\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif path is None:\n\t\t\t\traise Exception( 'The argument \"path\" must be specified' )\n\t\t\telif max is None:\n\t\t\t\traise Exception( 'The argument \"max\" must be specified' )\n\t\t\telse:\n\t\t\t\tself.file_path = path\n\t\t\t\tself.tables = [ ]\n\t\t\t\twith fitz.open( self.file_path ) as doc:\n\t\t\t\t\tfor i, page in enumerate( doc ):\n\t\t\t\t\t\tif max is not None and i &gt;= max:\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\ttf = page.find_tables( )\n\t\t\t\t\t\tfor t in getattr( tf, 'tables', [ ] ):\n\t\t\t\t\t\t\tself.tables.append( pd.DataFrame( t.extract( ) ) )\n\t\t\t\treturn self.tables\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'PDF'\n\t\t\texception.method = (\n\t\t\t\t\t'extract_tables( self, path: str, max: Optional[ int ] = None ) -&gt; List[ '\n\t\t\t\t\t'pd.DataFrame ]')\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef export_csv( self, tables: List[ pd.DataFrame ], filename: str ) -&gt; None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t-----------\n\t\t\tExport a list of DataFrames (tables) to individual CSV files.\n\n\t\t\tParameters:\n\t\t\t- tables (List[pd.DataFrame]): List of tables to export\n\t\t\t- filename (str): Prefix for output filenames (e.g., 'output_table')\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif tables is None:\n\t\t\t\traise Exception( 'The argument \"tables\" must not be None' )\n\t\t\telif filename is None:\n\t\t\t\traise Exception( 'The argument \"filename\" must not be None' )\n\t\t\telse:\n\t\t\t\tself.tables = tables\n\t\t\t\tfor i, df in enumerate( self.tables ):\n\t\t\t\t\tdf.to_csv( f'{filename}_{i + 1}.csv', index = False )\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'PDF'\n\t\t\texception.method = (\n\t\t\t\t\t'export_csv( self, tables: List[ pd.DataFrame ], filename: str ) -&gt; '\n\t\t\t\t\t'None')\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef export_text( self, lines: List[ str ], path: str ) -&gt; None:\n\t\t\"\"\"\n\n\t\t\tExport extracted words of\n\t\t\tpages to a plain pages file.\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- words (List[str]): List of pages words\n\t\t\t- path (str): Path to output pages file\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif lines is None:\n\t\t\t\traise Exception( 'The argument \"words\" must be provided.' )\n\t\t\telif path is None:\n\t\t\t\traise Exception( 'The argument \"path\" must be provided.' )\n\t\t\telse:\n\t\t\t\tself.file_path = path\n\t\t\t\tself.lines = lines\n\t\t\t\twith open( self.file_path, 'w', encoding='utf-8', errors='ignore' ) as f:\n\t\t\t\t\tfor line in self.lines:\n\t\t\t\t\t\tf.write( line + '\\n' )\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'PDF'\n\t\t\texception.method = 'export_text( self, lines: List[ str ], path: str ) -&gt; None'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef export_excel( self, tables: List[ pd.DataFrame ], path: str ) -&gt; None:\n\t\t\"\"\"\n\n\t\t\tExport all extracted tables into a single\n\t\t\tExcel workbook with one sheet per df.\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- tables (List[pd.DataFrame]): List of tables to export\n\t\t\t- path (str): Path to the output Excel file\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif tables is None:\n\t\t\t\traise Exception( 'The argument \"tables\" must not be None' )\n\t\t\telif path is None:\n\t\t\t\traise Exception( 'The argument \"path\" must not be None' )\n\t\t\telse:\n\t\t\t\tself.tables = tables\n\t\t\t\tself.file_path = path\n\t\t\t\twith pd.ExcelWriter( self.file_path, engine='xlsxwriter' ) as _writer:\n\t\t\t\t\tfor i, df in enumerate( self.tables ):\n\t\t\t\t\t\t_sheet = f'Table_{i + 1}'\n\t\t\t\t\t\tdf.to_excel( _writer, sheet_name=_sheet, index=False )\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'PDF'\n\t\t\texception.method = ('export_excel( self, tables: List[ pd.DataFrame ], path: str ) -&gt; '\n\t\t\t                    'None')\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.PDF.__dir__","title":"__dir__","text":"<pre><code>__dir__()\n</code></pre>"},{"location":"reference/api/#processing.PDF.__dir__--purpose","title":"Purpose:","text":"<p>Provides a list of strings representing class members.</p>"},{"location":"reference/api/#processing.PDF.__dir__--parameters","title":"Parameters:","text":"<ul> <li>self</li> </ul>"},{"location":"reference/api/#processing.PDF.__dir__--returns","title":"Returns:","text":"<ul> <li>List[ str ] | None</li> </ul> Source code in <code>processing.py</code> <pre><code>def __dir__( self ) -&gt; List[ str ] | None:\n\t'''\n\n\t\tPurpose:\n\t\t---------\n\t\tProvides a list of strings representing class members.\n\n\n\t\tParameters:\n\t\t-----------\n\t\t- self\n\n\t\tReturns:\n\t\t--------\n\t\t- List[ str ] | None\n\n\t'''\n\treturn [ 'strip_headers', 'minimum_length', 'extract_tables',\n\t         'path', 'page', 'pages', 'words', 'clean_lines', 'extracted_lines',\n\t         'extracted_tables', 'extracted_pages', 'extract_lines',\n\t         'extract_text', 'export_csv', 'export_text', 'export_excel' ]\n</code></pre>"},{"location":"reference/api/#processing.PDF.__init__","title":"__init__","text":"<pre><code>__init__(headers=False, min=10, tables=True)\n</code></pre>"},{"location":"reference/api/#processing.PDF.__init__--purpose","title":"Purpose:","text":"<p>Initialize the PDF pages extractor with configurable settings.</p>"},{"location":"reference/api/#processing.PDF.__init__--parameters","title":"Parameters:","text":"<ul> <li>headers (bool): If True, attempts to strip recurring headers/footers.</li> <li>min (int): Minimum num of characters for a line to be included.</li> <li>tables (bool): If True, extract pages from detected tables using block grouping.</li> </ul> Source code in <code>processing.py</code> <pre><code>def __init__( self, headers: bool=False, min: int=10, tables: bool=True ) -&gt; None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t-----------\n\t\tInitialize the PDF pages extractor with configurable settings.\n\n\t\tParameters:\n\t\t-----------\n\t\t- headers (bool): If True, attempts to strip recurring headers/footers.\n\t\t- min (int): Minimum num of characters for a line to be included.\n\t\t- tables (bool): If True, extract pages from detected tables using block\n\t\tgrouping.\n\n\t\"\"\"\n\tsuper( ).__init__( )\n\tself.strip_headers = headers\n\tself.minimum_length = min\n\tself.extract_tables = tables\n\tself.pages = [ ]\n\tself.lines = [ ]\n\tself.clean_lines = [ ]\n\tself.extracted_lines = [ ]\n\tself.extracted_tables = [ ]\n\tself.extracted_pages = [ ]\n\tself.tables = None\n\tself.file_path = ''\n\tself.page = ''\n</code></pre>"},{"location":"reference/api/#processing.PDF.export_csv","title":"export_csv","text":"<pre><code>export_csv(tables, filename)\n</code></pre>"},{"location":"reference/api/#processing.PDF.export_csv--purpose","title":"Purpose:","text":"<p>Export a list of DataFrames (tables) to individual CSV files.</p> <p>Parameters: - tables (List[pd.DataFrame]): List of tables to export - filename (str): Prefix for output filenames (e.g., 'output_table')</p> Source code in <code>processing.py</code> <pre><code>def export_csv( self, tables: List[ pd.DataFrame ], filename: str ) -&gt; None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t-----------\n\t\tExport a list of DataFrames (tables) to individual CSV files.\n\n\t\tParameters:\n\t\t- tables (List[pd.DataFrame]): List of tables to export\n\t\t- filename (str): Prefix for output filenames (e.g., 'output_table')\n\n\t\"\"\"\n\ttry:\n\t\tif tables is None:\n\t\t\traise Exception( 'The argument \"tables\" must not be None' )\n\t\telif filename is None:\n\t\t\traise Exception( 'The argument \"filename\" must not be None' )\n\t\telse:\n\t\t\tself.tables = tables\n\t\t\tfor i, df in enumerate( self.tables ):\n\t\t\t\tdf.to_csv( f'{filename}_{i + 1}.csv', index = False )\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'PDF'\n\t\texception.method = (\n\t\t\t\t'export_csv( self, tables: List[ pd.DataFrame ], filename: str ) -&gt; '\n\t\t\t\t'None')\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.PDF.export_excel","title":"export_excel","text":"<pre><code>export_excel(tables, path)\n</code></pre> <p>Export all extracted tables into a single Excel workbook with one sheet per df.</p>"},{"location":"reference/api/#processing.PDF.export_excel--parameters","title":"Parameters:","text":"<ul> <li>tables (List[pd.DataFrame]): List of tables to export</li> <li>path (str): Path to the output Excel file</li> </ul> Source code in <code>processing.py</code> <pre><code>def export_excel( self, tables: List[ pd.DataFrame ], path: str ) -&gt; None:\n\t\"\"\"\n\n\t\tExport all extracted tables into a single\n\t\tExcel workbook with one sheet per df.\n\n\t\tParameters:\n\t\t-----------\n\t\t- tables (List[pd.DataFrame]): List of tables to export\n\t\t- path (str): Path to the output Excel file\n\n\t\"\"\"\n\ttry:\n\t\tif tables is None:\n\t\t\traise Exception( 'The argument \"tables\" must not be None' )\n\t\telif path is None:\n\t\t\traise Exception( 'The argument \"path\" must not be None' )\n\t\telse:\n\t\t\tself.tables = tables\n\t\t\tself.file_path = path\n\t\t\twith pd.ExcelWriter( self.file_path, engine='xlsxwriter' ) as _writer:\n\t\t\t\tfor i, df in enumerate( self.tables ):\n\t\t\t\t\t_sheet = f'Table_{i + 1}'\n\t\t\t\t\tdf.to_excel( _writer, sheet_name=_sheet, index=False )\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'PDF'\n\t\texception.method = ('export_excel( self, tables: List[ pd.DataFrame ], path: str ) -&gt; '\n\t\t                    'None')\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.PDF.export_text","title":"export_text","text":"<pre><code>export_text(lines, path)\n</code></pre> <p>Export extracted words of pages to a plain pages file.</p>"},{"location":"reference/api/#processing.PDF.export_text--parameters","title":"Parameters:","text":"<ul> <li>words (List[str]): List of pages words</li> <li>path (str): Path to output pages file</li> </ul> Source code in <code>processing.py</code> <pre><code>def export_text( self, lines: List[ str ], path: str ) -&gt; None:\n\t\"\"\"\n\n\t\tExport extracted words of\n\t\tpages to a plain pages file.\n\n\t\tParameters:\n\t\t-----------\n\t\t- words (List[str]): List of pages words\n\t\t- path (str): Path to output pages file\n\n\t\"\"\"\n\ttry:\n\t\tif lines is None:\n\t\t\traise Exception( 'The argument \"words\" must be provided.' )\n\t\telif path is None:\n\t\t\traise Exception( 'The argument \"path\" must be provided.' )\n\t\telse:\n\t\t\tself.file_path = path\n\t\t\tself.lines = lines\n\t\t\twith open( self.file_path, 'w', encoding='utf-8', errors='ignore' ) as f:\n\t\t\t\tfor line in self.lines:\n\t\t\t\t\tf.write( line + '\\n' )\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'PDF'\n\t\texception.method = 'export_text( self, lines: List[ str ], path: str ) -&gt; None'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.PDF.extract_lines","title":"extract_lines","text":"<pre><code>extract_lines(path, max=None)\n</code></pre>"},{"location":"reference/api/#processing.PDF.extract_lines--purpose","title":"Purpose:","text":"<p>Extract words of pages from a PDF, optionally limiting to the first N pages.</p>"},{"location":"reference/api/#processing.PDF.extract_lines--parameters","title":"Parameters:","text":"<ul> <li>path (str): Path to the PDF file</li> <li>max (Optional[int]): Max num of pages to process (None for all pages)</li> </ul>"},{"location":"reference/api/#processing.PDF.extract_lines--returns","title":"Returns:","text":"<ul> <li>List[str]: Cleaned list of non-empty words</li> </ul> Source code in <code>processing.py</code> <pre><code>def extract_lines( self, path: str, max: Optional[ int ]=None ) -&gt; List[ str ] | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t--------\n\t\tExtract words of pages from a PDF,\n\t\toptionally limiting to the first N pages.\n\n\t\tParameters:\n\t\t----------\n\t\t- path (str): Path to the PDF file\n\t\t- max (Optional[int]): Max num of pages to process (None for all pages)\n\n\t\tReturns:\n\t\t--------\n\t\t- List[str]: Cleaned list of non-empty words\n\n\t\"\"\"\n\ttry:\n\t\tif path is None:\n\t\t\traise Exception( 'The argument \"path\" must be specified' )\n\t\telse:\n\t\t\tself.file_path = path\n\t\t\tself.extracted_lines = [ ]\n\t\t\twith fitz.open( self.file_path ) as doc:\n\t\t\t\tfor i, page in enumerate( doc ):\n\t\t\t\t\tif max is not None and i &gt;= max:\n\t\t\t\t\t\tbreak\n\t\t\t\t\tif self.extract_tables:\n\t\t\t\t\t\tpage_lines = self._extract_tables( page )\n\t\t\t\t\telse:\n\t\t\t\t\t\t_text = page.get_text( 'text' )\n\t\t\t\t\t\tpage_lines = _text.splitlines( )\n\t\t\t\t\tfiltered = self._filter_lines( page_lines )\n\t\t\t\t\tself.extracted_lines.extend( filtered )\n\t\t\treturn self.extracted_lines\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'PDF'\n\t\texception.method = ('extract_lines( self, path: str, max: Optional[ int ]=None ) -&gt; '\n\t\t                    'List[ str ]')\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.PDF.extract_tables","title":"extract_tables","text":"<pre><code>extract_tables(path, max=None)\n</code></pre>"},{"location":"reference/api/#processing.PDF.extract_tables--purpose","title":"Purpose:","text":"<p>Extract tables from the PDF and return them as a list of DataFrames.</p> <p>Parameters: - path (str): Path to the PDF file - max (Optional[int]): Maximum num of pages to process</p>"},{"location":"reference/api/#processing.PDF.extract_tables--returns","title":"Returns:","text":"<ul> <li>List[pd.DataFrame]: List of DataFrames representing detected tables</li> </ul> Source code in <code>processing.py</code> <pre><code>def extract_tables( self, path: str, max: Optional[ int ]=None ) -&gt; (\n\t\tList[ pd.DataFrame ] | None):\n\t\"\"\"\n\n\t\tPurpose:\n\t\t-----------\n\t\tExtract tables from the PDF and return them as a list of DataFrames.\n\n\t\tParameters:\n\t\t- path (str): Path to the PDF file\n\t\t- max (Optional[int]): Maximum num of pages to process\n\n\t\tReturns:\n\t\t--------\n\t\t- List[pd.DataFrame]: List of DataFrames representing detected tables\n\n\t\"\"\"\n\ttry:\n\t\tif path is None:\n\t\t\traise Exception( 'The argument \"path\" must be specified' )\n\t\telif max is None:\n\t\t\traise Exception( 'The argument \"max\" must be specified' )\n\t\telse:\n\t\t\tself.file_path = path\n\t\t\tself.tables = [ ]\n\t\t\twith fitz.open( self.file_path ) as doc:\n\t\t\t\tfor i, page in enumerate( doc ):\n\t\t\t\t\tif max is not None and i &gt;= max:\n\t\t\t\t\t\tbreak\n\t\t\t\t\ttf = page.find_tables( )\n\t\t\t\t\tfor t in getattr( tf, 'tables', [ ] ):\n\t\t\t\t\t\tself.tables.append( pd.DataFrame( t.extract( ) ) )\n\t\t\treturn self.tables\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'PDF'\n\t\texception.method = (\n\t\t\t\t'extract_tables( self, path: str, max: Optional[ int ] = None ) -&gt; List[ '\n\t\t\t\t'pd.DataFrame ]')\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.PDF.extract_text","title":"extract_text","text":"<pre><code>extract_text(path, max=None)\n</code></pre>"},{"location":"reference/api/#processing.PDF.extract_text--purpose","title":"Purpose:","text":"<p>Extract the entire pages from a PDF into one continuous path.</p>"},{"location":"reference/api/#processing.PDF.extract_text--parameters","title":"Parameters:","text":"<ul> <li>path (str): Path to the PDF file</li> <li>max (Optional[int]): Maximum num of pages to process</li> </ul>"},{"location":"reference/api/#processing.PDF.extract_text--returns","title":"Returns:","text":"<ul> <li>str: Full concatenated pages</li> </ul> Source code in <code>processing.py</code> <pre><code>def extract_text( self, path: str, max: Optional[ int ]=None ) -&gt; str | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t---------\n\t\tExtract the entire pages from a PDF into one continuous path.\n\n\t\tParameters:\n\t\t-----------\n\t\t- path (str): Path to the PDF file\n\t\t- max (Optional[int]): Maximum num of pages to process\n\n\t\tReturns:\n\t\t--------\n\t\t- str: Full concatenated pages\n\n\t\"\"\"\n\ttry:\n\t\tif path is None:\n\t\t\traise Exception( 'The argument \"path\" must be specified' )\n\t\telse:\n\t\t\tif max is not None and max &gt; 0:\n\t\t\t\tself.file_path = path\n\t\t\t\tself.lines = self.extract_lines( self.file_path, max=max )\n\t\t\t\treturn '\\n'.join( self.lines )\n\t\t\telif max is None or max &lt;= 0:\n\t\t\t\tself.file_path = path\n\t\t\t\tself.lines = self.extract_lines( self.file_path )\n\t\t\t\treturn '\\n'.join( self.lines )\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'PDF'\n\t\texception.method = 'extract_text( self, path: str, max: Optional[ int ]=None ) -&gt; str:'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Processor","title":"Processor","text":"<p>Purpose: Base class for processing classes</p> Source code in <code>processing.py</code> <pre><code>class Processor( ):\n\t'''\n\n\t\tPurpose:\n\t\tBase class for processing classes\n\n\t'''\n\tlemmatizer: WordNetLemmatizer\n\tstemmer: PorterStemmer\n\tfile_path: Optional[ str ]\n\tlowercase: Optional[ str ]\n\tnormalized: Optional[ str ]\n\tlemmatized: Optional[ str ]\n\ttokenized: Optional[ str ]\n\tencoding: Optional[ Encoding ]\n\tchunk_size: Optional[ int ]\n\tcorrected: Optional[ str ]\n\traw_input: Optional[ str ]\n\traw_html: Optional[ str ]\n\traw_pages: Optional[ List[ str ] ]\n\twords: Optional[ List[ str ] ]\n\ttokens: Optional[ List[ str ] ]\n\tlines: Optional[ List[ str ] ]\n\tfiles: Optional[ List[ str ] ]\n\tpages: Optional[ List[ str ] ]\n\tparagraphs: Optional[ List[ str ] ]\n\tids: Optional[ List[ int ] ]\n\tcleaned_lines: Optional[ List[ str ] ]\n\tcleaned_tokens: Optional[ List[ str ] ]\n\tcleaned_pages: Optional[ List[ str ] ]\n\tcleaned_html: Optional[ str ]\n\tstop_words: Optional[ set ]\n\tvocabulary: Optional[ List[ str ] ]\n\tremoved: Optional[ List[ str ] ]\n\tfrequency_distribution: Optional[ Dict ]\n\tconditional_distribution: Optional[ Dict ]\n\n\tdef __init__( self ):\n\t\tself.lemmatizer = WordNetLemmatizer( )\n\t\tself.stemmer = PorterStemmer( )\n\t\tself.files = [ ]\n\t\tself.words = [ ]\n\t\tself.tokens = [ ]\n\t\tself.lines = [ ]\n\t\tself.pages = [ ]\n\t\tself.ids = [ ]\n\t\tself.paragraphs = [ ]\n\t\tself.chunks = [ ]\n\t\tself.chunk_size = 0\n\t\tself.cleaned_lines = [ ]\n\t\tself.cleaned_tokens = [ ]\n\t\tself.cleaned_pages = [ ]\n\t\tself.removed = [ ]\n\t\tself.raw_pages = [ ]\n\t\tself.stop_words = set( )\n\t\tself.vocabulary = [ ]\n\t\tself.frequency_distribution = { }\n\t\tself.conditional_distribution = { }\n\t\tself.encoding = None\n\t\tself.file_path = ''\n\t\tself.raw_input = ''\n\t\tself.normalized = ''\n\t\tself.lemmatized = ''\n\t\tself.tokenized = ''\n\t\tself.cleaned_text = ''\n\t\tself.cleaned_html = None\n\t\tself.corrected = None\n\t\tself.lowercase = None\n\t\tself.raw_html = None\n</code></pre>"},{"location":"reference/api/#processing.Text","title":"Text","text":"<p>               Bases: <code>Processor</code></p> <pre><code>Purpose:\n---------\nClass providing path preprocessing functionality\n</code></pre>"},{"location":"reference/api/#processing.Text--methods","title":"Methods:","text":"<p>split_lines( self, path: str ) -&gt; list split_pages( self, path: str, delimit: str ) -&gt; list collapse_whitespace( self, path: str ) -&gt; str remove_punctuation( self, path: str ) -&gt; str:     remove_special( self, path: str, keep_spaces: bool ) -&gt; str:     remove_html( self, path: str ) -&gt; str     remove_errors( self, path: str ) -&gt; str     remove_markdown( self, path: str ) -&gt; str     remove_stopwords( self, path: str ) -&gt; str     remove_headers( self, pages, min: int=3 ) -&gt; str normalize_text( path: str ) -&gt; str lemmatize_tokens( words: List[ str ] ) -&gt; str tokenize_text( path: str ) -&gt; str tokenize_words( path: str ) -&gt; List[ str ] tokenize_sentences( path: str ) -&gt; str chunk_text( self, path: str, max: int=800 ) -&gt; List[ str ] chunk_words( self, path: str, max: int=800, over: int=50 ) -&gt; List[ str ] split_paragraphs( self, path: str ) -&gt; List[ str ] compute_frequency_distribution( self, words: List[ str ], proc: bool=True ) -&gt; List[ str ] compute_conditional_distribution( self, words: List[ str ], condition: str=None, proc: bool=True ) -&gt; List[ str ] create_vocabulary( self, frequency, min: int=1 ) -&gt; List[ str ] create_wordbag( words: List[ str ] ) -&gt; dict create_word2vec( sentences: List[ str ], vector_size=100, window=5, min_count=1 ) -&gt; Word2Vec create_tfidf( words: List[ str ], max_features=1000, prep=True ) -&gt; tuple</p> Source code in <code>processing.py</code> <pre><code>class Text( Processor ):\n\t'''\n\n\t\tPurpose:\n\t\t---------\n\t\tClass providing path preprocessing functionality\n\n\t    Methods:\n\t    --------\n\t    split_lines( self, path: str ) -&gt; list\n\t    split_pages( self, path: str, delimit: str ) -&gt; list\n\t    collapse_whitespace( self, path: str ) -&gt; str\n\t    remove_punctuation( self, path: str ) -&gt; str:\n\t\tremove_special( self, path: str, keep_spaces: bool ) -&gt; str:\n\t\tremove_html( self, path: str ) -&gt; str\n\t\tremove_errors( self, path: str ) -&gt; str\n\t\tremove_markdown( self, path: str ) -&gt; str\n\t\tremove_stopwords( self, path: str ) -&gt; str\n\t\tremove_headers( self, pages, min: int=3 ) -&gt; str\n\t    normalize_text( path: str ) -&gt; str\n\t    lemmatize_tokens( words: List[ str ] ) -&gt; str\n\t    tokenize_text( path: str ) -&gt; str\n\t    tokenize_words( path: str ) -&gt; List[ str ]\n\t    tokenize_sentences( path: str ) -&gt; str\n\t    chunk_text( self, path: str, max: int=800 ) -&gt; List[ str ]\n\t    chunk_words( self, path: str, max: int=800, over: int=50 ) -&gt; List[ str ]\n\t    split_paragraphs( self, path: str ) -&gt; List[ str ]\n\t    compute_frequency_distribution( self, words: List[ str ], proc: bool=True ) -&gt; List[ str ]\n\t    compute_conditional_distribution( self, words: List[ str ], condition: str=None,\n\t    proc: bool=True ) -&gt; List[ str ]\n\t    create_vocabulary( self, frequency, min: int=1 ) -&gt; List[ str ]\n\t    create_wordbag( words: List[ str ] ) -&gt; dict\n\t    create_word2vec( sentences: List[ str ], vector_size=100, window=5, min_count=1 ) -&gt;\n\t    Word2Vec\n\t    create_tfidf( words: List[ str ], max_features=1000, prep=True ) -&gt; tuple\n\n\t'''\n\n\tdef __init__( self ):\n\t\t'''\n\n\t\t\tPurpose:\n\t\t\t---------\n\t\t\tConstructor for 'Text' objects\n\n\t\t'''\n\t\tsuper( ).__init__( )\n\t\tself.lemmatizer = WordNetLemmatizer( )\n\t\tself.stemmer = PorterStemmer( )\n\t\tself.words = [ ]\n\t\tself.tokens = [ ]\n\t\tself.lines = [ ]\n\t\tself.pages = [ ]\n\t\tself.ids = [ ]\n\t\tself.paragraphs = [ ]\n\t\tself.chunks = [ ]\n\t\tself.chunk_size = 0\n\t\tself.cleaned_lines = [ ]\n\t\tself.cleaned_tokens = [ ]\n\t\tself.cleaned_pages = [ ]\n\t\tself.removed = [ ]\n\t\tself.raw_pages = [ ]\n\t\tself.stop_words = set( )\n\t\tself.vocabulary = [ ]\n\t\tself.frequency_distribution = { }\n\t\tself.conditional_distribution = { }\n\t\tself.encoding = None\n\t\tself.file_path = ''\n\t\tself.raw_input = ''\n\t\tself.normalized = ''\n\t\tself.lemmatized = ''\n\t\tself.tokenized = ''\n\t\tself.cleaned_text = ''\n\t\tself.cleaned_html = None\n\t\tself.corrected = None\n\t\tself.lowercase = None\n\t\tself.raw_html = None\n\t\tself.translator = None\n\t\tself.tokenizer = None\n\t\tself.vectorizer = None\n\n\tdef __dir__( self ) -&gt; List[ str ] | None:\n\t\t'''\n\n\t\t\tPurpose:\n\t\t\t---------\n\t\t\tProvides a list of strings representing class members.\n\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- self\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- List[ str ] | None\n\n\t\t'''\n\t\treturn [ 'file_path', 'raw_input', 'raw_pages', 'normalized', 'lemmatized',\n\t\t         'tokenized', 'corrected', 'cleaned_text', 'words', 'paragraphs',\n\t\t         'words', 'pages', 'chunks', 'chunk_size', 'cleaned_pages',\n\t\t         'stop_words', 'cleaned_lines', 'removed', 'lowercase', 'encoding', 'vocabulary',\n\t\t         'translator', 'lemmatizer', 'stemmer', 'tokenizer', 'vectorizer',\n\t\t         'split_lines', 'split_pages', 'collapse_whitespace',\n\t\t         'remove_punctuation', 'remove_special', 'remove_html',\n\t\t         'remove_markdown', 'remove_stopwords', 'remove_headers', 'tiktokenize',\n\t\t         'normalize_text', 'tokenize_text', 'tokenize_words',\n\t\t         'tokenize_sentences', 'chunk_text', 'chunk_words',\n\t\t         'create_wordbag', 'create_word2vec', 'create_tfidf',\n\t\t         'clean_files', 'convert_jsonl', 'conditional_distribution' ]\n\n\tdef collapse_whitespace( self, text: str ) -&gt; str | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t-----------\n\t\t\tRemoves extra spaces and blank words from the string 'text'.\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- text : str\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\tA cleaned_lines path path with:\n\t\t\t\t- Consecutive whitespace reduced to a single space\n\t\t\t\t- Leading/trailing spaces removed\n\t\t\t\t- Blank words removed\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif text is None:\n\t\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\t\telse:\n\t\t\t\tself.raw_input = text\n\t\t\t\tself.cleaned_text = re.sub( r'[ \\t]+', ' ', self.raw_input )\n\t\t\t\tself.cleaned_lines = [ line.strip( ) for line in self.cleaned_text.splitlines( ) ]\n\t\t\t\tself.lines = [ line for line in self.cleaned_lines if line ]\n\t\t\t\treturn ' '.join( self.lines )\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = 'collapse_whitespace( self, path: str ) -&gt; str:'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef remove_punctuation( self, text: str ) -&gt; str | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t-----------\n\t\t\tRemoves all punctuation characters from the path path path.\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- pages : str\n\t\t\t\tThe path path path to be cleaned_lines.\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- str\n\t\t\t\tThe path path with all punctuation removed.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif text is None:\n\t\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\t\telse:\n\t\t\t\tself.raw_input = text\n\t\t\t\tself.translator = str.maketrans( '', '', string.punctuation )\n\t\t\t\tself.cleaned_text = self.raw_input.translate( self.translator )\n\t\t\t\treturn self.cleaned_text\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = 'remove_punctuation( self, text: str ) -&gt; str:'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef remove_special( self, text: str ) -&gt; str | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t-----------\n\t\t\tRemoves special characters from the path path path.\n\n\t\t\tThis function:\n\t\t\t  - Retains only alphanumeric characters and whitespace\n\t\t\t  - Removes symbols like @, #, $, %, &amp;, etc.\n\t\t\t  - Preserves letters, numbers, and spaces\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- pages : str\n\t\t\t\tThe raw path path path potentially\n\t\t\t\tcontaining special characters.\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- str\n\t\t\t\tA cleaned_lines path containing\n\t\t\t\tonly letters, numbers, and spaces.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif text is None:\n\t\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\t\telse:\n\t\t\t\tcleaned = [ ]\n\t\t\t\tkeepers = [ '(', ')', '$', '. ', ';', ':' ]\n\t\t\t\tfor char in text:\n\t\t\t\t\tif char in keepers:\n\t\t\t\t\t\tcleaned.append( char )\n\t\t\t\t\telif char.isalnum( ) or char.isspace( ):\n\t\t\t\t\t\tcleaned.append( char )\n\t\t\t\treturn ''.join( cleaned )\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = 'remove_special( self, text: str ) -&gt; str:'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef remove_html( self, text: str ) -&gt; str | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t--------\n\t\t\tRemoves HTML tags from the path path path.\n\n\t\t\tThis function:\n\t\t\t  - Parses the path as HTML\n\t\t\t  - Extracts and returns only the visible content without tags\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- pages : str\n\t\t\t\tThe path path containing HTML tags.\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- str\n\t\t\t\tA cleaned_lines path with all HTML tags removed.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif text is None:\n\t\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\t\telse:\n\t\t\t\tself.raw_html = text\n\t\t\t\tself.cleaned_html = BeautifulSoup( self.raw_html, 'html.parser' ).get_text( )\n\t\t\t\treturn self.cleaned_html\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = 'remove_html( self, text: str ) -&gt; str'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef remove_markdown( self, text: str ) -&gt; str | None:\n\t\t\"\"\"\n\n\n\t\t\tPurpose:\n\t\t\t-----------\n\t\t\tRemoves Markdown syntax (e.g., *, #, [], etc.)\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- pages : str\n\t\t\t\tThe formatted path pages.\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- str\n\t\t\t\tA cleaned_lines version of the pages with formatting removed.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif text is None:\n\t\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\t\telse:\n\t\t\t\tself.raw_input = text\n\t\t\t\tself.cleaned_text = re.sub( r'\\[.*?]\\(.*?\\)', '', text )\n\t\t\t\tself.corrected = re.sub( r'[`_*#~&gt;-]', '', self.cleaned_text )\n\t\t\t\t_retval = re.sub( r'!\\[.*?]\\(.*?\\)', '', self.corrected )\n\t\t\t\treturn _retval\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = 'remove_markdown( self, path: str ) -&gt; str'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef remove_stopwords( self, text: str ) -&gt; str | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t-----------\n\t\t\tThis function:\n\t\t\t  - Removes English stopwords from the path pages path.\n\t\t\t  - Tokenizes the path pages\n\t\t\t  - Removes common stopwords (e.g., \"the\", \"is\", \"and\", etc.)\n\t\t\t  - Returns the pages with only meaningful words\n\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- pages : str\n\t\t\t\tThe text string.\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- str\n\t\t\t\tA text string without stopwords.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif text is None:\n\t\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\t\telse:\n\t\t\t\tself.raw_input = text.lower( )\n\t\t\t\tself.stop_words = stopwords.words( 'english' )\n\t\t\t\tself.tokens = nltk.word_tokenize( self.raw_input )\n\t\t\t\tself.cleaned_tokens = [ w for w in self.tokens if\n\t\t\t\t                        w.isalnum( ) and w not in self.stop_words ]\n\t\t\t\tself.cleaned_text = ' '.join( self.cleaned_tokens )\n\t\t\t\treturn self.cleaned_text\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = 'remove_stopwords( self, text: str ) -&gt; str'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef clean_space( self, text: str ) -&gt; str | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t_____________\n\t\t\tRemoves extra spaces and blank words from the path pages.\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- pages : str\n\t\t\t\tThe raw path pages path to be cleaned_lines.\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- str\n\t\t\t\tA cleaned_lines pages path with:\n\t\t\t\t\t- Consecutive whitespace reduced to a single space\n\t\t\t\t\t- Leading/trailing spaces removed\n\t\t\t\t\t- Blank words removed\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif text is None:\n\t\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\t\telse:\n\t\t\t\tself.raw_input = text.lower( )\n\t\t\t\ttabs = re.sub( r'[ \\t]+', ' ', text.lower( ) )\n\t\t\t\tcollapsed = re.sub( r'\\s+', ' ', tabs ).strip( )\n\t\t\t\tself.cleaned_text = collapsed\n\t\t\t\treturn self.cleaned_text\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = 'remove_errors( self, text: str ) -&gt; str'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef remove_headers( self, pages: List[ str ], min: int=3 ) -&gt; List[ str ] | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t--------\n\t\t\tRemoves repetitive headers and footers across a list of pages by frequency analysis.\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- pages (list of str):\n\t\t\t\tA list where each element is the full path of one page.\n\n\t\t\t- min (int):\n\t\t\t\tMinimum num of times a line must appear at the top/bottom to be a header/footer.\n\n\t\t\tReturns:\n\t\t\t---------\n\t\t\t- list of str:\n\t\t\t\tList of cleaned_lines page words without detected headers/footers.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif pages is None:\n\t\t\t\traise Exception( 'The argument \"pages\" is required.' )\n\t\t\telse:\n\t\t\t\t_headers = defaultdict( int )\n\t\t\t\t_footers = defaultdict( int )\n\t\t\t\tself.pages = pages\n\n\t\t\t\t# First pass: collect frequency of top/bottom words\n\t\t\t\tfor _page in self.pages:\n\t\t\t\t\tself.lines = _page.strip( ).splitlines( )\n\t\t\t\t\tif not self.lines:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t_headers[ self.lines[ 0 ].strip( ) ] += 1\n\t\t\t\t\t_footers[ self.lines[ -1 ].strip( ) ] += 1\n\n\t\t\t\t# Identify candidates for removal\n\t\t\t\t_head = { line for line, count in _headers.items( ) if\n\t\t\t\t          count &gt;= min }\n\t\t\t\t_foot = { line for line, count in _footers.items( ) if\n\t\t\t\t          count &gt;= min }\n\n\t\t\t\t# Second pass: clean pages\n\t\t\t\tfor _page in self.pages:\n\t\t\t\t\tif not self.lines:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tself.lines = _page.strip( ).splitlines( )\n\t\t\t\t\tif not self.lines:\n\t\t\t\t\t\tself.cleaned_pages.append( _page )\n\t\t\t\t\t\tcontinue\n\n\t\t\t\t\t# Remove header\n\t\t\t\t\tif self.lines[ 0 ].strip( ) in _head:\n\t\t\t\t\t\tself.lines = self.lines[ 1: ]\n\n\t\t\t\t\t# Remove footer\n\t\t\t\t\tif self.lines and self.lines[ -1 ].strip( ) in _foot:\n\t\t\t\t\t\tself.lines = self.lines[ : -1 ]\n\n\t\t\t\t\tself.cleaned_pages.append( '\\n'.join( self.lines ) )\n\t\t\t\t_retval = self.cleaned_pages\n\t\t\t\treturn _retval\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = ('remove_headers( self, pages: List[ str ], min: int=3 ) -&gt; List['\n\t\t\t                    'str]')\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef normalize_text( self, text: str ) -&gt; str | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t-----------\n\t\t\tNormalizes the path pages path.\n\t\t\t  - Converts pages to lowercase\n\t\t\t  - Removes accented characters (e.g., \u00e9 -&gt; e)\n\t\t\t  - Removes leading/trailing spaces\n\t\t\t  - Collapses multiple whitespace characters into a single space\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- pages : str\n\t\t\t\tThe raw path pages path to be normalized.\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- str\n\t\t\t\tA normalized, cleaned_lines version of the path path.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif text is None:\n\t\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\t\telse:\n\t\t\t\tself.raw_input = text\n\t\t\t\tself.normalized = unicodedata.normalize( 'NFKD', text ).encode( 'ascii',\n\t\t\t\t\t'ignore' ).decode( 'utf-8' )\n\t\t\t\tself.normalized = re.sub( r'\\s+', ' ', self.normalized ).strip( ).lower( )\n\t\t\t\treturn self.normalized\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = 'normalize_text( self, text: str ) -&gt; str'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef tokenize_text( self, text: str ) -&gt; List[ str ] | None:\n\t\t'''\n\n\t\t\tPurpose:\n\t\t\t---------\n\t\t\tSplits the raw path removes non-words and returns words\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- cleaned_line: (str) - clean documents.\n\n\t\t\tReturns:\n\t\t\t- list: Cleaned and normalized documents.\n\n\t\t'''\n\t\ttry:\n\t\t\tif text is None:\n\t\t\t\traise Exception( 'The argument \"text\" was None' )\n\t\t\telse:\n\t\t\t\t_tokens = nltk.word_tokenize( text )\n\t\t\t\tself.words = [ t for t in _tokens ]\n\t\t\t\tself.tokens = [ re.sub( r'[^\\w\"-]', '', w ) for w in self.words if w.strip( ) ]\n\t\t\t\treturn self.tokens\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = 'tokenize_text( self, path: str ) -&gt; List[ str ]'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\t# noinspection PyTypeChecker\n\tdef tiktokenize( self, text: str, encoding: str='cl100k_base' ) -&gt; List[ str ] | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t---------\n\t\t\tTokenizes text text into subword words using OpenAI's tiktoken tokenizer.\n\t\t\tThis function leverages the tiktoken library, which provides byte-pair encoding (BPE)\n\t\t\ttokenization used in models such as GPT-3.5 and GPT-4. Unlike standard word\n\t\t\ttokenization,\n\t\t\tthis function splits text into model-specific subword units.\n\n\t\t\tParameters\n\t\t\t----------\n\t\t\t- text : str\n\t\t\t\tThe text string to be tokenized.\n\n\t\t\t- model : str, optional\n\t\t\t\tThe tokenizer model to use. Examples include 'cl100k_base' (default),\n\t\t\t\t'gpt-3.5-turbo', or 'gpt-4'. Ensure the model is supported by tiktoken.\n\n\t\t\tReturns\n\t\t\t-------\n\t\t\t- List[str]\n\t\t\t\tA list of string words representing BPE subword units.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif text is None:\n\t\t\t\traise Exception( 'The argument \"text\" was None' )\n\t\t\telse:\n\t\t\t\tself.encoding = tiktoken.get_encoding( encoding )\n\t\t\t\ttoken_ids = self.encoding.encode( text )\n\t\t\t\treturn token_ids  # or [self.encoding.decode_single_token_bytes(t) for t in token_ids]\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = ('tiktokenize( self, text: str, encoding: str=\"cl100k_base\" ) -&gt; '\n\t\t\t                    'List[ str ]')\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef tokenize_words( self, words: List[ str ] ) -&gt; List[ List[ str ] ]| None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t-----------\n\t\t\t  - Tokenizes the path pages path into individual word words.\n\t\t\t  - Converts pages to lowercase\n\t\t\t  - Uses NLTK's word_tokenize to split\n\t\t\t  the pages into words and punctuation words\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- words : List[ str ]\n\t\t\t\tA list of strings to be tokenized.\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- A list of token strings (words and punctuation) extracted from the pages.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif words is None:\n\t\t\t\traise Exception( 'The argument \"words\" was None' )\n\t\t\telse:\n\t\t\t\tself.words = words\n\t\t\t\tfor w in self.words:\n\t\t\t\t\t_tokens = nltk.word_tokenize( w )\n\t\t\t\t\tself.tokens.append( _tokens )\n\t\t\t\treturn self.tokens\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = 'tokenize_words( self, path: str ) -&gt; List[ str ]'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef chunk_text( self, text: str, size: int=50, return_as_string: bool=True ) -&gt; (List[\n\t\t                                                                                    str ] |\n\t                                                                                     None):\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t-----------\n\t\t\tTokenizes cleaned_lines pages and breaks it into chunks for downstream vectors.\n\t\t\t  - Converts pages to lowercase\n\t\t\t  - Tokenizes pages using NLTK's word_tokenize\n\t\t\t  - Breaks words into chunks of a specified size\n\t\t\t  - Optionally joins words into strings (for transformer models)\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- pages : str\n\t\t\t\tThe cleaned_lines path pages to be tokenized and chunked.\n\n\t\t\t- size : int, optional (default=50)\n\t\t\t\tNumber of words per chunk_words.\n\n\t\t\t- return_as_string : bool, optional (default=True)\n\t\t\t\tIf True, returns each chunk_words as a path; otherwise, returns a get_list of\n\t\t\t\twords.\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- a list\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif text is None:\n\t\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\t\telse:\n\t\t\t\tself.tokens = nltk.word_tokenize( text )\n\t\t\t\tself.chunks = [ self.tokens[ i: i + size ] for i in\n\t\t\t\t                range( 0, len( self.tokens ), size ) ]\n\t\t\t\tif return_as_string:\n\t\t\t\t\treturn [ ' '.join( chunk ) for chunk in self.chunks ]\n\t\t\t\telse:\n\t\t\t\t\treturn self.chunks\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = 'chunk_text( self, text: str, max: int=800 ) -&gt; list[ str ]'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef chunk_words( self, words: List[ str ], size: int=50, as_string: bool=True ) -&gt; List[ str ] | List[ List[ str ] ] | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t-----------\n\t\t\tBreaks a list of words/tokens into a List[ List[ str ] ] or a string.\n\n\t\t\tThis function:\n\t\t\t- Groups words into chunks of min `size`\n\t\t\t- Returns a a List[ List[ str ] or string\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- words : a list of tokenizd words\n\n\t\t\t- size : int, optional (default=50)\n\t\t\tNumber of words per chunk_words.\n\n\t\t\t- as_string : bool, optional (default=True)\n\t\t\tReturns a string if True, else a List[ List[ str ] ] if False.\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- List[ List[ str ] ]\n\t\t\tA list of a list of token chunks. Each chunk is a list of words.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif words is None:\n\t\t\t\traise Exception( 'The argument \"words\" is required.' )\n\t\t\telse:\n\t\t\t\tself.tokens = [ token for sublist in words for token in sublist ]\n\t\t\t\tself.chunks = [ self.tokens[ i: i + size ]\n\t\t\t\t                for i in range( 0, len( self.tokens ), size ) ]\n\t\t\t\tif as_string:\n\t\t\t\t\treturn [ ' '.join( chunk ) for chunk in self.chunks ]\n\t\t\t\telse:\n\t\t\t\t\treturn self.chunks\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Token'\n\t\t\texception.method = (\n\t\t\t\t\t'chunk_words( self, words: list[ str ], max: int=800, over: int=50 ) -&gt; list[ '\n\t\t\t\t\t'str ]')\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef split_sentences( self, text: str ) -&gt; List[ str ] | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t________\n\t\t\tSplits the text string into a list of\n\t\t\tstrings using NLTK's Punkt sentence tokenizer.\n\t\t\tThis function is useful for preparing text for further processing,\n\t\t\tsuch as tokenization, parsing, or named entity recognition.\n\n\t\t\tParameters\n\t\t\t----------\n\t\t\t- text : str\n\t\t\tThe raw text string to be segmented into sentences.\n\n\t\t\tReturns\n\t\t\t-------\n\t\t\t- List[ str ]\n\t\t\tA list of sentence strings, each corresponding to a single sentence detected\n\t\t\tin the text text.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif text is None:\n\t\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\t\telse:\n\t\t\t\tself.lines = nltk.sent_tokenize( text )\n\t\t\t\treturn self.lines\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = 'split_sentences( self, text: str ) -&gt; List[ str ]'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef split_pages( self, path: str, delimit: str = '\\f' ) -&gt; List[ str ] | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t--------\n\t\t\tReads path from a file, splits it into words,\n\t\t\tand groups them into path.\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- path (str): Path to the path file.\n\t\t\t- delimiter (str): Page separator path (default is '\\f' for form feed).\n\n\t\t\tReturns:\n\t\t\t---------\n\t\t\t- list[ str ]  where each element is the path.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif path is None:\n\t\t\t\traise Exception( 'The argument \"path\" is required' )\n\t\t\telse:\n\t\t\t\tself.file_path = path\n\t\t\t\twith open( self.file_path, 'r', encoding = 'utf-8', errors = 'ignore' ) as _file:\n\t\t\t\t\t_content = _file.read( )\n\t\t\t\t\tself.raw_pages = _content.split( delimit )\n\n\t\t\t\tfor _page in self.raw_pages:\n\t\t\t\t\tself.lines = _page.strip( ).splitlines( )\n\t\t\t\t\tself.cleaned_text = '\\n'.join(\n\t\t\t\t\t\t[ l.strip( ) for l in self.lines if l.strip( ) ] )\n\t\t\t\t\tself.cleaned_pages.append( self.cleaned_text )\n\t\t\t\treturn self.cleaned_pages\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = 'split_pages( self, path: str, delimit: str=\"\\f\" ) -&gt; List[ str ]'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef split_paragraphs( self, path: str ) -&gt; List[ str ] | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t---------\n\t\t\tReads  a file and splits it into paragraphs. A paragraph is defined as a block\n\t\t\tof path separated by one or more empty lines (eg, '\\n\\n').\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- path (str): Path to the path file.\n\n\t\t\tReturns:\n\t\t\t---------\n\t\t\t- list of str: List of paragraph strings.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif path is None:\n\t\t\t\traise Exception( 'The argument \"path\" is required.' )\n\t\t\telse:\n\t\t\t\tself.file_path = path\n\t\t\t\twith open( self.file_path, 'r', encoding='utf-8', errors='ignore' ) as _file:\n\t\t\t\t\tself.raw_input = _file.read( )\n\t\t\t\t\tself.paragraphs = [ pg.strip( ) for pg in self.raw_input.split( '\\n\\n' ) if\n\t\t\t\t\t                    pg.strip( ) ]\n\n\t\t\t\t\treturn self.paragraphs\n\t\texcept UnicodeDecodeError:\n\t\t\twith open( self.file_path, 'r', encoding = 'latin1', errors = 'ignore' ) as _file:\n\t\t\t\tself.raw_input = _file.read( )\n\t\t\t\tself.paragraphs = [ pg.strip( ) for pg in self.raw_input.split( '\\n\\n' ) if\n\t\t\t\t                    pg.strip( ) ]\n\t\t\t\treturn self.paragraphs\n\n\tdef compute_frequency_distribution( self, lines: List[ str ] ) -&gt; FreqDist | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t--------\n\t\t\tCreates a word frequency freq_dist from a list of documents.\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- lines (list): List of raw or preprocessed path documents.\n\t\t\t- process (bool): Applies normalization, tokenization, stopword removal,\n\t\t\tand lemmatization.\n\n\t\t\tReturns:\n\t\t\t- dict: Dictionary of words and their corresponding frequencies.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif lines is None:\n\t\t\t\traise Exception( 'The argument \"words\" is required.' )\n\t\t\telse:\n\t\t\t\tself.lines = lines\n\t\t\t\tall_tokens: list[ str ] = [ ]\n\t\t\t\tfor _line in lines:\n\t\t\t\t\ttoks = self.tokenize_text( _line )\n\t\t\t\t\tall_tokens.extend( toks )\n\t\t\t\tself.frequency_distribution = dict( Counter( all_tokens ) )\n\t\t\t\treturn self.frequency_distribution\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = ('compute_frequency_distribution( self, documents: list, process: '\n\t\t\t                    'bool=True) -&gt; FreqDist')\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef compute_conditional_distribution( self, lines: List[ str ], condition=None,\n\t                                      process: bool=True ) -&gt; ConditionalFreqDist | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t--------\n\t\t\tComputes a Conditional Frequency Distribution (CFD)\n\t\t\t over a collection of documents.\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- documents (list):\n\t\t\t\tA list of path sections (pages, paragraphs, etc.).\n\n\t\t\t- condition (function):\n\t\t\t\tA function to determine the condition/grouping. If None, uses document index.\n\n\t\t\t- process (bool):\n\t\t\t\tIf True, applies normalization, tokenization,\n\t\t\t\tstopword removal, and lemmatization.\n\n\t\t\tReturns:\n\t\t\t- ConditionalFreqDist:\n\t\t\t\tAn NLTK ConditionalFreqDist object mapping conditions to word frequencies.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif lines is None:\n\t\t\t\traise Exception( 'The argument \"words\" is required.' )\n\t\t\telse:\n\t\t\t\tself.lines = lines\n\t\t\t\tcfd = ConditionalFreqDist( )\n\t\t\t\tfor idx, line in enumerate( lines ):\n\t\t\t\t\tkey = condition( line ) if condition else f'Doc_{idx}'\n\t\t\t\t\ttoks = self.tokenize_text( self.normalize_text( line ) if process else line )\n\t\t\t\t\tfor t in toks:\n\t\t\t\t\t\tcfd[ key ][ t ] += 1\n\t\t\t\tself.conditional_distribution = cfd\n\t\t\t\treturn cfd\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = ('compute_conditional_distribution( self, words: List[ str ], '\n\t\t\t                    'condition=None, process: bool=True ) -&gt; ConditionalFreqDist')\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef create_vocabulary( self, freq_dist: Dict, min: int=1 ) -&gt; List[ str ] | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t---------\n\t\t\tBuilds a vocabulary list from a frequency\n\t\t\tdistribution by applying a minimum frequency threshold.\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- freq_dist (dict):\n\t\t\t\tA dictionary mapping words to their frequencies.\n\n\t\t\t- min (int): Minimum num\n\t\t\t\tof occurrences required for a word to be included.\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- list: Sorted list of unique vocabulary words.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif freq_dist is None:\n\t\t\t\traise Exception( 'The argument \"freq_dist\" is required.' )\n\t\t\telse:\n\t\t\t\tself.frequency_distribution = freq_dist\n\t\t\t\tself.words = [ word for word, freq in freq_dist.items( ) if freq &gt;= min ]\n\t\t\t\tself.vocabulary = sorted( self.words )\n\t\t\t\treturn self.vocabulary\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = ('create_vocabulary( self, freq_dist: dict, min: int=1 ) -&gt; List['\n\t\t\t                    'str]')\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef create_wordbag( self, words: List[ str ] ) -&gt; dict | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t--------\n\t\t\tConstruct a Bag-of-Words (BoW) frequency dictionary from a list of strings.\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- words (list): List of words from a document.\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- dict: Word frequency dictionary.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif words is None:\n\t\t\t\traise Exception( 'The argument \"words\" is required.' )\n\t\t\telse:\n\t\t\t\tself.words = words\n\t\t\t\treturn dict( Counter( self.words ) )\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = 'create_wordbag( self, words: List[ str ] ) -&gt; dict'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef create_word2vec( self, words: List[ List[ str ] ], size=100, window=5, min=1 ) -&gt; Word2Vec | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t--------\n\t\t\tTrain a Word2Vec embedding small_model from tokenized sentences.\n\n\t\t\tParameters:\n\t\t\t--------\n\t\t\t- sentences (get_list of get_list of str): List of tokenized sentences.\n\t\t\t- vector_size (int): Dimensionality of word vec.\n\t\t\t- window (int): Max distance between current and predicted word.\n\t\t\t- min_count (int): Minimum frequency for inclusion in vocabulary.\n\n\t\t\tReturns:\n\t\t\t-------\n\t\t\t- Word2Vec: Trained Gensim Word2Vec small_model.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif words is None:\n\t\t\t\traise Exception( 'The argument \"words\" is required.' )\n\t\t\telse:\n\t\t\t\tself.words = words\n\t\t\t\treturn Word2Vec( sentences=self.words, vector_size=size,\n\t\t\t\t\twindow=window, min_count=min )\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = ('create_word2vec( self, words: List[ str ], '\n\t\t\t                    'size=100, window=5, min=1 ) -&gt; Word2Vec')\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef clean_files( self, src: str, dest: str ) -&gt; None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t________\n\t\t\tCleans text files given a source directory (src) and destination directory (dest)\n\n\t\t\tParameters:\n\t\t\t----------\n\t\t\t- src (str): Source directory\n\t\t\t- dest (str): Destination directory\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- None\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif src is None:\n\t\t\t\traise Exception( 'The argument \"src\" is required.' )\n\t\t\telif dest is None:\n\t\t\t\traise Exception( 'The argument \"dest\" is required.' )\n\t\t\telse:\n\t\t\t\tsource = src\n\t\t\t\tdestination = dest\n\t\t\t\tfiles = os.listdir( source )\n\t\t\t\tfor f in files:\n\t\t\t\t\tprocessed = [ ]\n\t\t\t\t\tfilename = os.path.basename( f )\n\t\t\t\t\tsource_path = source + '\\\\' + filename\n\t\t\t\t\ttext = open( source_path, 'r', encoding='utf-8', errors='ignore' ).read( )\n\t\t\t\t\tpunc = self.remove_special( text )\n\t\t\t\t\tdirty = self.split_sentences( punc )\n\t\t\t\t\tfor d in dirty:\n\t\t\t\t\t\tif d != \" \":\n\t\t\t\t\t\t\tlower = d.lower( )\n\t\t\t\t\t\t\tnormal = self.normalize_text( lower )\n\t\t\t\t\t\t\tslim = self.collapse_whitespace( normal )\n\t\t\t\t\t\t\tprocessed.append( slim )\n\n\t\t\t\t\tdest_path = destination + '\\\\' + filename\n\t\t\t\t\tclean = open( dest_path, 'wt', encoding='utf-8', errors='ignore' )\n\t\t\t\t\tfor p in processed:\n\t\t\t\t\t\tclean.write( p )\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = 'clean_files( self, src: str, dest: str )'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef convert_jsonl( self, source: str, desination: str ) -&gt; None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t________\n\t\t\tCoverts text files to JSONL format given a source directory (Source)\n\t\t\t and destination directory (destination)\n\n\t\t\tParameters:\n\t\t\t--------\n\t\t\t- source (str): Source directory\n\t\t\t- destination (str): Destination directory\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- None\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif source is None:\n\t\t\t\traise Exception( 'The argument \"source\" is required.' )\n\t\t\telif desination is None:\n\t\t\t\traise Exception( 'The argument \"desination\" is required.' )\n\t\t\telse:\n\t\t\t\t_source = source\n\t\t\t\t_destination = desination\n\t\t\t\tself.files = os.listdir( _source )\n\t\t\t\t_processed = [ ]\n\t\t\t\tfor _f in self.files:\n\t\t\t\t\t_count = 0\n\t\t\t\t\t_basename = os.path.basename( _f )\n\t\t\t\t\t_sourcepath = _source + f'\\\\{_basename}'\n\t\t\t\t\t_text = open( _sourcepath, 'r', encoding='utf-8', errors='ignore' ).read( )\n\t\t\t\t\t_stops = self.remove_stopwords( _text )\n\t\t\t\t\t_tokens = self.tokenize_text( _stops )\n\t\t\t\t\t_chunks = self.chunk_text( _text )\n\t\t\t\t\t_filename = _basename.rstrip( '.txt' )\n\t\t\t\t\t_destinationpath = _destination + f'\\\\{_filename}.jsonl'\n\t\t\t\t\t_clean = open( _destinationpath, 'wt', encoding='utf-8', errors='ignore' )\n\t\t\t\t\tfor _i in range( len( _chunks ) ):\n\t\t\t\t\t\t_list = _chunks[ _i ]\n\t\t\t\t\t\t_part = ''.join( _list )\n\t\t\t\t\t\t_row = '{' + f'\\\"Line-{_i}\\\":\\\"{_part}\\\"' + '}' + '\\r'\n\t\t\t\t\t\t_processed.append( _row )\n\n\t\t\t\t\tfor _t in _processed:\n\t\t\t\t\t\t_clean.write( _t )\n\n\t\t\t\t\t_clean.flush( )\n\t\t\t\t\t_clean.close( )\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Text'\n\t\t\texception.method = 'convert_jsonl( self, source: str, desination: str )'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.__dir__","title":"__dir__","text":"<pre><code>__dir__()\n</code></pre>"},{"location":"reference/api/#processing.Text.__dir__--purpose","title":"Purpose:","text":"<p>Provides a list of strings representing class members.</p>"},{"location":"reference/api/#processing.Text.__dir__--parameters","title":"Parameters:","text":"<ul> <li>self</li> </ul>"},{"location":"reference/api/#processing.Text.__dir__--returns","title":"Returns:","text":"<ul> <li>List[ str ] | None</li> </ul> Source code in <code>processing.py</code> <pre><code>def __dir__( self ) -&gt; List[ str ] | None:\n\t'''\n\n\t\tPurpose:\n\t\t---------\n\t\tProvides a list of strings representing class members.\n\n\n\t\tParameters:\n\t\t-----------\n\t\t- self\n\n\t\tReturns:\n\t\t--------\n\t\t- List[ str ] | None\n\n\t'''\n\treturn [ 'file_path', 'raw_input', 'raw_pages', 'normalized', 'lemmatized',\n\t         'tokenized', 'corrected', 'cleaned_text', 'words', 'paragraphs',\n\t         'words', 'pages', 'chunks', 'chunk_size', 'cleaned_pages',\n\t         'stop_words', 'cleaned_lines', 'removed', 'lowercase', 'encoding', 'vocabulary',\n\t         'translator', 'lemmatizer', 'stemmer', 'tokenizer', 'vectorizer',\n\t         'split_lines', 'split_pages', 'collapse_whitespace',\n\t         'remove_punctuation', 'remove_special', 'remove_html',\n\t         'remove_markdown', 'remove_stopwords', 'remove_headers', 'tiktokenize',\n\t         'normalize_text', 'tokenize_text', 'tokenize_words',\n\t         'tokenize_sentences', 'chunk_text', 'chunk_words',\n\t         'create_wordbag', 'create_word2vec', 'create_tfidf',\n\t         'clean_files', 'convert_jsonl', 'conditional_distribution' ]\n</code></pre>"},{"location":"reference/api/#processing.Text.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre>"},{"location":"reference/api/#processing.Text.__init__--purpose","title":"Purpose:","text":"<p>Constructor for 'Text' objects</p> Source code in <code>processing.py</code> <pre><code>def __init__( self ):\n\t'''\n\n\t\tPurpose:\n\t\t---------\n\t\tConstructor for 'Text' objects\n\n\t'''\n\tsuper( ).__init__( )\n\tself.lemmatizer = WordNetLemmatizer( )\n\tself.stemmer = PorterStemmer( )\n\tself.words = [ ]\n\tself.tokens = [ ]\n\tself.lines = [ ]\n\tself.pages = [ ]\n\tself.ids = [ ]\n\tself.paragraphs = [ ]\n\tself.chunks = [ ]\n\tself.chunk_size = 0\n\tself.cleaned_lines = [ ]\n\tself.cleaned_tokens = [ ]\n\tself.cleaned_pages = [ ]\n\tself.removed = [ ]\n\tself.raw_pages = [ ]\n\tself.stop_words = set( )\n\tself.vocabulary = [ ]\n\tself.frequency_distribution = { }\n\tself.conditional_distribution = { }\n\tself.encoding = None\n\tself.file_path = ''\n\tself.raw_input = ''\n\tself.normalized = ''\n\tself.lemmatized = ''\n\tself.tokenized = ''\n\tself.cleaned_text = ''\n\tself.cleaned_html = None\n\tself.corrected = None\n\tself.lowercase = None\n\tself.raw_html = None\n\tself.translator = None\n\tself.tokenizer = None\n\tself.vectorizer = None\n</code></pre>"},{"location":"reference/api/#processing.Text.chunk_text","title":"chunk_text","text":"<pre><code>chunk_text(text, size=50, return_as_string=True)\n</code></pre>"},{"location":"reference/api/#processing.Text.chunk_text--purpose","title":"Purpose:","text":"<p>Tokenizes cleaned_lines pages and breaks it into chunks for downstream vectors.   - Converts pages to lowercase   - Tokenizes pages using NLTK's word_tokenize   - Breaks words into chunks of a specified size   - Optionally joins words into strings (for transformer models)</p>"},{"location":"reference/api/#processing.Text.chunk_text--parameters","title":"Parameters:","text":"<ul> <li> <p>pages : str         The cleaned_lines path pages to be tokenized and chunked.</p> </li> <li> <p>size : int, optional (default=50)         Number of words per chunk_words.</p> </li> <li> <p>return_as_string : bool, optional (default=True)         If True, returns each chunk_words as a path; otherwise, returns a get_list of         words.</p> </li> </ul>"},{"location":"reference/api/#processing.Text.chunk_text--returns","title":"Returns:","text":"<ul> <li>a list</li> </ul> Source code in <code>processing.py</code> <pre><code>def chunk_text( self, text: str, size: int=50, return_as_string: bool=True ) -&gt; (List[\n\t                                                                                    str ] |\n                                                                                     None):\n\t\"\"\"\n\n\t\tPurpose:\n\t\t-----------\n\t\tTokenizes cleaned_lines pages and breaks it into chunks for downstream vectors.\n\t\t  - Converts pages to lowercase\n\t\t  - Tokenizes pages using NLTK's word_tokenize\n\t\t  - Breaks words into chunks of a specified size\n\t\t  - Optionally joins words into strings (for transformer models)\n\n\t\tParameters:\n\t\t-----------\n\t\t- pages : str\n\t\t\tThe cleaned_lines path pages to be tokenized and chunked.\n\n\t\t- size : int, optional (default=50)\n\t\t\tNumber of words per chunk_words.\n\n\t\t- return_as_string : bool, optional (default=True)\n\t\t\tIf True, returns each chunk_words as a path; otherwise, returns a get_list of\n\t\t\twords.\n\n\t\tReturns:\n\t\t--------\n\t\t- a list\n\n\t\"\"\"\n\ttry:\n\t\tif text is None:\n\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\telse:\n\t\t\tself.tokens = nltk.word_tokenize( text )\n\t\t\tself.chunks = [ self.tokens[ i: i + size ] for i in\n\t\t\t                range( 0, len( self.tokens ), size ) ]\n\t\t\tif return_as_string:\n\t\t\t\treturn [ ' '.join( chunk ) for chunk in self.chunks ]\n\t\t\telse:\n\t\t\t\treturn self.chunks\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = 'chunk_text( self, text: str, max: int=800 ) -&gt; list[ str ]'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.chunk_words","title":"chunk_words","text":"<pre><code>chunk_words(words, size=50, as_string=True)\n</code></pre>"},{"location":"reference/api/#processing.Text.chunk_words--purpose","title":"Purpose:","text":"<p>Breaks a list of words/tokens into a List[ List[ str ] ] or a string.</p> <p>This function: - Groups words into chunks of min <code>size</code> - Returns a a List[ List[ str ] or string</p>"},{"location":"reference/api/#processing.Text.chunk_words--parameters","title":"Parameters:","text":"<ul> <li> <p>words : a list of tokenizd words</p> </li> <li> <p>size : int, optional (default=50) Number of words per chunk_words.</p> </li> <li> <p>as_string : bool, optional (default=True) Returns a string if True, else a List[ List[ str ] ] if False.</p> </li> </ul>"},{"location":"reference/api/#processing.Text.chunk_words--returns","title":"Returns:","text":"<ul> <li>List[ List[ str ] ] A list of a list of token chunks. Each chunk is a list of words.</li> </ul> Source code in <code>processing.py</code> <pre><code>def chunk_words( self, words: List[ str ], size: int=50, as_string: bool=True ) -&gt; List[ str ] | List[ List[ str ] ] | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t-----------\n\t\tBreaks a list of words/tokens into a List[ List[ str ] ] or a string.\n\n\t\tThis function:\n\t\t- Groups words into chunks of min `size`\n\t\t- Returns a a List[ List[ str ] or string\n\n\t\tParameters:\n\t\t-----------\n\t\t- words : a list of tokenizd words\n\n\t\t- size : int, optional (default=50)\n\t\tNumber of words per chunk_words.\n\n\t\t- as_string : bool, optional (default=True)\n\t\tReturns a string if True, else a List[ List[ str ] ] if False.\n\n\t\tReturns:\n\t\t--------\n\t\t- List[ List[ str ] ]\n\t\tA list of a list of token chunks. Each chunk is a list of words.\n\n\t\"\"\"\n\ttry:\n\t\tif words is None:\n\t\t\traise Exception( 'The argument \"words\" is required.' )\n\t\telse:\n\t\t\tself.tokens = [ token for sublist in words for token in sublist ]\n\t\t\tself.chunks = [ self.tokens[ i: i + size ]\n\t\t\t                for i in range( 0, len( self.tokens ), size ) ]\n\t\t\tif as_string:\n\t\t\t\treturn [ ' '.join( chunk ) for chunk in self.chunks ]\n\t\t\telse:\n\t\t\t\treturn self.chunks\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Token'\n\t\texception.method = (\n\t\t\t\t'chunk_words( self, words: list[ str ], max: int=800, over: int=50 ) -&gt; list[ '\n\t\t\t\t'str ]')\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.clean_files","title":"clean_files","text":"<pre><code>clean_files(src, dest)\n</code></pre> <p>Purpose:</p> <p>Cleans text files given a source directory (src) and destination directory (dest)</p>"},{"location":"reference/api/#processing.Text.clean_files--parameters","title":"Parameters:","text":"<ul> <li>src (str): Source directory</li> <li>dest (str): Destination directory</li> </ul>"},{"location":"reference/api/#processing.Text.clean_files--returns","title":"Returns:","text":"<ul> <li>None</li> </ul> Source code in <code>processing.py</code> <pre><code>def clean_files( self, src: str, dest: str ) -&gt; None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t________\n\t\tCleans text files given a source directory (src) and destination directory (dest)\n\n\t\tParameters:\n\t\t----------\n\t\t- src (str): Source directory\n\t\t- dest (str): Destination directory\n\n\t\tReturns:\n\t\t--------\n\t\t- None\n\n\t\"\"\"\n\ttry:\n\t\tif src is None:\n\t\t\traise Exception( 'The argument \"src\" is required.' )\n\t\telif dest is None:\n\t\t\traise Exception( 'The argument \"dest\" is required.' )\n\t\telse:\n\t\t\tsource = src\n\t\t\tdestination = dest\n\t\t\tfiles = os.listdir( source )\n\t\t\tfor f in files:\n\t\t\t\tprocessed = [ ]\n\t\t\t\tfilename = os.path.basename( f )\n\t\t\t\tsource_path = source + '\\\\' + filename\n\t\t\t\ttext = open( source_path, 'r', encoding='utf-8', errors='ignore' ).read( )\n\t\t\t\tpunc = self.remove_special( text )\n\t\t\t\tdirty = self.split_sentences( punc )\n\t\t\t\tfor d in dirty:\n\t\t\t\t\tif d != \" \":\n\t\t\t\t\t\tlower = d.lower( )\n\t\t\t\t\t\tnormal = self.normalize_text( lower )\n\t\t\t\t\t\tslim = self.collapse_whitespace( normal )\n\t\t\t\t\t\tprocessed.append( slim )\n\n\t\t\t\tdest_path = destination + '\\\\' + filename\n\t\t\t\tclean = open( dest_path, 'wt', encoding='utf-8', errors='ignore' )\n\t\t\t\tfor p in processed:\n\t\t\t\t\tclean.write( p )\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = 'clean_files( self, src: str, dest: str )'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.clean_space","title":"clean_space","text":"<pre><code>clean_space(text)\n</code></pre> <p>Purpose:</p> <p>Removes extra spaces and blank words from the path pages.</p>"},{"location":"reference/api/#processing.Text.clean_space--parameters","title":"Parameters:","text":"<ul> <li>pages : str         The raw path pages path to be cleaned_lines.</li> </ul>"},{"location":"reference/api/#processing.Text.clean_space--returns","title":"Returns:","text":"<ul> <li>str         A cleaned_lines pages path with:                 - Consecutive whitespace reduced to a single space                 - Leading/trailing spaces removed                 - Blank words removed</li> </ul> Source code in <code>processing.py</code> <pre><code>def clean_space( self, text: str ) -&gt; str | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t_____________\n\t\tRemoves extra spaces and blank words from the path pages.\n\n\t\tParameters:\n\t\t-----------\n\t\t- pages : str\n\t\t\tThe raw path pages path to be cleaned_lines.\n\n\t\tReturns:\n\t\t--------\n\t\t- str\n\t\t\tA cleaned_lines pages path with:\n\t\t\t\t- Consecutive whitespace reduced to a single space\n\t\t\t\t- Leading/trailing spaces removed\n\t\t\t\t- Blank words removed\n\n\t\"\"\"\n\ttry:\n\t\tif text is None:\n\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\telse:\n\t\t\tself.raw_input = text.lower( )\n\t\t\ttabs = re.sub( r'[ \\t]+', ' ', text.lower( ) )\n\t\t\tcollapsed = re.sub( r'\\s+', ' ', tabs ).strip( )\n\t\t\tself.cleaned_text = collapsed\n\t\t\treturn self.cleaned_text\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = 'remove_errors( self, text: str ) -&gt; str'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.collapse_whitespace","title":"collapse_whitespace","text":"<pre><code>collapse_whitespace(text)\n</code></pre>"},{"location":"reference/api/#processing.Text.collapse_whitespace--purpose","title":"Purpose:","text":"<p>Removes extra spaces and blank words from the string 'text'.</p>"},{"location":"reference/api/#processing.Text.collapse_whitespace--parameters","title":"Parameters:","text":"<ul> <li>text : str</li> </ul>"},{"location":"reference/api/#processing.Text.collapse_whitespace--returns","title":"Returns:","text":"<p>A cleaned_lines path path with:         - Consecutive whitespace reduced to a single space         - Leading/trailing spaces removed         - Blank words removed</p> Source code in <code>processing.py</code> <pre><code>def collapse_whitespace( self, text: str ) -&gt; str | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t-----------\n\t\tRemoves extra spaces and blank words from the string 'text'.\n\n\t\tParameters:\n\t\t-----------\n\t\t- text : str\n\n\t\tReturns:\n\t\t--------\n\t\tA cleaned_lines path path with:\n\t\t\t- Consecutive whitespace reduced to a single space\n\t\t\t- Leading/trailing spaces removed\n\t\t\t- Blank words removed\n\n\t\"\"\"\n\ttry:\n\t\tif text is None:\n\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\telse:\n\t\t\tself.raw_input = text\n\t\t\tself.cleaned_text = re.sub( r'[ \\t]+', ' ', self.raw_input )\n\t\t\tself.cleaned_lines = [ line.strip( ) for line in self.cleaned_text.splitlines( ) ]\n\t\t\tself.lines = [ line for line in self.cleaned_lines if line ]\n\t\t\treturn ' '.join( self.lines )\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = 'collapse_whitespace( self, path: str ) -&gt; str:'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.compute_conditional_distribution","title":"compute_conditional_distribution","text":"<pre><code>compute_conditional_distribution(\n    lines, condition=None, process=True\n)\n</code></pre>"},{"location":"reference/api/#processing.Text.compute_conditional_distribution--purpose","title":"Purpose:","text":"<p>Computes a Conditional Frequency Distribution (CFD)  over a collection of documents.</p>"},{"location":"reference/api/#processing.Text.compute_conditional_distribution--parameters","title":"Parameters:","text":"<ul> <li> <p>documents (list):         A list of path sections (pages, paragraphs, etc.).</p> </li> <li> <p>condition (function):         A function to determine the condition/grouping. If None, uses document index.</p> </li> <li> <p>process (bool):         If True, applies normalization, tokenization,         stopword removal, and lemmatization.</p> </li> </ul> <ul> <li>ConditionalFreqDist:         An NLTK ConditionalFreqDist object mapping conditions to word frequencies.</li> </ul> Source code in <code>processing.py</code> <pre><code>def compute_conditional_distribution( self, lines: List[ str ], condition=None,\n                                      process: bool=True ) -&gt; ConditionalFreqDist | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t--------\n\t\tComputes a Conditional Frequency Distribution (CFD)\n\t\t over a collection of documents.\n\n\t\tParameters:\n\t\t-----------\n\t\t- documents (list):\n\t\t\tA list of path sections (pages, paragraphs, etc.).\n\n\t\t- condition (function):\n\t\t\tA function to determine the condition/grouping. If None, uses document index.\n\n\t\t- process (bool):\n\t\t\tIf True, applies normalization, tokenization,\n\t\t\tstopword removal, and lemmatization.\n\n\t\tReturns:\n\t\t- ConditionalFreqDist:\n\t\t\tAn NLTK ConditionalFreqDist object mapping conditions to word frequencies.\n\n\t\"\"\"\n\ttry:\n\t\tif lines is None:\n\t\t\traise Exception( 'The argument \"words\" is required.' )\n\t\telse:\n\t\t\tself.lines = lines\n\t\t\tcfd = ConditionalFreqDist( )\n\t\t\tfor idx, line in enumerate( lines ):\n\t\t\t\tkey = condition( line ) if condition else f'Doc_{idx}'\n\t\t\t\ttoks = self.tokenize_text( self.normalize_text( line ) if process else line )\n\t\t\t\tfor t in toks:\n\t\t\t\t\tcfd[ key ][ t ] += 1\n\t\t\tself.conditional_distribution = cfd\n\t\t\treturn cfd\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = ('compute_conditional_distribution( self, words: List[ str ], '\n\t\t                    'condition=None, process: bool=True ) -&gt; ConditionalFreqDist')\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.compute_frequency_distribution","title":"compute_frequency_distribution","text":"<pre><code>compute_frequency_distribution(lines)\n</code></pre>"},{"location":"reference/api/#processing.Text.compute_frequency_distribution--purpose","title":"Purpose:","text":"<p>Creates a word frequency freq_dist from a list of documents.</p>"},{"location":"reference/api/#processing.Text.compute_frequency_distribution--parameters","title":"Parameters:","text":"<ul> <li>lines (list): List of raw or preprocessed path documents.</li> <li>process (bool): Applies normalization, tokenization, stopword removal, and lemmatization.</li> </ul> <p>Returns: - dict: Dictionary of words and their corresponding frequencies.</p> Source code in <code>processing.py</code> <pre><code>def compute_frequency_distribution( self, lines: List[ str ] ) -&gt; FreqDist | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t--------\n\t\tCreates a word frequency freq_dist from a list of documents.\n\n\t\tParameters:\n\t\t-----------\n\t\t- lines (list): List of raw or preprocessed path documents.\n\t\t- process (bool): Applies normalization, tokenization, stopword removal,\n\t\tand lemmatization.\n\n\t\tReturns:\n\t\t- dict: Dictionary of words and their corresponding frequencies.\n\n\t\"\"\"\n\ttry:\n\t\tif lines is None:\n\t\t\traise Exception( 'The argument \"words\" is required.' )\n\t\telse:\n\t\t\tself.lines = lines\n\t\t\tall_tokens: list[ str ] = [ ]\n\t\t\tfor _line in lines:\n\t\t\t\ttoks = self.tokenize_text( _line )\n\t\t\t\tall_tokens.extend( toks )\n\t\t\tself.frequency_distribution = dict( Counter( all_tokens ) )\n\t\t\treturn self.frequency_distribution\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = ('compute_frequency_distribution( self, documents: list, process: '\n\t\t                    'bool=True) -&gt; FreqDist')\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.convert_jsonl","title":"convert_jsonl","text":"<pre><code>convert_jsonl(source, desination)\n</code></pre> <p>Purpose:</p> <p>Coverts text files to JSONL format given a source directory (Source)  and destination directory (destination)</p>"},{"location":"reference/api/#processing.Text.convert_jsonl--parameters","title":"Parameters:","text":"<ul> <li>source (str): Source directory</li> <li>destination (str): Destination directory</li> </ul>"},{"location":"reference/api/#processing.Text.convert_jsonl--returns","title":"Returns:","text":"<ul> <li>None</li> </ul> Source code in <code>processing.py</code> <pre><code>def convert_jsonl( self, source: str, desination: str ) -&gt; None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t________\n\t\tCoverts text files to JSONL format given a source directory (Source)\n\t\t and destination directory (destination)\n\n\t\tParameters:\n\t\t--------\n\t\t- source (str): Source directory\n\t\t- destination (str): Destination directory\n\n\t\tReturns:\n\t\t--------\n\t\t- None\n\n\t\"\"\"\n\ttry:\n\t\tif source is None:\n\t\t\traise Exception( 'The argument \"source\" is required.' )\n\t\telif desination is None:\n\t\t\traise Exception( 'The argument \"desination\" is required.' )\n\t\telse:\n\t\t\t_source = source\n\t\t\t_destination = desination\n\t\t\tself.files = os.listdir( _source )\n\t\t\t_processed = [ ]\n\t\t\tfor _f in self.files:\n\t\t\t\t_count = 0\n\t\t\t\t_basename = os.path.basename( _f )\n\t\t\t\t_sourcepath = _source + f'\\\\{_basename}'\n\t\t\t\t_text = open( _sourcepath, 'r', encoding='utf-8', errors='ignore' ).read( )\n\t\t\t\t_stops = self.remove_stopwords( _text )\n\t\t\t\t_tokens = self.tokenize_text( _stops )\n\t\t\t\t_chunks = self.chunk_text( _text )\n\t\t\t\t_filename = _basename.rstrip( '.txt' )\n\t\t\t\t_destinationpath = _destination + f'\\\\{_filename}.jsonl'\n\t\t\t\t_clean = open( _destinationpath, 'wt', encoding='utf-8', errors='ignore' )\n\t\t\t\tfor _i in range( len( _chunks ) ):\n\t\t\t\t\t_list = _chunks[ _i ]\n\t\t\t\t\t_part = ''.join( _list )\n\t\t\t\t\t_row = '{' + f'\\\"Line-{_i}\\\":\\\"{_part}\\\"' + '}' + '\\r'\n\t\t\t\t\t_processed.append( _row )\n\n\t\t\t\tfor _t in _processed:\n\t\t\t\t\t_clean.write( _t )\n\n\t\t\t\t_clean.flush( )\n\t\t\t\t_clean.close( )\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = 'convert_jsonl( self, source: str, desination: str )'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.create_vocabulary","title":"create_vocabulary","text":"<pre><code>create_vocabulary(freq_dist, min=1)\n</code></pre>"},{"location":"reference/api/#processing.Text.create_vocabulary--purpose","title":"Purpose:","text":"<p>Builds a vocabulary list from a frequency distribution by applying a minimum frequency threshold.</p>"},{"location":"reference/api/#processing.Text.create_vocabulary--parameters","title":"Parameters:","text":"<ul> <li> <p>freq_dist (dict):         A dictionary mapping words to their frequencies.</p> </li> <li> <p>min (int): Minimum num         of occurrences required for a word to be included.</p> </li> </ul>"},{"location":"reference/api/#processing.Text.create_vocabulary--returns","title":"Returns:","text":"<ul> <li>list: Sorted list of unique vocabulary words.</li> </ul> Source code in <code>processing.py</code> <pre><code>def create_vocabulary( self, freq_dist: Dict, min: int=1 ) -&gt; List[ str ] | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t---------\n\t\tBuilds a vocabulary list from a frequency\n\t\tdistribution by applying a minimum frequency threshold.\n\n\t\tParameters:\n\t\t-----------\n\t\t- freq_dist (dict):\n\t\t\tA dictionary mapping words to their frequencies.\n\n\t\t- min (int): Minimum num\n\t\t\tof occurrences required for a word to be included.\n\n\t\tReturns:\n\t\t--------\n\t\t- list: Sorted list of unique vocabulary words.\n\n\t\"\"\"\n\ttry:\n\t\tif freq_dist is None:\n\t\t\traise Exception( 'The argument \"freq_dist\" is required.' )\n\t\telse:\n\t\t\tself.frequency_distribution = freq_dist\n\t\t\tself.words = [ word for word, freq in freq_dist.items( ) if freq &gt;= min ]\n\t\t\tself.vocabulary = sorted( self.words )\n\t\t\treturn self.vocabulary\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = ('create_vocabulary( self, freq_dist: dict, min: int=1 ) -&gt; List['\n\t\t                    'str]')\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.create_word2vec","title":"create_word2vec","text":"<pre><code>create_word2vec(words, size=100, window=5, min=1)\n</code></pre>"},{"location":"reference/api/#processing.Text.create_word2vec--purpose","title":"Purpose:","text":"<p>Train a Word2Vec embedding small_model from tokenized sentences.</p>"},{"location":"reference/api/#processing.Text.create_word2vec--parameters","title":"Parameters:","text":"<ul> <li>sentences (get_list of get_list of str): List of tokenized sentences.</li> <li>vector_size (int): Dimensionality of word vec.</li> <li>window (int): Max distance between current and predicted word.</li> <li>min_count (int): Minimum frequency for inclusion in vocabulary.</li> </ul>"},{"location":"reference/api/#processing.Text.create_word2vec--returns","title":"Returns:","text":"<ul> <li>Word2Vec: Trained Gensim Word2Vec small_model.</li> </ul> Source code in <code>processing.py</code> <pre><code>def create_word2vec( self, words: List[ List[ str ] ], size=100, window=5, min=1 ) -&gt; Word2Vec | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t--------\n\t\tTrain a Word2Vec embedding small_model from tokenized sentences.\n\n\t\tParameters:\n\t\t--------\n\t\t- sentences (get_list of get_list of str): List of tokenized sentences.\n\t\t- vector_size (int): Dimensionality of word vec.\n\t\t- window (int): Max distance between current and predicted word.\n\t\t- min_count (int): Minimum frequency for inclusion in vocabulary.\n\n\t\tReturns:\n\t\t-------\n\t\t- Word2Vec: Trained Gensim Word2Vec small_model.\n\n\t\"\"\"\n\ttry:\n\t\tif words is None:\n\t\t\traise Exception( 'The argument \"words\" is required.' )\n\t\telse:\n\t\t\tself.words = words\n\t\t\treturn Word2Vec( sentences=self.words, vector_size=size,\n\t\t\t\twindow=window, min_count=min )\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = ('create_word2vec( self, words: List[ str ], '\n\t\t                    'size=100, window=5, min=1 ) -&gt; Word2Vec')\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.create_wordbag","title":"create_wordbag","text":"<pre><code>create_wordbag(words)\n</code></pre>"},{"location":"reference/api/#processing.Text.create_wordbag--purpose","title":"Purpose:","text":"<p>Construct a Bag-of-Words (BoW) frequency dictionary from a list of strings.</p>"},{"location":"reference/api/#processing.Text.create_wordbag--parameters","title":"Parameters:","text":"<ul> <li>words (list): List of words from a document.</li> </ul>"},{"location":"reference/api/#processing.Text.create_wordbag--returns","title":"Returns:","text":"<ul> <li>dict: Word frequency dictionary.</li> </ul> Source code in <code>processing.py</code> <pre><code>def create_wordbag( self, words: List[ str ] ) -&gt; dict | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t--------\n\t\tConstruct a Bag-of-Words (BoW) frequency dictionary from a list of strings.\n\n\t\tParameters:\n\t\t-----------\n\t\t- words (list): List of words from a document.\n\n\t\tReturns:\n\t\t--------\n\t\t- dict: Word frequency dictionary.\n\n\t\"\"\"\n\ttry:\n\t\tif words is None:\n\t\t\traise Exception( 'The argument \"words\" is required.' )\n\t\telse:\n\t\t\tself.words = words\n\t\t\treturn dict( Counter( self.words ) )\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = 'create_wordbag( self, words: List[ str ] ) -&gt; dict'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.normalize_text","title":"normalize_text","text":"<pre><code>normalize_text(text)\n</code></pre>"},{"location":"reference/api/#processing.Text.normalize_text--purpose","title":"Purpose:","text":"<p>Normalizes the path pages path.   - Converts pages to lowercase   - Removes accented characters (e.g., \u00e9 -&gt; e)   - Removes leading/trailing spaces   - Collapses multiple whitespace characters into a single space</p>"},{"location":"reference/api/#processing.Text.normalize_text--parameters","title":"Parameters:","text":"<ul> <li>pages : str         The raw path pages path to be normalized.</li> </ul>"},{"location":"reference/api/#processing.Text.normalize_text--returns","title":"Returns:","text":"<ul> <li>str         A normalized, cleaned_lines version of the path path.</li> </ul> Source code in <code>processing.py</code> <pre><code>def normalize_text( self, text: str ) -&gt; str | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t-----------\n\t\tNormalizes the path pages path.\n\t\t  - Converts pages to lowercase\n\t\t  - Removes accented characters (e.g., \u00e9 -&gt; e)\n\t\t  - Removes leading/trailing spaces\n\t\t  - Collapses multiple whitespace characters into a single space\n\n\t\tParameters:\n\t\t-----------\n\t\t- pages : str\n\t\t\tThe raw path pages path to be normalized.\n\n\t\tReturns:\n\t\t--------\n\t\t- str\n\t\t\tA normalized, cleaned_lines version of the path path.\n\n\t\"\"\"\n\ttry:\n\t\tif text is None:\n\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\telse:\n\t\t\tself.raw_input = text\n\t\t\tself.normalized = unicodedata.normalize( 'NFKD', text ).encode( 'ascii',\n\t\t\t\t'ignore' ).decode( 'utf-8' )\n\t\t\tself.normalized = re.sub( r'\\s+', ' ', self.normalized ).strip( ).lower( )\n\t\t\treturn self.normalized\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = 'normalize_text( self, text: str ) -&gt; str'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.remove_headers","title":"remove_headers","text":"<pre><code>remove_headers(pages, min=3)\n</code></pre>"},{"location":"reference/api/#processing.Text.remove_headers--purpose","title":"Purpose:","text":"<p>Removes repetitive headers and footers across a list of pages by frequency analysis.</p>"},{"location":"reference/api/#processing.Text.remove_headers--parameters","title":"Parameters:","text":"<ul> <li> <p>pages (list of str):         A list where each element is the full path of one page.</p> </li> <li> <p>min (int):         Minimum num of times a line must appear at the top/bottom to be a header/footer.</p> </li> </ul>"},{"location":"reference/api/#processing.Text.remove_headers--returns","title":"Returns:","text":"<ul> <li>list of str:         List of cleaned_lines page words without detected headers/footers.</li> </ul> Source code in <code>processing.py</code> <pre><code>def remove_headers( self, pages: List[ str ], min: int=3 ) -&gt; List[ str ] | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t--------\n\t\tRemoves repetitive headers and footers across a list of pages by frequency analysis.\n\n\t\tParameters:\n\t\t-----------\n\t\t- pages (list of str):\n\t\t\tA list where each element is the full path of one page.\n\n\t\t- min (int):\n\t\t\tMinimum num of times a line must appear at the top/bottom to be a header/footer.\n\n\t\tReturns:\n\t\t---------\n\t\t- list of str:\n\t\t\tList of cleaned_lines page words without detected headers/footers.\n\n\t\"\"\"\n\ttry:\n\t\tif pages is None:\n\t\t\traise Exception( 'The argument \"pages\" is required.' )\n\t\telse:\n\t\t\t_headers = defaultdict( int )\n\t\t\t_footers = defaultdict( int )\n\t\t\tself.pages = pages\n\n\t\t\t# First pass: collect frequency of top/bottom words\n\t\t\tfor _page in self.pages:\n\t\t\t\tself.lines = _page.strip( ).splitlines( )\n\t\t\t\tif not self.lines:\n\t\t\t\t\tcontinue\n\t\t\t\t_headers[ self.lines[ 0 ].strip( ) ] += 1\n\t\t\t\t_footers[ self.lines[ -1 ].strip( ) ] += 1\n\n\t\t\t# Identify candidates for removal\n\t\t\t_head = { line for line, count in _headers.items( ) if\n\t\t\t          count &gt;= min }\n\t\t\t_foot = { line for line, count in _footers.items( ) if\n\t\t\t          count &gt;= min }\n\n\t\t\t# Second pass: clean pages\n\t\t\tfor _page in self.pages:\n\t\t\t\tif not self.lines:\n\t\t\t\t\tcontinue\n\t\t\t\tself.lines = _page.strip( ).splitlines( )\n\t\t\t\tif not self.lines:\n\t\t\t\t\tself.cleaned_pages.append( _page )\n\t\t\t\t\tcontinue\n\n\t\t\t\t# Remove header\n\t\t\t\tif self.lines[ 0 ].strip( ) in _head:\n\t\t\t\t\tself.lines = self.lines[ 1: ]\n\n\t\t\t\t# Remove footer\n\t\t\t\tif self.lines and self.lines[ -1 ].strip( ) in _foot:\n\t\t\t\t\tself.lines = self.lines[ : -1 ]\n\n\t\t\t\tself.cleaned_pages.append( '\\n'.join( self.lines ) )\n\t\t\t_retval = self.cleaned_pages\n\t\t\treturn _retval\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = ('remove_headers( self, pages: List[ str ], min: int=3 ) -&gt; List['\n\t\t                    'str]')\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.remove_html","title":"remove_html","text":"<pre><code>remove_html(text)\n</code></pre>"},{"location":"reference/api/#processing.Text.remove_html--purpose","title":"Purpose:","text":"<p>Removes HTML tags from the path path path.</p> This function <ul> <li>Parses the path as HTML</li> <li>Extracts and returns only the visible content without tags</li> </ul>"},{"location":"reference/api/#processing.Text.remove_html--parameters","title":"Parameters:","text":"<ul> <li>pages : str         The path path containing HTML tags.</li> </ul>"},{"location":"reference/api/#processing.Text.remove_html--returns","title":"Returns:","text":"<ul> <li>str         A cleaned_lines path with all HTML tags removed.</li> </ul> Source code in <code>processing.py</code> <pre><code>def remove_html( self, text: str ) -&gt; str | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t--------\n\t\tRemoves HTML tags from the path path path.\n\n\t\tThis function:\n\t\t  - Parses the path as HTML\n\t\t  - Extracts and returns only the visible content without tags\n\n\t\tParameters:\n\t\t-----------\n\t\t- pages : str\n\t\t\tThe path path containing HTML tags.\n\n\t\tReturns:\n\t\t--------\n\t\t- str\n\t\t\tA cleaned_lines path with all HTML tags removed.\n\n\t\"\"\"\n\ttry:\n\t\tif text is None:\n\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\telse:\n\t\t\tself.raw_html = text\n\t\t\tself.cleaned_html = BeautifulSoup( self.raw_html, 'html.parser' ).get_text( )\n\t\t\treturn self.cleaned_html\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = 'remove_html( self, text: str ) -&gt; str'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.remove_markdown","title":"remove_markdown","text":"<pre><code>remove_markdown(text)\n</code></pre>"},{"location":"reference/api/#processing.Text.remove_markdown--purpose","title":"Purpose:","text":"<p>Removes Markdown syntax (e.g., *, #, [], etc.)</p>"},{"location":"reference/api/#processing.Text.remove_markdown--parameters","title":"Parameters:","text":"<ul> <li>pages : str         The formatted path pages.</li> </ul>"},{"location":"reference/api/#processing.Text.remove_markdown--returns","title":"Returns:","text":"<ul> <li>str         A cleaned_lines version of the pages with formatting removed.</li> </ul> Source code in <code>processing.py</code> <pre><code>def remove_markdown( self, text: str ) -&gt; str | None:\n\t\"\"\"\n\n\n\t\tPurpose:\n\t\t-----------\n\t\tRemoves Markdown syntax (e.g., *, #, [], etc.)\n\n\t\tParameters:\n\t\t-----------\n\t\t- pages : str\n\t\t\tThe formatted path pages.\n\n\t\tReturns:\n\t\t--------\n\t\t- str\n\t\t\tA cleaned_lines version of the pages with formatting removed.\n\n\t\"\"\"\n\ttry:\n\t\tif text is None:\n\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\telse:\n\t\t\tself.raw_input = text\n\t\t\tself.cleaned_text = re.sub( r'\\[.*?]\\(.*?\\)', '', text )\n\t\t\tself.corrected = re.sub( r'[`_*#~&gt;-]', '', self.cleaned_text )\n\t\t\t_retval = re.sub( r'!\\[.*?]\\(.*?\\)', '', self.corrected )\n\t\t\treturn _retval\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = 'remove_markdown( self, path: str ) -&gt; str'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.remove_punctuation","title":"remove_punctuation","text":"<pre><code>remove_punctuation(text)\n</code></pre>"},{"location":"reference/api/#processing.Text.remove_punctuation--purpose","title":"Purpose:","text":"<p>Removes all punctuation characters from the path path path.</p>"},{"location":"reference/api/#processing.Text.remove_punctuation--parameters","title":"Parameters:","text":"<ul> <li>pages : str         The path path path to be cleaned_lines.</li> </ul>"},{"location":"reference/api/#processing.Text.remove_punctuation--returns","title":"Returns:","text":"<ul> <li>str         The path path with all punctuation removed.</li> </ul> Source code in <code>processing.py</code> <pre><code>def remove_punctuation( self, text: str ) -&gt; str | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t-----------\n\t\tRemoves all punctuation characters from the path path path.\n\n\t\tParameters:\n\t\t-----------\n\t\t- pages : str\n\t\t\tThe path path path to be cleaned_lines.\n\n\t\tReturns:\n\t\t--------\n\t\t- str\n\t\t\tThe path path with all punctuation removed.\n\n\t\"\"\"\n\ttry:\n\t\tif text is None:\n\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\telse:\n\t\t\tself.raw_input = text\n\t\t\tself.translator = str.maketrans( '', '', string.punctuation )\n\t\t\tself.cleaned_text = self.raw_input.translate( self.translator )\n\t\t\treturn self.cleaned_text\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = 'remove_punctuation( self, text: str ) -&gt; str:'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.remove_special","title":"remove_special","text":"<pre><code>remove_special(text)\n</code></pre>"},{"location":"reference/api/#processing.Text.remove_special--purpose","title":"Purpose:","text":"<p>Removes special characters from the path path path.</p> This function <ul> <li>Retains only alphanumeric characters and whitespace</li> <li>Removes symbols like @, #, $, %, &amp;, etc.</li> <li>Preserves letters, numbers, and spaces</li> </ul>"},{"location":"reference/api/#processing.Text.remove_special--parameters","title":"Parameters:","text":"<ul> <li>pages : str         The raw path path path potentially         containing special characters.</li> </ul>"},{"location":"reference/api/#processing.Text.remove_special--returns","title":"Returns:","text":"<ul> <li>str         A cleaned_lines path containing         only letters, numbers, and spaces.</li> </ul> Source code in <code>processing.py</code> <pre><code>def remove_special( self, text: str ) -&gt; str | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t-----------\n\t\tRemoves special characters from the path path path.\n\n\t\tThis function:\n\t\t  - Retains only alphanumeric characters and whitespace\n\t\t  - Removes symbols like @, #, $, %, &amp;, etc.\n\t\t  - Preserves letters, numbers, and spaces\n\n\t\tParameters:\n\t\t-----------\n\t\t- pages : str\n\t\t\tThe raw path path path potentially\n\t\t\tcontaining special characters.\n\n\t\tReturns:\n\t\t--------\n\t\t- str\n\t\t\tA cleaned_lines path containing\n\t\t\tonly letters, numbers, and spaces.\n\n\t\"\"\"\n\ttry:\n\t\tif text is None:\n\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\telse:\n\t\t\tcleaned = [ ]\n\t\t\tkeepers = [ '(', ')', '$', '. ', ';', ':' ]\n\t\t\tfor char in text:\n\t\t\t\tif char in keepers:\n\t\t\t\t\tcleaned.append( char )\n\t\t\t\telif char.isalnum( ) or char.isspace( ):\n\t\t\t\t\tcleaned.append( char )\n\t\t\treturn ''.join( cleaned )\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = 'remove_special( self, text: str ) -&gt; str:'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.remove_stopwords","title":"remove_stopwords","text":"<pre><code>remove_stopwords(text)\n</code></pre>"},{"location":"reference/api/#processing.Text.remove_stopwords--purpose","title":"Purpose:","text":"<p>This function:   - Removes English stopwords from the path pages path.   - Tokenizes the path pages   - Removes common stopwords (e.g., \"the\", \"is\", \"and\", etc.)   - Returns the pages with only meaningful words</p>"},{"location":"reference/api/#processing.Text.remove_stopwords--parameters","title":"Parameters:","text":"<ul> <li>pages : str         The text string.</li> </ul>"},{"location":"reference/api/#processing.Text.remove_stopwords--returns","title":"Returns:","text":"<ul> <li>str         A text string without stopwords.</li> </ul> Source code in <code>processing.py</code> <pre><code>def remove_stopwords( self, text: str ) -&gt; str | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t-----------\n\t\tThis function:\n\t\t  - Removes English stopwords from the path pages path.\n\t\t  - Tokenizes the path pages\n\t\t  - Removes common stopwords (e.g., \"the\", \"is\", \"and\", etc.)\n\t\t  - Returns the pages with only meaningful words\n\n\n\t\tParameters:\n\t\t-----------\n\t\t- pages : str\n\t\t\tThe text string.\n\n\t\tReturns:\n\t\t--------\n\t\t- str\n\t\t\tA text string without stopwords.\n\n\t\"\"\"\n\ttry:\n\t\tif text is None:\n\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\telse:\n\t\t\tself.raw_input = text.lower( )\n\t\t\tself.stop_words = stopwords.words( 'english' )\n\t\t\tself.tokens = nltk.word_tokenize( self.raw_input )\n\t\t\tself.cleaned_tokens = [ w for w in self.tokens if\n\t\t\t                        w.isalnum( ) and w not in self.stop_words ]\n\t\t\tself.cleaned_text = ' '.join( self.cleaned_tokens )\n\t\t\treturn self.cleaned_text\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = 'remove_stopwords( self, text: str ) -&gt; str'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.split_pages","title":"split_pages","text":"<pre><code>split_pages(path, delimit='\\x0c')\n</code></pre>"},{"location":"reference/api/#processing.Text.split_pages--purpose","title":"Purpose:","text":"<p>Reads path from a file, splits it into words, and groups them into path.</p>"},{"location":"reference/api/#processing.Text.split_pages--parameters","title":"Parameters:","text":"<ul> <li>path (str): Path to the path file.</li> <li>delimiter (str): Page separator path (default is '\f' for form feed).</li> </ul>"},{"location":"reference/api/#processing.Text.split_pages--returns","title":"Returns:","text":"<ul> <li>list[ str ]  where each element is the path.</li> </ul> Source code in <code>processing.py</code> <pre><code>def split_pages( self, path: str, delimit: str = '\\f' ) -&gt; List[ str ] | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t--------\n\t\tReads path from a file, splits it into words,\n\t\tand groups them into path.\n\n\t\tParameters:\n\t\t-----------\n\t\t- path (str): Path to the path file.\n\t\t- delimiter (str): Page separator path (default is '\\f' for form feed).\n\n\t\tReturns:\n\t\t---------\n\t\t- list[ str ]  where each element is the path.\n\n\t\"\"\"\n\ttry:\n\t\tif path is None:\n\t\t\traise Exception( 'The argument \"path\" is required' )\n\t\telse:\n\t\t\tself.file_path = path\n\t\t\twith open( self.file_path, 'r', encoding = 'utf-8', errors = 'ignore' ) as _file:\n\t\t\t\t_content = _file.read( )\n\t\t\t\tself.raw_pages = _content.split( delimit )\n\n\t\t\tfor _page in self.raw_pages:\n\t\t\t\tself.lines = _page.strip( ).splitlines( )\n\t\t\t\tself.cleaned_text = '\\n'.join(\n\t\t\t\t\t[ l.strip( ) for l in self.lines if l.strip( ) ] )\n\t\t\t\tself.cleaned_pages.append( self.cleaned_text )\n\t\t\treturn self.cleaned_pages\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = 'split_pages( self, path: str, delimit: str=\"\\f\" ) -&gt; List[ str ]'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.split_paragraphs","title":"split_paragraphs","text":"<pre><code>split_paragraphs(path)\n</code></pre> <pre><code>                    Purpose:\n                    ---------\n                    Reads  a file and splits it into paragraphs. A paragraph is defined as a block\n                    of path separated by one or more empty lines (eg, '\n</code></pre> <p>').</p> <pre><code>                    Parameters:\n                    -----------\n                    - path (str): Path to the path file.\n\n                    Returns:\n                    ---------\n                    - list of str: List of paragraph strings.\n</code></pre> Source code in <code>processing.py</code> <pre><code>def split_paragraphs( self, path: str ) -&gt; List[ str ] | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t---------\n\t\tReads  a file and splits it into paragraphs. A paragraph is defined as a block\n\t\tof path separated by one or more empty lines (eg, '\\n\\n').\n\n\t\tParameters:\n\t\t-----------\n\t\t- path (str): Path to the path file.\n\n\t\tReturns:\n\t\t---------\n\t\t- list of str: List of paragraph strings.\n\n\t\"\"\"\n\ttry:\n\t\tif path is None:\n\t\t\traise Exception( 'The argument \"path\" is required.' )\n\t\telse:\n\t\t\tself.file_path = path\n\t\t\twith open( self.file_path, 'r', encoding='utf-8', errors='ignore' ) as _file:\n\t\t\t\tself.raw_input = _file.read( )\n\t\t\t\tself.paragraphs = [ pg.strip( ) for pg in self.raw_input.split( '\\n\\n' ) if\n\t\t\t\t                    pg.strip( ) ]\n\n\t\t\t\treturn self.paragraphs\n\texcept UnicodeDecodeError:\n\t\twith open( self.file_path, 'r', encoding = 'latin1', errors = 'ignore' ) as _file:\n\t\t\tself.raw_input = _file.read( )\n\t\t\tself.paragraphs = [ pg.strip( ) for pg in self.raw_input.split( '\\n\\n' ) if\n\t\t\t                    pg.strip( ) ]\n\t\t\treturn self.paragraphs\n</code></pre>"},{"location":"reference/api/#processing.Text.split_sentences","title":"split_sentences","text":"<pre><code>split_sentences(text)\n</code></pre> <p>Purpose:</p> <p>Splits the text string into a list of strings using NLTK's Punkt sentence tokenizer. This function is useful for preparing text for further processing, such as tokenization, parsing, or named entity recognition.</p>"},{"location":"reference/api/#processing.Text.split_sentences--parameters","title":"Parameters","text":"<ul> <li>text : str The raw text string to be segmented into sentences.</li> </ul>"},{"location":"reference/api/#processing.Text.split_sentences--returns","title":"Returns","text":"<ul> <li>List[ str ] A list of sentence strings, each corresponding to a single sentence detected in the text text.</li> </ul> Source code in <code>processing.py</code> <pre><code>def split_sentences( self, text: str ) -&gt; List[ str ] | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t________\n\t\tSplits the text string into a list of\n\t\tstrings using NLTK's Punkt sentence tokenizer.\n\t\tThis function is useful for preparing text for further processing,\n\t\tsuch as tokenization, parsing, or named entity recognition.\n\n\t\tParameters\n\t\t----------\n\t\t- text : str\n\t\tThe raw text string to be segmented into sentences.\n\n\t\tReturns\n\t\t-------\n\t\t- List[ str ]\n\t\tA list of sentence strings, each corresponding to a single sentence detected\n\t\tin the text text.\n\n\t\"\"\"\n\ttry:\n\t\tif text is None:\n\t\t\traise Exception( 'The argument \"text\" is required.' )\n\t\telse:\n\t\t\tself.lines = nltk.sent_tokenize( text )\n\t\t\treturn self.lines\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = 'split_sentences( self, text: str ) -&gt; List[ str ]'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.tiktokenize","title":"tiktokenize","text":"<pre><code>tiktokenize(text, encoding='cl100k_base')\n</code></pre>"},{"location":"reference/api/#processing.Text.tiktokenize--purpose","title":"Purpose:","text":"<p>Tokenizes text text into subword words using OpenAI's tiktoken tokenizer. This function leverages the tiktoken library, which provides byte-pair encoding (BPE) tokenization used in models such as GPT-3.5 and GPT-4. Unlike standard word tokenization, this function splits text into model-specific subword units.</p>"},{"location":"reference/api/#processing.Text.tiktokenize--parameters","title":"Parameters","text":"<ul> <li> <p>text : str         The text string to be tokenized.</p> </li> <li> <p>model : str, optional         The tokenizer model to use. Examples include 'cl100k_base' (default),         'gpt-3.5-turbo', or 'gpt-4'. Ensure the model is supported by tiktoken.</p> </li> </ul>"},{"location":"reference/api/#processing.Text.tiktokenize--returns","title":"Returns","text":"<ul> <li>List[str]         A list of string words representing BPE subword units.</li> </ul> Source code in <code>processing.py</code> <pre><code>def tiktokenize( self, text: str, encoding: str='cl100k_base' ) -&gt; List[ str ] | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t---------\n\t\tTokenizes text text into subword words using OpenAI's tiktoken tokenizer.\n\t\tThis function leverages the tiktoken library, which provides byte-pair encoding (BPE)\n\t\ttokenization used in models such as GPT-3.5 and GPT-4. Unlike standard word\n\t\ttokenization,\n\t\tthis function splits text into model-specific subword units.\n\n\t\tParameters\n\t\t----------\n\t\t- text : str\n\t\t\tThe text string to be tokenized.\n\n\t\t- model : str, optional\n\t\t\tThe tokenizer model to use. Examples include 'cl100k_base' (default),\n\t\t\t'gpt-3.5-turbo', or 'gpt-4'. Ensure the model is supported by tiktoken.\n\n\t\tReturns\n\t\t-------\n\t\t- List[str]\n\t\t\tA list of string words representing BPE subword units.\n\n\t\"\"\"\n\ttry:\n\t\tif text is None:\n\t\t\traise Exception( 'The argument \"text\" was None' )\n\t\telse:\n\t\t\tself.encoding = tiktoken.get_encoding( encoding )\n\t\t\ttoken_ids = self.encoding.encode( text )\n\t\t\treturn token_ids  # or [self.encoding.decode_single_token_bytes(t) for t in token_ids]\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = ('tiktokenize( self, text: str, encoding: str=\"cl100k_base\" ) -&gt; '\n\t\t                    'List[ str ]')\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.tokenize_text","title":"tokenize_text","text":"<pre><code>tokenize_text(text)\n</code></pre>"},{"location":"reference/api/#processing.Text.tokenize_text--purpose","title":"Purpose:","text":"<p>Splits the raw path removes non-words and returns words</p>"},{"location":"reference/api/#processing.Text.tokenize_text--parameters","title":"Parameters:","text":"<ul> <li>cleaned_line: (str) - clean documents.</li> </ul> <p>Returns: - list: Cleaned and normalized documents.</p> Source code in <code>processing.py</code> <pre><code>def tokenize_text( self, text: str ) -&gt; List[ str ] | None:\n\t'''\n\n\t\tPurpose:\n\t\t---------\n\t\tSplits the raw path removes non-words and returns words\n\n\t\tParameters:\n\t\t-----------\n\t\t- cleaned_line: (str) - clean documents.\n\n\t\tReturns:\n\t\t- list: Cleaned and normalized documents.\n\n\t'''\n\ttry:\n\t\tif text is None:\n\t\t\traise Exception( 'The argument \"text\" was None' )\n\t\telse:\n\t\t\t_tokens = nltk.word_tokenize( text )\n\t\t\tself.words = [ t for t in _tokens ]\n\t\t\tself.tokens = [ re.sub( r'[^\\w\"-]', '', w ) for w in self.words if w.strip( ) ]\n\t\t\treturn self.tokens\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = 'tokenize_text( self, path: str ) -&gt; List[ str ]'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Text.tokenize_words","title":"tokenize_words","text":"<pre><code>tokenize_words(words)\n</code></pre>"},{"location":"reference/api/#processing.Text.tokenize_words--purpose","title":"Purpose:","text":"<ul> <li>Tokenizes the path pages path into individual word words.</li> <li>Converts pages to lowercase</li> <li>Uses NLTK's word_tokenize to split   the pages into words and punctuation words</li> </ul>"},{"location":"reference/api/#processing.Text.tokenize_words--parameters","title":"Parameters:","text":"<ul> <li>words : List[ str ]         A list of strings to be tokenized.</li> </ul>"},{"location":"reference/api/#processing.Text.tokenize_words--returns","title":"Returns:","text":"<ul> <li>A list of token strings (words and punctuation) extracted from the pages.</li> </ul> Source code in <code>processing.py</code> <pre><code>def tokenize_words( self, words: List[ str ] ) -&gt; List[ List[ str ] ]| None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t-----------\n\t\t  - Tokenizes the path pages path into individual word words.\n\t\t  - Converts pages to lowercase\n\t\t  - Uses NLTK's word_tokenize to split\n\t\t  the pages into words and punctuation words\n\n\t\tParameters:\n\t\t-----------\n\t\t- words : List[ str ]\n\t\t\tA list of strings to be tokenized.\n\n\t\tReturns:\n\t\t--------\n\t\t- A list of token strings (words and punctuation) extracted from the pages.\n\n\t\"\"\"\n\ttry:\n\t\tif words is None:\n\t\t\traise Exception( 'The argument \"words\" was None' )\n\t\telse:\n\t\t\tself.words = words\n\t\t\tfor w in self.words:\n\t\t\t\t_tokens = nltk.word_tokenize( w )\n\t\t\t\tself.tokens.append( _tokens )\n\t\t\treturn self.tokens\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Text'\n\t\texception.method = 'tokenize_words( self, path: str ) -&gt; List[ str ]'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Word","title":"Word","text":"<p>               Bases: <code>Processor</code></p>"},{"location":"reference/api/#processing.Word--purpose","title":"Purpose:","text":"<p>A class to extract, clean, and analyze text from Microsoft Word documents.</p>"},{"location":"reference/api/#processing.Word--methods","title":"Methods:","text":"<p>split_sentences( self ) -&gt; None clean_sentences( self ) -&gt; None create_vocabulary( self ) -&gt; None compute_frequency_distribution( self ) -&gt; None summarize( self ) -&gt; None:</p> Source code in <code>processing.py</code> <pre><code>class Word( Processor ):\n\t\"\"\"\n\n\t\tPurpose:\n\t\t--------\n\t\tA class to extract, clean, and analyze text from Microsoft Word documents.\n\n\t\tMethods:\n\t\t--------\n\t\tsplit_sentences( self ) -&gt; None\n\t\tclean_sentences( self ) -&gt; None\n\t\tcreate_vocabulary( self ) -&gt; None\n\t\tcompute_frequency_distribution( self ) -&gt; None\n\t\tsummarize( self ) -&gt; None:\n\n\t\"\"\"\n\tsentences: Optional[ List[ str ] ]\n\tcleaned_sentences: Optional[ List[ str ] ]\n\tdocument: Optional[ Docx ]\n\traw_text: Optional[ str ]\n\tparagraphs: Optional[ List[ str ] ]\n\tfile_path: Optional[ str ]\n\tvocabulary: Optional[ set ]\n\tdocument: Optional[ Docx ]\n\n\tdef __init__( self, filepath: str ) -&gt; None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t--------\n\t\t\tInitializes the WordTextProcessor with the path to the .docx file.\n\n\t\t\tParameters:\n\t\t\t----------\n\t\t\t:param filepath: Path to the Microsoft Word document (.docx)\n\n\t\t\"\"\"\n\t\tsuper( ).__init__( )\n\t\tself.file_path = filepath\n\t\tself.raw_text = ''\n\t\tself.paragraphs = [ ]\n\t\tself.sentences = [ ]\n\t\tself.cleaned_sentences = [ ]\n\t\tself.vocabulary = set( )\n\n\tdef __dir__( self ) -&gt; List[ str ] | None:\n\t\t'''\n\n\t\t\tPurpose:\n\t\t\t---------\n\t\t\tProvides a list of strings representing class members.\n\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\t- self\n\n\t\t\tReturns:\n\t\t\t--------\n\t\t\t- List[ str ] | None\n\n\t\t'''\n\t\treturn [ 'extract_text', 'split_sentences', 'clean_sentences',\n\t\t         'create_vocabulary', 'compute_frequency_distribution',\n\t\t         'summarize', 'filepath', 'raw_text', 'paragraphs',\n\t\t         'sentences', 'cleaned_sentences', 'vocabulary', 'freq_dist' ]\n\n\tdef extract_text( self ) -&gt; str | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t--------\n\t\t\tExtracts raw text and paragraphs from the .docx file.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tself.document = Docx( self.file_path )\n\t\t\tself.paragraphs = [ para.text.strip( ) for para in self.document.paragraphs if\n\t\t\t                    para.text.strip( ) ]\n\t\t\tself.raw_text = '\\n'.join( self.paragraphs )\n\t\t\treturn self.raw_text\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Word'\n\t\t\texception.method = 'extract_text( self ) -&gt; str'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef split_sentences( self ) -&gt; List[ str ] | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t--------\n\t\t\tSplits the raw text into sentences.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tself.sentences = sent_tokenize( self.raw_text )\n\t\t\treturn self.sentences\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Word'\n\t\t\texception.method = 'split_sentences( self ) -&gt; List[ str ]'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef clean_sentences( self ) -&gt; List[ str ] | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t-------\n\t\t\tCleans each _sentence: removes extra whitespace, punctuation, and lowers the text.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tfor _sentence in self.sentences:\n\t\t\t\t_sentence = re.sub( r'[\\r\\n\\t]+', ' ', _sentence )\n\t\t\t\t_sentence = re.sub( r\"[^a-zA-Z0-9\\s']\", '', _sentence )\n\t\t\t\t_sentence = re.sub( r'\\s{2,}', ' ', _sentence ).strip( ).lower( )\n\t\t\t\tself.cleaned_sentences.append( _sentence )\n\n\t\t\treturn self.cleaned_sentences\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Word'\n\t\t\texception.method = 'clean_sentences( self ) -&gt; List[ str ]'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef create_vocabulary( self ) -&gt; set | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t--------\n\t\t\tComputes vocabulary terms from cleaned sentences.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\tall_words = [ ]\n\t\t\tself.stop_words = set( stopwords.words( 'english' ) )\n\t\t\tfor _sentence in self.cleaned_sentences:\n\t\t\t\ttokens = word_tokenize( _sentence )\n\t\t\t\ttokens = [ token for token in tokens if\n\t\t\t\t           token.isalpha( ) and token not in self.stop_words ]\n\t\t\t\tall_words.extend( tokens )\n\t\t\tself.vocabulary = set( all_words )\n\t\t\treturn self.vocabulary\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Word'\n\t\t\texception.method = 'create_vocabulary( self ) -&gt; List[ str ]'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef compute_frequency_distribution( self ) -&gt; Dict[ str, int ] | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t-------\n\t\t\tComputes frequency distribution of the vocabulary.\n\n\t\t\"\"\"\n\t\ttry:\n\t\t\twords = [ ]\n\t\t\tfor sentence in self.cleaned_sentences:\n\t\t\t\ttokens = word_tokenize( sentence )\n\t\t\t\ttokens = [ token for token in tokens if token.isalpha( ) ]\n\t\t\t\twords.extend( tokens )\n\t\t\tself.frequency_distribution = dict( Counter( words ) )\n\t\t\treturn self.frequency_distribution\n\t\texcept Exception as e:\n\t\t\texception = Error( e )\n\t\t\texception.module = 'processing'\n\t\t\texception.cause = 'Word'\n\t\t\texception.method = 'compute_frequency_distribution( self ) -&gt; Dict[ str, int ]'\n\t\t\terror = ErrorDialog( exception )\n\t\t\terror.show( )\n\n\tdef summarize( self ) -&gt; List[ str ] | None:\n\t\t\"\"\"\n\n\t\t\tPurpose:\n\t\t\t-------\n\t\t\tPrints a summary of extracted and processed text.\n\n\t\t\"\"\"\n\t\tprint( f'Document: {self.file_path}' )\n\t\tprint( f'Paragraphs: {len( self.paragraphs )}' )\n\t\tprint( f'Sentences: {len( self.sentences )}' )\n\t\tprint( f'Vocabulary Size: {len( self.vocabulary )}' )\n\t\tprint(\n\t\t\tf'Top 10 Frequent Words: {Counter( self.frequency_distribution ).most_common( 10 )}' )\n</code></pre>"},{"location":"reference/api/#processing.Word.__dir__","title":"__dir__","text":"<pre><code>__dir__()\n</code></pre>"},{"location":"reference/api/#processing.Word.__dir__--purpose","title":"Purpose:","text":"<p>Provides a list of strings representing class members.</p>"},{"location":"reference/api/#processing.Word.__dir__--parameters","title":"Parameters:","text":"<ul> <li>self</li> </ul>"},{"location":"reference/api/#processing.Word.__dir__--returns","title":"Returns:","text":"<ul> <li>List[ str ] | None</li> </ul> Source code in <code>processing.py</code> <pre><code>def __dir__( self ) -&gt; List[ str ] | None:\n\t'''\n\n\t\tPurpose:\n\t\t---------\n\t\tProvides a list of strings representing class members.\n\n\n\t\tParameters:\n\t\t-----------\n\t\t- self\n\n\t\tReturns:\n\t\t--------\n\t\t- List[ str ] | None\n\n\t'''\n\treturn [ 'extract_text', 'split_sentences', 'clean_sentences',\n\t         'create_vocabulary', 'compute_frequency_distribution',\n\t         'summarize', 'filepath', 'raw_text', 'paragraphs',\n\t         'sentences', 'cleaned_sentences', 'vocabulary', 'freq_dist' ]\n</code></pre>"},{"location":"reference/api/#processing.Word.__init__","title":"__init__","text":"<pre><code>__init__(filepath)\n</code></pre>"},{"location":"reference/api/#processing.Word.__init__--purpose","title":"Purpose:","text":"<p>Initializes the WordTextProcessor with the path to the .docx file.</p>"},{"location":"reference/api/#processing.Word.__init__--parameters","title":"Parameters:","text":"<p>:param filepath: Path to the Microsoft Word document (.docx)</p> Source code in <code>processing.py</code> <pre><code>def __init__( self, filepath: str ) -&gt; None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t--------\n\t\tInitializes the WordTextProcessor with the path to the .docx file.\n\n\t\tParameters:\n\t\t----------\n\t\t:param filepath: Path to the Microsoft Word document (.docx)\n\n\t\"\"\"\n\tsuper( ).__init__( )\n\tself.file_path = filepath\n\tself.raw_text = ''\n\tself.paragraphs = [ ]\n\tself.sentences = [ ]\n\tself.cleaned_sentences = [ ]\n\tself.vocabulary = set( )\n</code></pre>"},{"location":"reference/api/#processing.Word.clean_sentences","title":"clean_sentences","text":"<pre><code>clean_sentences()\n</code></pre>"},{"location":"reference/api/#processing.Word.clean_sentences--purpose","title":"Purpose:","text":"<p>Cleans each _sentence: removes extra whitespace, punctuation, and lowers the text.</p> Source code in <code>processing.py</code> <pre><code>def clean_sentences( self ) -&gt; List[ str ] | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t-------\n\t\tCleans each _sentence: removes extra whitespace, punctuation, and lowers the text.\n\n\t\"\"\"\n\ttry:\n\t\tfor _sentence in self.sentences:\n\t\t\t_sentence = re.sub( r'[\\r\\n\\t]+', ' ', _sentence )\n\t\t\t_sentence = re.sub( r\"[^a-zA-Z0-9\\s']\", '', _sentence )\n\t\t\t_sentence = re.sub( r'\\s{2,}', ' ', _sentence ).strip( ).lower( )\n\t\t\tself.cleaned_sentences.append( _sentence )\n\n\t\treturn self.cleaned_sentences\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Word'\n\t\texception.method = 'clean_sentences( self ) -&gt; List[ str ]'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Word.compute_frequency_distribution","title":"compute_frequency_distribution","text":"<pre><code>compute_frequency_distribution()\n</code></pre>"},{"location":"reference/api/#processing.Word.compute_frequency_distribution--purpose","title":"Purpose:","text":"<p>Computes frequency distribution of the vocabulary.</p> Source code in <code>processing.py</code> <pre><code>def compute_frequency_distribution( self ) -&gt; Dict[ str, int ] | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t-------\n\t\tComputes frequency distribution of the vocabulary.\n\n\t\"\"\"\n\ttry:\n\t\twords = [ ]\n\t\tfor sentence in self.cleaned_sentences:\n\t\t\ttokens = word_tokenize( sentence )\n\t\t\ttokens = [ token for token in tokens if token.isalpha( ) ]\n\t\t\twords.extend( tokens )\n\t\tself.frequency_distribution = dict( Counter( words ) )\n\t\treturn self.frequency_distribution\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Word'\n\t\texception.method = 'compute_frequency_distribution( self ) -&gt; Dict[ str, int ]'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Word.create_vocabulary","title":"create_vocabulary","text":"<pre><code>create_vocabulary()\n</code></pre>"},{"location":"reference/api/#processing.Word.create_vocabulary--purpose","title":"Purpose:","text":"<p>Computes vocabulary terms from cleaned sentences.</p> Source code in <code>processing.py</code> <pre><code>def create_vocabulary( self ) -&gt; set | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t--------\n\t\tComputes vocabulary terms from cleaned sentences.\n\n\t\"\"\"\n\ttry:\n\t\tall_words = [ ]\n\t\tself.stop_words = set( stopwords.words( 'english' ) )\n\t\tfor _sentence in self.cleaned_sentences:\n\t\t\ttokens = word_tokenize( _sentence )\n\t\t\ttokens = [ token for token in tokens if\n\t\t\t           token.isalpha( ) and token not in self.stop_words ]\n\t\t\tall_words.extend( tokens )\n\t\tself.vocabulary = set( all_words )\n\t\treturn self.vocabulary\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Word'\n\t\texception.method = 'create_vocabulary( self ) -&gt; List[ str ]'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Word.extract_text","title":"extract_text","text":"<pre><code>extract_text()\n</code></pre>"},{"location":"reference/api/#processing.Word.extract_text--purpose","title":"Purpose:","text":"<p>Extracts raw text and paragraphs from the .docx file.</p> Source code in <code>processing.py</code> <pre><code>def extract_text( self ) -&gt; str | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t--------\n\t\tExtracts raw text and paragraphs from the .docx file.\n\n\t\"\"\"\n\ttry:\n\t\tself.document = Docx( self.file_path )\n\t\tself.paragraphs = [ para.text.strip( ) for para in self.document.paragraphs if\n\t\t                    para.text.strip( ) ]\n\t\tself.raw_text = '\\n'.join( self.paragraphs )\n\t\treturn self.raw_text\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Word'\n\t\texception.method = 'extract_text( self ) -&gt; str'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Word.split_sentences","title":"split_sentences","text":"<pre><code>split_sentences()\n</code></pre>"},{"location":"reference/api/#processing.Word.split_sentences--purpose","title":"Purpose:","text":"<p>Splits the raw text into sentences.</p> Source code in <code>processing.py</code> <pre><code>def split_sentences( self ) -&gt; List[ str ] | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t--------\n\t\tSplits the raw text into sentences.\n\n\t\"\"\"\n\ttry:\n\t\tself.sentences = sent_tokenize( self.raw_text )\n\t\treturn self.sentences\n\texcept Exception as e:\n\t\texception = Error( e )\n\t\texception.module = 'processing'\n\t\texception.cause = 'Word'\n\t\texception.method = 'split_sentences( self ) -&gt; List[ str ]'\n\t\terror = ErrorDialog( exception )\n\t\terror.show( )\n</code></pre>"},{"location":"reference/api/#processing.Word.summarize","title":"summarize","text":"<pre><code>summarize()\n</code></pre>"},{"location":"reference/api/#processing.Word.summarize--purpose","title":"Purpose:","text":"<p>Prints a summary of extracted and processed text.</p> Source code in <code>processing.py</code> <pre><code>def summarize( self ) -&gt; List[ str ] | None:\n\t\"\"\"\n\n\t\tPurpose:\n\t\t-------\n\t\tPrints a summary of extracted and processed text.\n\n\t\"\"\"\n\tprint( f'Document: {self.file_path}' )\n\tprint( f'Paragraphs: {len( self.paragraphs )}' )\n\tprint( f'Sentences: {len( self.sentences )}' )\n\tprint( f'Vocabulary Size: {len( self.vocabulary )}' )\n\tprint(\n\t\tf'Top 10 Frequent Words: {Counter( self.frequency_distribution ).most_common( 10 )}' )\n</code></pre>"}]}